{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import fitz\n",
    "import torch\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from langchain import hub\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "#from langchain_community.llms import HuggingFacePipeline\n",
    "#from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import PyPDFLoader, PDFMinerLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,NLTKTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "#TEST\n",
    "from FakeLLM import FakePromptCopyLLM\n",
    "import re  # For regular expressions\n",
    "from nltk.corpus import stopwords  # You'll need to install NLTK first\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Retrieval-Augmented Generation for Large Language Models: A Survey\\n\\nYunfan Gao 1 , Yun Xiong 2 , Xinyu Gao 2 , Kangxiang Jia 2 , Jinliu Pan 2 , Yuxi Bi 3 , Yi\\nDai1 , Jiawei Sun1 , Qianyu Guo4 , Meng Wang 3 and Haofen Wang 1,3 ∗\\n1 Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n2 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n3 College of Design and Innovation, Tongji University\\n4 School of Computer Science, Fudan University\\n\\n4\\n2\\n0\\n2', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 0}),\n",
       " Document(page_content='4\\n2\\n0\\n2\\n\\nn\\na\\nJ\\n\\n5\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n4\\nv\\n7\\n9\\n9\\n0\\n1\\n.\\n2\\n1\\n3\\n2\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\n(RAG)\\n\\nuntraceable', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 502}),\n",
       " Document(page_content='Large Language Models (LLMs) demonstrate\\nsignificant capabilities but face challenges such\\nas hallucination, outdated knowledge, and nontransparent,\\nreasoning processes.\\nhas\\nRetrieval-Augmented Generation\\nemerged as a promising solution by incorporating\\nknowledge from external databases. This enhances\\nthe accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for\\ncontinuous knowledge updates and integration of\\ndomain-specific information. RAG synergistically', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 603}),\n",
       " Document(page_content='domain-specific information. RAG synergistically\\nmerges LLMs’ intrinsic knowledge with the vast,\\ndynamic repositories of external databases. This\\ncomprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms,\\nencompassing the Naive RAG, the Advanced RAG,\\nand the Modular RAG. It meticulously scrutinizes\\nthe tripartite foundation of RAG frameworks,\\nwhich includes the retrieval , the generation and\\nthe augmentation techniques. The paper highlights\\nthe state-of-the-art', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 1057}),\n",
       " Document(page_content='the state-of-the-art\\ntechnologies embedded in\\neach of these critical components, providing a\\nprofound understanding of the advancements in\\nRAG systems. Furthermore, this paper introduces\\nthe metrics and benchmarks for assessing RAG\\nmodels, along with the most up-to-date evaluation\\nIn conclusion, the paper delineates\\nframework.\\nincluding the\\nprospective avenues for research,\\nidentification of challenges,\\nthe expansion of\\nmulti-modalities, and the progression of the RAG\\ninfrastructure and its ecosystem. 1.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 1537}),\n",
       " Document(page_content='1 Introduction\\nLarge language models (LLMs) such as the GPT series [Brown et al., 2020, OpenAI, 2023] and the LLama series [Touvron et al., 2023], along with other models like\\n[Google, 2023], have achieved remarkable sucGemini\\ncess in natural language processing, demonstrating supe\\n∗Corresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\n\\nRAG-Survey\\n\\nincorrect', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 2048}),\n",
       " Document(page_content='rior performance on various benchmarks including SuperGLUE [Wang et al., 2019], MMLU [Hendrycks et al., 2020],\\nand BIG-bench [Srivastava et al., 2022].\\nDespite these\\nadvancements, LLMs exhibit notable limitations, particularly in handling domain-specific or highly specialized queries [Kandpal et al., 2023]. A common issue is\\nthe generation of\\ninformation, or ”hallucinations” [Zhang et al., 2023b], especially when queries extend', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 2470}),\n",
       " Document(page_content='beyond the model’s training data or necessitate up-to-date information. These shortcomings underscore the impracticality of deploying LLMs as black-box solutions in real-world\\nproduction environments without additional safeguards. One\\npromising approach to mitigate these limitations is RetrievalAugmented Generation (RAG), which integrates external\\ndata retrieval into the generative process, thereby enhancing\\nthe model’s ability to provide accurate and relevant responses.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 2902}),\n",
       " Document(page_content='the model’s ability to provide accurate and relevant responses.\\nRAG, introduced by Lewis et al. [Lewis et al., 2020] in\\nmid-2020, stands as a paradigm within the realm of LLMs,\\nenhancing generative tasks. Specifically, RAG involves an\\ninitial retrieval step where the LLMs query an external data\\nsource to obtain relevant information before proceeding to answer questions or generate text. This process not only informs', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 3314}),\n",
       " Document(page_content='the subsequent generation phase but also ensures that the responses are grounded in retrieved evidence, thereby significantly enhancing the accuracy and relevance of the output.\\nThe dynamic retrieval of information from knowledge bases\\nduring the inference phase allows RAG to address issues such\\nas the generation of factually incorrect content, commonly\\nreferred to as “hallucinations.” The integration of RAG into', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 3734}),\n",
       " Document(page_content='referred to as “hallucinations.” The integration of RAG into\\nLLMs has seen rapid adoption and has become a pivotal technology in refining the capabilities of chatbots and rendering\\nLLMs more viable for practical applications.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 4090}),\n",
       " Document(page_content='The evolutionary trajectory of RAG unfolds across four\\ndistinctive phases, as illustrated in Figure 1.\\nIn its inception in 2017, aligned with the emergence of the Transformer architecture, the primary thrust was on assimilating\\nadditional knowledge through Pre-Training Models (PTM)\\nto augment language models. This epoch witnessed RAG’s\\nfoundational efforts predominantly directed at optimizing\\npre-training methodologies.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 4317}),\n",
       " Document(page_content='Following this initial phase, a period of relative dormancy\\nensued before the advent of chatGPT, during which there was\\nminimal advancement in related research for RAG. The subsequent arrival of chatGPT marked a pivotal moment in the\\n\\n \\n \\n \\n \\n \\n \\n\\x0cFigure 1: Technology tree of RAG research development featuring representative works', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 4742}),\n",
       " Document(page_content='trajectory, propelling LLMs into the forefront. The community’s focal point shifted towards harnessing the capabilities of LLMs to attain heightened controllability and address evolving requirements. Consequently, the lion’s share\\nof RAG endeavors concentrated on inference, with a minority dedicated to fine-tuning processes. As LLM capabilities continued to advance, especially with the introduction of', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 5076}),\n",
       " Document(page_content='GPT-4, the landscape of RAG technology underwent a significant transformation. The emphasis evolved into a hybrid\\napproach, combining the strengths of RAG and fine-tuning,\\nalongside a dedicated minority continuing the focus on optimizing pre-training methodologies.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 5481}),\n",
       " Document(page_content='Despite the rapid growth of RAG research, there has been\\na lack of systematic consolidation and abstraction in the field,\\nwhich poses challenges in understanding the comprehensive\\nlandscape of RAG advancements. This survey aims to outline the entire RAG process and encompass the current and\\nfuture directions of RAG research, by providing a thorough\\nexamination of retrieval augmentation in LLMs.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 5748}),\n",
       " Document(page_content='Therefore, this paper aims to comprehensively summarize\\nand organize the technical principles, developmental history,\\ncontent, and, in particular, the relevant methods and applications after the emergence of LLMs, as well as the evaluation\\nmethods and application scenarios of RAG. It seeks to pro\\nvide a comprehensive overview and analysis of existing RAG\\ntechnologies and offer conclusions and prospects for future\\ndevelopment methods. This survey intends to furnish readers', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 6147}),\n",
       " Document(page_content='development methods. This survey intends to furnish readers\\nand practitioners with a thorough and systematic comprehension of large models and RAG, elucidate the progression and\\nkey technologies of retrieval augmentation, clarify the merits\\nand limitations of various technologies along with their suitable contexts, and forecast potential future developments.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 6564}),\n",
       " Document(page_content='Our contributions are as follows:\\n• We present a thorough and systematic review of the\\nstate-of-the-art RAG, delineating its evolution through\\nparadigms including naive RAG, advanced RAG, and\\nmodular RAG. This review contextualizes the broader\\nscope of RAG research within the landscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generator” and “Augmentation”, and', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 6926}),\n",
       " Document(page_content='of “Retrieval”, “Generator” and “Augmentation”, and\\ndelve into their synergies, elucidating how these components intricately collaborate to form a cohesive and\\neffective RAG framework.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 7339}),\n",
       " Document(page_content='• We construct a thorough evaluation framework for RAG,\\noutlining the evaluation objectives and metrics. Our\\ncomparative analysis clarifies the strengths and weaknesses of RAG compared to fine-tuning from various\\n\\n\\x0cperspectives. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to\\ntackle current challenges, expansions into multi-modal\\nsettings, and the development of its ecosystem.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 7525}),\n",
       " Document(page_content='The paper unfolds as follows: Section 2 and 3 define RAG\\nand detail its developmental process. Section 4 through 6 explore core components—Retrieval, “Generation” and “Augmentation”—highlighting diverse embedded technologies.\\nSection 7 focuses on RAG’s evaluation system. Section 8\\ncompare RAG with other LLM optimization methods and\\nsuggest potential directions for its evolution. The paper concludes in Section 9.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 7951}),\n",
       " Document(page_content='2 Definition\\nThe definition of RAG can be summarized from its workflow.\\nFigure 2 depicts a typical RAG application workflow. In this\\nscenario, a user inquires ChatGPT about a recent high-profile\\nevent (i.e., the abrupt dismissal and reinstatement of OpenAI’s CEO) which generated considerable public discourse.\\nChatGPT as the most renowned and widely utilized LLM,\\nconstrained by its pretraining data, lacks knowledge of recent events. RAG addresses this gap by retrieving up-to-date', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 8368}),\n",
       " Document(page_content='document excerpts from external knowledge bases. In this instance, it procures a selection of news articles pertinent to the\\ninquiry. These articles, alongside the initial question, are then\\namalgamated into an enriched prompt that enables ChatGPT\\nto synthesize an informed response. This example illustrates\\nthe RAG process, demonstrating its capability to enhance the\\nmodel’s responses with real-time information retrieval.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 8852}),\n",
       " Document(page_content='Technologically, RAG has been enriched through various\\ninnovative approaches addressing pivotal questions such as\\n“what to retrieve” “when to retrieve” and “how to use the\\nretrieved information”. For “what to retrieve” research has\\nprogressed from simple token [Khandelwal et al., 2019] and\\nentity retrieval [Nishikawa et al., 2022] to more complex\\nstructures like chunks [Ram et al., 2023] and knowledge\\ngraph [Kang et al., 2023], with studies focusing on the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 9279}),\n",
       " Document(page_content='graph [Kang et al., 2023], with studies focusing on the\\nretrieval and the level of data structurgranularity of\\ning.\\nCoarse granularity brings more information but\\nwith lower precision. Retrieving structured text provides\\nmore information while sacrificing efficiency. The question of “when to retrieve” has led to strategies ranging\\nfrom single [Wang et al., 2023e, Shi et al., 2023] to adaptive [Jiang et al., 2023b, Huang et al., 2023] and multiple\\nretrieval [Izacard et al., 2022] methods. High frequency of', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 9684}),\n",
       " Document(page_content='retrieval [Izacard et al., 2022] methods. High frequency of\\nretrieval brings more information and lower efficiency. As\\nfor ”how to use” the retrieved data, integration techniques\\nhave been developed across various levels of the model\\n[Khattab et al., 2022],\\narchitecture,\\nintermediate\\nlayers [Liang et al., 2023]. Although the “intermediate” and\\n“output layers” are more effective, there are problems with\\nthe need for training and low efficiency.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 10135}),\n",
       " Document(page_content='including the input\\n[Borgeaud et al., 2022],\\n\\noutput\\n\\nand\\n\\nRAG is a paradigm that enhances LLMs by integrating external knowledge bases. It employs a synergistic approach,\\ncombining information retrieval mechanisms and In-Context\\nLearning (ICL) to bolster the LLM’s performance.\\nIn this\\nframework, a query initiated by a user prompts the retrieval of', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 10584}),\n",
       " Document(page_content='pertinent information via search algorithms. This information\\nis then woven into the LLM’s prompts, providing additional\\ncontext for the generation process. RAG’s key advantage lies\\nin its obviation of the need for retraining of LLMs for taskspecific applications. Developers can instead append an external knowledge repository, enriching the input and thereby\\nrefining the model’s output precision. RAG has become one\\nof the most popular architectures in LLMs’ systems, due to', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 10936}),\n",
       " Document(page_content='of the most popular architectures in LLMs’ systems, due to\\nits high practicality and low barrier to entry, with many conversational products being built almost entirely on RAG.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 11355}),\n",
       " Document(page_content='The RAG workflow comprises three key steps. First, the\\ncorpus is partitioned into discrete chunks, upon which vector indices are constructed utilizing an encoder model. Second, RAG identifies and retrieves chunks based on their vector similarity to the query and indexed chunks. Finally, the\\nmodel synthesizes a response conditioned on the contextual\\ninformation gleaned from the retrieved chunks. These steps', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 11533}),\n",
       " Document(page_content='information gleaned from the retrieved chunks. These steps\\nform the fundamental framework of the RAG process, underpinning its information retrieval and context-aware generation capabilities. Next, we will provide an introduction to the\\nRAG research framework.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 11884}),\n",
       " Document(page_content='3 RAG Framework\\nThe RAG research paradigm is continuously evolving, and\\nthis section primarily delineates its progression. We categorize it into three types: Naive RAG, Advanced RAG, and\\nModular RAG. While RAG were cost-effective and surpassed\\nthe performance of the native LLM, they also exhibited several limitations. The development of Advanced RAG and\\nModular RAG was a response to these specific shortcomings\\nin Naive RAG.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 12146}),\n",
       " Document(page_content='3.1 Naive RAG\\nThe Naive RAG research paradigm represents the earliest\\nmethodology, which gained prominence shortly after the\\nwidespread adoption of ChatGPT. The Naive RAG follows a\\ntraditional process that includes indexing, retrieval, and generation. It is also characterized as a “Retrieve-Read” framework [Ma et al., 2023a].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 12575}),\n",
       " Document(page_content='Indexing\\nThe indexing process is a crucial initial step in data preparation that occurs offline and involves several stages. It begins\\nwith data indexing, where original data is cleansed and extracted, and various file formats such as PDF, HTML, Word,\\nand Markdown are converted into standardized plain text. In\\norder to fit within the context limitations of language models,\\nthis text is then segmented into smaller, more manageable\\nchunks in a process known as chunking. These chunks are', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 12904}),\n",
       " Document(page_content='chunks in a process known as chunking. These chunks are\\nsubsequently transformed into vector representations through\\nan embedding model, chosen for its balance between inference efficiency and model size. This facilitates similarity\\ncomparisons during the retrieval phase. Finally, an index is\\ncreated to store these text chunks and their vector embeddings as key-value pairs, which allows for efficient and scalable search capabilities.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 13338}),\n",
       " Document(page_content='Retrieval\\nUpon receipt of a user query, the system employs the same encoding model utilized during the indexing phase to transcode\\n\\n\\x0cFigure 2: A representative instance of the RAG process applied to question answering', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 13777}),\n",
       " Document(page_content='the input into a vector representation.\\nIt then proceeds to\\ncompute the similarity scores between the query vector and\\nthe vectorized chunks within the indexed corpus. The system\\nprioritizes and retrieves the top K chunks that demonstrate\\nthe greatest similarity to the query. These chunks are subsequently used as the expanded contextual basis for addressing\\nthe user’s request.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 13996}),\n",
       " Document(page_content='Generation\\nThe posed query and selected documents are synthesized into\\na coherent prompt to which a large language model is tasked\\nwith formulating a response. The model’s approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge\\nor restrict its responses to the information contained within\\nthe provided documents.\\nIn cases of ongoing dialogues,\\nany existing conversational history can be integrated into the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 14377}),\n",
       " Document(page_content='any existing conversational history can be integrated into the\\nprompt, enabling the model to engage in multi-turn dialogue\\ninteractions effectively.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 14800}),\n",
       " Document(page_content='Drawbacks in Naive RAG\\nNaive RAG faces significant challenges in three key areas:\\n“Retrieval,” “Generation,” and “Augmentation”.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 14950}),\n",
       " Document(page_content='Retrieval quality poses diverse challenges, including low\\nprecision, leading to misaligned retrieved chunks and potential issues like hallucination or mid-air drop. Low recall\\nalso occurs, resulting in the failure to retrieve all relevant\\nchunks, thereby hindering the LLMs’ ability to craft compre\\nhensive responses. Outdated information further compounds\\nthe problem, potentially yielding inaccurate retrieval results.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 15080}),\n",
       " Document(page_content='the problem, potentially yielding inaccurate retrieval results.\\nResponse generation quality presents hallucination challenge, where the model generates answers not grounded in\\nthe provided context, as well as issues of irrelevant context\\nand potential toxicity or bias in the model’s output.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 15437}),\n",
       " Document(page_content='The augmentation process presents its own challenges in\\neffectively integrating context from retrieved passages with\\nthe current generation task, potentially leading to disjointed\\nor incoherent output. Redundancy and repetition are also\\nconcerns, especially when multiple retrieved passages contain similar information, resulting in repetitive content in the\\ngenerated response.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 15730}),\n",
       " Document(page_content='Discerning the importance and relevance of multiple retrieved passages to the generation task is another challenge,\\nrequiring the proper balance of each passage’s value. Additionally, reconciling differences in writing styles and tones to\\nensure consistency in the output is crucial.\\n\\nLastly, there’s a risk of generation models overly depending on augmented information, potentially resulting in outputs that merely reiterate the retrieved content without providing new value or synthesized information.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 16110}),\n",
       " Document(page_content='3.2 Advanced RAG\\nAdvanced RAG has been developed with targeted enhancements to address the shortcomings of Naive RAG. In terms\\nof retrieval quality, Advanced RAG implements pre-retrieval\\n\\n\\x0cand post-retrieval strategies. To address the indexing challenges experienced by Naive RAG, Advanced RAG has refined its indexing approach using techniques such as sliding window, fine-grained segmentation, and metadata. It has\\nalso introduced various methods to optimize the retrieval process [ILIN, 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 16616}),\n",
       " Document(page_content='Pre-Retrieval Process\\nOptimizing Data Indexing.The goal of optimizing data indexing is to enhance the quality of the content being indexed.\\nThis involves five primary strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 17114}),\n",
       " Document(page_content='Enhancing data granularity aims to elevate text standardization, consistency, factual accuracy, and rich context to improve the RAG system’s performance. This includes removing irrelevant information, dispelling ambiguity in entities\\nand terms, confirming factual accuracy, maintaining context,\\nand updating outdated documents.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 17413}),\n",
       " Document(page_content='Optimizing index structures involves adjusting the size of\\nchunks to capture relevant context, querying across multiple\\nindex paths, and incorporating information from the graph\\nstructure to capture relevant context by leveraging relationships between nodes in a graph data index.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 17742}),\n",
       " Document(page_content='Adding metadata information involves integrating referenced metadata, such as dates and purposes, into chunks for\\nfiltering purposes, and incorporating metadata like chapters\\nand subsections of references to improve retrieval efficiency.\\nAlignment optimization addresses alignment issues and\\ndisparities between documents by introducing “hypothetical\\nquestions” [Li et al., 2023d] into documents to rectify alignment issues and differences.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 18024}),\n",
       " Document(page_content='Retrieval\\nDuring the retrieval stage, the primary focus is on identifying\\nthe appropriate context by calculating the similarity between\\nthe query and chunks. The embedding model is central to\\nthis process. In the advanced RAG, there is potential for optimization of the embedding models.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 18466}),\n",
       " Document(page_content='Fine-tuning Embedding. Fine-tuning embedding models\\nsignificantly impact the relevance of retrieved content in RAG\\nsystems. This process involves customizing embedding models to enhance retrieval relevance in domain-specific contexts,\\nespecially for professional domains dealing with evolving or\\nrare terms. The BGE embedding model [BAAI, 2023], such\\nas BGE-large-EN developed by BAAI2, is an example of a\\nhigh-performance embedding model that can be fine-tuned', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 18755}),\n",
       " Document(page_content='high-performance embedding model that can be fine-tuned\\nto optimize retrieval relevance. Training data for fine-tuning\\ncan be generated using language models like GPT-3.5-turbo\\nto formulate questions grounded on document chunks, which\\nare then used as fine-tuning pairs.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 19161}),\n",
       " Document(page_content='Dynamic Embedding adapts to the context in which words\\nare used, unlike static embedding, which uses a single vector for each word [Karpukhin et al., 2020]. For example,\\nin transformer models like BERT, the same word can have\\nvaried embeddings depending on surrounding words. OpenAI’s embeddings-ada-02 model3, built upon the principles\\n\\n2https://huggingface.co/BAAI/bge-large-en\\n3https://platform.openai.com/docs/guides/embeddings', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 19433}),\n",
       " Document(page_content='of LLMs like GPT, is a sophisticated dynamic embedding\\nmodel that captures contextual understanding. However, it\\nmay not exhibit the same sensitivity to context as the latest\\nfull-size language models like GPT-4.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 19866}),\n",
       " Document(page_content='Post-Retrieval Process\\nAfter retrieving valuable context from the database, it is essential to merge it with the query as an input into LLMs while\\naddressing challenges posed by context window limits. Simply presenting all relevant documents to the LLM at once may\\nexceed the context window limit, introduce noise, and hinder\\nthe focus on crucial information. Additional processing of the\\nretrieved content is necessary to address these issues.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 20080}),\n",
       " Document(page_content='Re-Ranking. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt\\nis a key strategy.\\nThis concept has been implemented\\nin frameworks such as LlamaIndex4, LangChain5, and\\nHayStack [Blagojevi, 2023]. For example, Diversity Ranker6\\nprioritizes reordering based on document diversity, while\\nLostInTheMiddleRanker alternates placing the best document at the beginning and end of the context window. Additionally, approaches like cohereAI rerank [Cohere, 2023],', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 20526}),\n",
       " Document(page_content='bge-rerank7, and LongLLMLingua [Jiang et al., 2023a] recalculate the semantic similarity between relevant text and the\\nquery, addressing the challenge of interpreting vector-based\\nsimulated searches for semantic similarity.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 21030}),\n",
       " Document(page_content='Prompt Compression. Research indicates that noise in retrieved documents adversely affects RAG performance.\\nIn\\npost-processing, the emphasis lies in compressing irrelevant\\ncontext, highlighting pivotal paragraphs, and reducing the\\noverall context length. Approaches such as Selective Context\\nand LLMLingua [Litman et al., 2020, Anderson et al., 2022]\\nutilize small', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 21255}),\n",
       " Document(page_content='utilize small\\nlanguage models to calculate prompt mutual information or perplexity, estimating element importance. Recomp [Xu et al., 2023a] addresses this by training compressors at different granularities, while Long\\nContext [Xu et al., 2023b] and “Walking in the Memory\\nMaze” [Chen et al., 2023a] design summarization techniques\\nto enhance LLM’s key information perception, particularly in\\ndealing with extensive contexts.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 21606}),\n",
       " Document(page_content='3.3 Modular RAG\\nThe modular RAG structure diverges from the traditional Naive RAG framework, providing greater versatility and flexibility.\\nIt integrates various methods to enhance functional modules, such as incorporating a search\\nmodule for similarity retrieval and applying a fine-tuning\\napproach in the retriever [Lin et al., 2023]. Restructured\\nRAG modules [Yu et al., 2022] and iterative methodologies', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 22033}),\n",
       " Document(page_content='RAG modules [Yu et al., 2022] and iterative methodologies\\nlike [Shao et al., 2023] have been developed to address specific issues. The modular RAG paradigm is increasingly becoming the norm in the RAG domain, allowing for either a\\nserialized pipeline or an end-to-end training approach across\\nmultiple modules. The comparison of three RAG paradigms', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 22383}),\n",
       " Document(page_content='4https://www.llamaindex.ai\\n5https://www.langchain.com/\\n6https://haystack.deepset.ai/blog/\\nenhancing-rag-pipelines-in-haystack\\n\\n7https://huggingface.co/BAAI/bge-reranker-large\\n\\n\\x0cFigure 3: Comparison between the three paradigms of RAG\\n\\nis depicted in Figure 3. However, Modular RAG is not standalone. Advanced RAG is a specialized form of modular\\nRAG, and further, Naive RAG itself is a special case of Advanced RAG. The relationship among the three paradigms is\\none of inheritance and development.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 22733}),\n",
       " Document(page_content='New Modules\\nSearch Module.\\nIn contrast to the similarity retrieval in\\nNaive/Advanced RAG, the Search Module is tailored to specific scenarios and incorporates direct searches on additional\\ncorpora. This integration is achieved using code generated\\nby the LLM, query languages such as SQL or Cypher, and\\nother custom tools. The data sources for these searches can\\ninclude search engines, text data, tabular data, and knowledge\\ngraphs [Wang et al., 2023d].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 23231}),\n",
       " Document(page_content='Memory Module. This module harnesses the memory capabilities of the LLM to guide retrieval. The approach involves identifying memories most similar to the current input.\\nSelfmem [Cheng et al., 2023b] utilizes a retrieval-enhanced\\ngenerator to create an unbounded memory pool iteratively,\\ncombining the “original question” and “dual question”. By\\nemploying a retrieval-enhanced generative model that uses its\\nown outputs to improve itself, the text becomes more aligned', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 23687}),\n",
       " Document(page_content='own outputs to improve itself, the text becomes more aligned\\nwith the data distribution during the reasoning process. Consequently, the model’s own outputs are utilized instead of the\\ntraining data [Wang et al., 2022a].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 24095}),\n",
       " Document(page_content='Fusion. RAG-Fusion [Raudaschl, 2023]enhances traditional search systems by addressing their limitations through\\na multi-query approach that expands user queries into mul\\ntiple, diverse perspectives using an LLM. This approach not\\nonly captures the explicit information users seek but also uncovers deeper, transformative knowledge. The fusion process involves parallel vector searches of both original and\\nexpanded queries, intelligent re-ranking to optimize results,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 24316}),\n",
       " Document(page_content='expanded queries, intelligent re-ranking to optimize results,\\nand pairing the best outcomes with new queries. This sophisticated method ensures search results that align closely with\\nboth the explicit and implicit intentions of the user, leading to\\nmore insightful and relevant information discovery.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 24722}),\n",
       " Document(page_content='Routing. The RAG system’s retrieval process utilizes diverse sources, differing in domain, language, and format,\\nwhich can be either alternated or merged based on the situation [Li et al., 2023b]. Query routing decides the subsequent action to a user’s query, with options ranging from\\nsummarization, searching specific databases, or merging different pathways into a single response. The query router also\\nchooses the appropriate data store for the query, which may', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 25024}),\n",
       " Document(page_content='chooses the appropriate data store for the query, which may\\ninclude various sources like vector stores, graph databases, or\\nrelational databases, or a hierarchy of indices—for instance, a\\nsummary index and a document block vector index for multidocument storage. The query router’s decision-making is predefined and executed via LLMs calls, which direct the query\\nto the chosen index.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 25431}),\n",
       " Document(page_content='Predict . It addresses the common issues of redundancy\\nand noise in retrieved content. Instead of directly retrieving\\nfrom a data source, this module utilizes the LLM to generate\\nthe necessary context [Yu et al., 2022]. The content produced\\nby the LLM is more likely to contain pertinent information\\ncompared to that obtained through direct retrieval.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 25817}),\n",
       " Document(page_content='Task Adapter. This module focuses on adapting RAG to a\\nvariety of downstream tasks. UPRISE automates the retrieval\\nof prompts for zero-shot task inputs from a pre-constructed\\ndata pool, thereby enhancing universality across tasks and\\nmodels [Cheng et al., 2023a]. Meanwhile, PROMPTAGATOR [Dai et al., 2022] utilizes LLM as a few-shot query generator and, based on the generated data, creates task-specific\\nretrievers. By leveraging the generalization capability of', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 26171}),\n",
       " Document(page_content='retrievers. By leveraging the generalization capability of\\nLLMs, it enables the development of task-specific end-to-end\\nretrievers with minimal examples.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 26577}),\n",
       " Document(page_content='New Patterns', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 26732}),\n",
       " Document(page_content='The organizational structure of Modular RAG is highly adaptable, allowing for the substitution or rearrangement of modules within the RAG process to suit specific problem contexts.\\nNaive RAG and Advanced RAG can both be considered as\\nbeing composed of some fixed modules. As illustrated in the\\nfigure 3, Naive RAG primarily consists of the “Retrieve” and\\n“Read” modules. A typical pattern of Advanced RAG builds\\nupon the foundation of Naive RAG by adding “Rewrite” and', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 26746}),\n",
       " Document(page_content='upon the foundation of Naive RAG by adding “Rewrite” and\\n“Rerank” modules. However, on the whole, modular RAG\\nenjoys greater diversity and flexibility.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 27158}),\n",
       " Document(page_content='Current research primarily explores two organizational\\nparadigms. The first involves adding or replacing modules,\\nwhile the second focuses on adjusting the organizational flow\\nbetween modules. This flexibility enables tailoring the RAG\\nprocess to effectively address a wide array of tasks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 27311}),\n",
       " Document(page_content='Adding or Replacing Modules.The strategy of introducing\\nor substituting modules involves maintaining the core structure of the Retrieval-Read process while integrating additional modules to enhance specific functionalities. The RRR\\nmodel [Ma et al., 2023a] introduces the Rewrite-RetrieveRead process, utilizing the LLM performance as a reinforcement learning incentive for a rewriting module. This enables\\nthe rewriter to fine-tune retrieval queries, thereby improving', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 27602}),\n",
       " Document(page_content='the rewriter to fine-tune retrieval queries, thereby improving\\nthe downstream task performance of the reader.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 28009}),\n",
       " Document(page_content='Similarly, modules can be selectively swapped in methodologies like Generate-Read [Yu et al., 2022], where the\\nLLM’s generation module takes the place of the retrieval\\nmodule. The Recite-Read approach [Sun et al., 2022] transforms external retrieval into retrieval from model weights,\\nrequiring the LLM to initially memorize task-specific information and subsequently produce output capable of handling\\nknowledge-intensive natural language processing tasks.\\n\\nAdjusting the Flow between Modules.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 28120}),\n",
       " Document(page_content='zheIn the realm\\nof module flow adjustment, there is a focus on enhancing\\nthe interaction between language models and retrieval models. DSP [Khattab et al., 2022] introduces the DemonstrateSearch-Predict framework, treating the context learning system as an explicit program rather than a final task prompt,\\nleading to more effective handling of knowledge-intensive\\ntasks. The ITER-RETGEN [Shao et al., 2023] approach utilizes generated content to guide retrieval,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 28616}),\n",
       " Document(page_content='iteratively implementing “retrieval-enhanced generation” and “generationenhanced retrieval” within a Retrieve-Read-Retrieve-Read\\nflow. This method demonstrates an innovative way of using\\none module’s output to improve the functionality of another.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 29080}),\n",
       " Document(page_content='Optimizing the RAG Pipeline\\nThe optimization of the retrieval process aims to enhance the\\nefficiency and quality of information in RAG systems. Current research focuses on integrating diverse search technologies, refining retrieval steps, incorporating cognitive backtracking, implementing versatile query strategies, and leveraging embedding similarity. These efforts collectively strive\\nto achieve a balance between retrieval efficiency and the\\ndepth of contextual information in RAG systems.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 29329}),\n",
       " Document(page_content='Hybrid Search Exploration. The RAG system optimizes its\\nperformance by intelligently integrating various techniques,\\nincluding keyword-based search, semantic search, and vector search. This approach leverages the unique strengths of\\neach method to accommodate diverse query types and information needs, ensuring consistent retrieval of highly relevant\\nand context-rich information. The use of hybrid search serves', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 29825}),\n",
       " Document(page_content='and context-rich information. The use of hybrid search serves\\nas a robust supplement to retrieval strategies, thereby enhancing the overall efficacy of the RAG pipeline.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 30177}),\n",
       " Document(page_content='Recursive Retrieval and Query Engine. Recursive retrieval\\ninvolves acquiring smaller chunks during the initial retrieval\\nphase to capture key semantic meanings. Subsequently, larger\\nchunks containing more contextual information are provided\\nto the LLM in later stages of the process. This two-step retrieval method helps to strike a balance between efficiency\\nand the delivery of contextually rich responses.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 30348}),\n",
       " Document(page_content='StepBack-prompt approach encourages the LLM to move\\naway from specific instances and engage in reasoning around\\nbroader concepts and principles [Zheng et al., 2023]. Experimental results demonstrate a significant performance increase\\nin various challenging, inference-based tasks when backward\\nprompts are used, highlighting their natural adaptability to the\\nRAG process. These retrieval-enhancing steps can be applied\\nboth in generating responses to backward prompts and in the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 30758}),\n",
       " Document(page_content='both in generating responses to backward prompts and in the\\nfinal question-answering process.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 31177}),\n",
       " Document(page_content='Sub-Queries. Depending on the scenario, various query\\nstrategies can be employed, such as using query engines\\nprovided by frameworks like LlamaIndex, leveraging tree\\nqueries, utilizing vector queries, or executing simple sequential querying of chunks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 31272}),\n",
       " Document(page_content='Hypothetical Document Embeddings. HyDE operates on\\nthe belief that the answers generated might be closer in the\\nembedding space than a direct query. Using the LLM, HyDE\\ncreates a hypothetical document (answer) in response to a\\nquery, embeds this document, and uses the resulting embedding to retrieve real documents similar to the hypothetical one. Instead of seeking embedding similarity based on', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 31525}),\n",
       " Document(page_content='the query, this approach focuses on the embedding similarity from one answer to another [Gao et al., 2022]. However,\\nit might not consistently produce desirable outcomes, especially when the language model is unfamiliar with the subject\\nmatter, potentially leading to more instances with errors.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 31923}),\n",
       " Document(page_content='4 Retrieval\\n\\nIn the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. However, creating a\\nproficient retriever presents significant challenges. This sectionelves into three fundamental questions: 1) How can we\\nachieve accurate semantic representations? 2) What methods\\n\\n\\x0ccan align the semantic spaces of queries and documents? 3)\\nHow can the retriever’s output be aligned with the preferences\\nof the Large Language Model?', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 32220}),\n",
       " Document(page_content='The combination of these diverse methods has led to notable advancements, resulting in enhanced retrieval outcomes\\nand improved performance for RAG.\\n\\n4.1 Enhancing Semantic Representations\\nIn RAG, the semantic space is essential as it involves the multidimensional mapping of queries and documents. Retrieval\\naccuracy in this semantic space significantly impacts RAG\\noutcomes. This section will present two methods for building\\naccurate semantic spaces.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 32688}),\n",
       " Document(page_content='Chunk optimization\\nWhen managing external documents, the initial step involves\\nbreaking them down into smaller chunks to extract finegrained features, which are then embedded to represent their\\nsemantics. However, embedding overly large or excessively\\nsmall text chunks may lead to sub-optimal outcomes. Therefore, identifying the optimal chunk size for documents within\\nthe corpus is crucial to ensuring the accuracy and relevance\\nof the retrieved results.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 33143}),\n",
       " Document(page_content='Choosing an appropriate chunking strategy requires careful consideration of several vital factors, such as the nature\\nof the indexed content, the embedding model and its optimal block size, the expected length and complexity of user\\nqueries, and the specific application’s utilization of the retrieved results. For instance, the selection of a chunking\\nmodel should be based on the content’s length—whether it', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 33602}),\n",
       " Document(page_content='model should be based on the content’s length—whether it\\nis longer or shorter. Additionally, different embedding models demonstrate distinct performance characteristics at varying block sizes. For example, sentence-transformer performs\\nbetter with single sentences, while text-embedding-ada-002\\nexcels with blocks containing 256 or 512 tokens.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 33955}),\n",
       " Document(page_content='Additionally, factors like the length and complexity of user\\ninput questions, and the specific needs of the application (e.g.,\\nsemantic search or question answering), have effect on the\\nchoice of a chunking strategy. This choice can be directly influenced by the token limits of the selected LLMs, requiring\\nadjustments to the block size. In reality, getting precise query\\nresults involves flexibly applying different chunking strategies. There is no one-size-fits-all ”best” strategy, only the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 34300}),\n",
       " Document(page_content='most appropriate one for a particular context.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 34795}),\n",
       " Document(page_content='Current research in RAG explores various block optimization techniques aimed at improving both retrieval efficiency\\nand accuracy. One such approach involves the use of sliding window technology, enabling layered retrieval by merging globally related information across multiple retrieval processes. Another strategy, known as the “small2big” method,\\nutilizes small text blocks during the initial search phase and\\nsubsequently provides larger related text blocks to the language model for processing.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 34843}),\n",
       " Document(page_content='The abstract embedding technique prioritizes top K retrieval based on document abstracts (or summaries), offering\\na comprehensive understanding of the entire document context. Additionally, the metadata filtering technique leverages\\ndocument metadata to enhance the filtering process. An innovative approach, the graph indexing technique, transforms\\nentities and relationships into nodes and connections, significantly improving relevance, particularly in the context of\\nmulti-hop problems.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 35344}),\n",
       " Document(page_content='Fine-tuning Embedding Models\\nOnce the appropriate size of chunks is determined,\\nthe\\nnext crucial step involves embedding these chunks and the\\nquery into the semantic space using an embedding model.\\nThe effectiveness of the embedding is critical as it impacts\\nthe model’s ability to represent the corpus. Recent research has introduced prominent embedding models such as\\nAngIE, Voyage, BGE,etc [Li and Li, 2023, VoyageAI, 2023,\\nBAAI, 2023]. These models have undergone pre-training on', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 35836}),\n",
       " Document(page_content='BAAI, 2023]. These models have undergone pre-training on\\nextensive corpora. However, their capability to accurately\\ncapture domain-specific information may be limited when applied to specialized domains.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 36263}),\n",
       " Document(page_content='Moreover, task-specific fine-tuning of embedding models\\nis essential to ensure that the model comprehends the user\\nquery in terms of content relevance. A model without finetuning may not adequately address the requirements of a specific task. Consequently, fine-tuning an embedding model becomes crucial for downstream applications. There are two\\nprimary paradigms in embedding fine-tuning methods.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 36468}),\n",
       " Document(page_content='Domain Knowledge Fine-tuning. To ensure that an embedding model accurately captures domain-specific information,\\nit is imperative to utilize domain-specific datasets for finetuning. This process diverges from standard language model\\nfine-tuning, chiefly in the nature of the datasets involved.\\nTypically, the dataset for embedding model fine-tuning encompasses three principal elements: queries, a corpus, and\\nrelevant documents. The model employs these queries to', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 36868}),\n",
       " Document(page_content='relevant documents. The model employs these queries to\\nidentify pertinent documents within the corpus. The efficacy of the model is then gauged based on its ability to retrieve these relevant documents in response to the queries.\\nThe dataset construction, model fine-tuning, and evaluation phases each present distinct challenges. The LlamaIndex [Liu, 2023] introduces a suite of pivotal classes and functions designed to enhance the embedding model fine-tuning', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 37278}),\n",
       " Document(page_content='workflow, thereby simplifying these intricate processes. By\\ncurating a corpus infused with domain knowledge and leveraging the methodologies offered, one can adeptly fine-tune an\\nembedding model to align closely with the specific requirements of the target domain.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 37740}),\n",
       " Document(page_content='Fine-tuning for Downstream Tasks. Fine-tuning embedding models for downstream tasks is a critical step in enhancing model performance. In the realm of utilizing RAG\\nfor these tasks, innovative methods have emerged to finetune embedding models by harnessing the capabilities of\\nLLMs. For example, PROMPTAGATOR [Dai et al., 2022]', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 38006}),\n",
       " Document(page_content='LLMs. For example, PROMPTAGATOR [Dai et al., 2022]\\nutilizes the LLM as a few-shot query generator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce domains. Another approach, LLM-Embedder [Zhang et al., 2023a], exploits LLMs to generate reward signals for data across multiple downstream tasks. The retriever is fine-tuned with two\\ntypes of supervised signals: hard labels for the dataset and', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 38283}),\n",
       " Document(page_content='types of supervised signals: hard labels for the dataset and\\nsoft rewards from the LLMs. This dual-signal approach fosters a more effective fine-tuning process, tailoring the embedding model to diverse downstream applications.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 38678}),\n",
       " Document(page_content='While these methods improve semantic representation\\nby incorporating domain knowledge and task-specific finetuning, retrievers may not always exhibit optimal compatibility with certain LLMs. To address this, some researchers have\\nexplored direct supervision of the fine-tuning process using\\nfeedback from LLMs. This direct supervision seeks to align\\nthe retriever more closely with the LLM, thereby improving\\nperformance on downstream tasks. A more comprehensive', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 38907}),\n",
       " Document(page_content='performance on downstream tasks. A more comprehensive\\ndiscussion on this topic is presented in Section 4.3.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 39316}),\n",
       " Document(page_content='4.2 Aligning Queries and Documents\\n\\nIn the context of RAG applications, retrievers may utilize\\na single embedding model for encoding both the query and\\nthe documents, or employ separate models for each. Additionally, the user’s original query may suffer from imprecise\\nphrasing and lack of semantic information. Therefore, it is\\ncrucial to align the semantic space of the user’s query with\\nthose of the documents. This section introduces two fundamental techniques aimed at achieving this alignment.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 39425}),\n",
       " Document(page_content='Query Rewriting\\nQuery rewriting is a fundamental approach for aligning\\nthe semantics of a query and a document. Methods\\nsuch as Query2Doc and ITER-RETGEN leverage LLMs\\nto create a pseudo-document by combining the original query with additional guidance [Wang et al., 2023c,\\nShao et al., 2023]. HyDE constructs query vectors using\\ntextual cues to generate a “hypothetical” document capturing essential patterns [Gao et al., 2022]. RRR introduces a', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 39926}),\n",
       " Document(page_content='framework that reverses the traditional retrieval and reading order, focusing on query rewriting [Ma et al., 2023a].\\nSTEP-BACKPROMPTING enables LLMs to perform abstract reasoning and retrieval based on high-level concepts [Zheng et al., 2023]. Additionally, the multi-query retrieval method utilizes LLMs to generate and execute multiple\\nsearch queries simultaneously, advantageous for addressing\\ncomplex problems with multiple sub-problems.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 40373}),\n",
       " Document(page_content='Embedding Transformation\\nBeyond broad strategies such as query rewriting, there exist\\nmore granular techniques specifically designed for embedding transformations. LlamaIndex [Liu, 2023] exemplifies\\nthis by introducing an adapter module that can be integrated\\nfollowing the query encoder. This adapter facilitates finetuning, thereby optimizing the representation of query embeddings to map them into a latent space that is more closely\\naligned with the intended tasks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 40816}),\n",
       " Document(page_content='The challenge of aligning queries with structured external documents, particularly when addressing the incongruity\\nbetween structured and unstructured data, is addressed by\\nSANTA [Li et al., 2023d].\\nIt enhances the retriever’s sensitivity to structured information through two pre-training\\nstrategies: first, by leveraging the intrinsic alignment between\\nstructured and unstructured data to inform contrastive learning in a structured-aware pre-training scheme; and second, by', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 41287}),\n",
       " Document(page_content='implementing Masked Entity Prediction. The latter utilizes\\nan entity-centric masking strategy that encourages language\\nmodels to predict and fill in the masked entities, thereby fostering a deeper understanding of structured data.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 41764}),\n",
       " Document(page_content='The issue of aligning queries with structured external documents, especially when dealing with the disparity between structured and unstructured data, is tackled by\\nSANTA [Li et al., 2023d]. This approach improves the retriever’s ability to recognize structured information through\\ntwo pre-training strategies: firstly, by utilizing the inherent alignment between structured and unstructured data to\\nguide contrastive learning in a structured-aware pre-training', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 41996}),\n",
       " Document(page_content='guide contrastive learning in a structured-aware pre-training\\nscheme; and secondly, by employing Masked Entity Prediction. The latter uses an entity-centric masking strategy to\\nprompt language models to predict and complete the masked\\nentities, thus promoting a more profound comprehension of\\nstructured data.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 42396}),\n",
       " Document(page_content='4.3 Aligning Retriever and LLM\\n\\nIn the RAG pipeline, enhancing retrieval hit rate through various techniques may not necessarily improve the final outcome, as the retrieved documents may not align with the specific requirements of the LLMs. Therefore, this section introduces two methods aimed at aligning the retriever outputs\\nwith the preferences of the LLMs.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 42707}),\n",
       " Document(page_content='Fine-tuning Retrievers\\nSeveral studies utilize feedback signals from LLMs to refine\\nretrieval models. For instance, AAR [Yu et al., 2023b] introduces supervisory signals for a pre-trained retriever using an\\nencoder-decoder architecture. This is achieved by identifying\\nthe LM’s preferred documents through FiD cross-attention\\nscores. Subsequently, the retriever undergoes fine-tuning\\nwith hard negative sampling and standard cross-entropy loss.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 43070}),\n",
       " Document(page_content='with hard negative sampling and standard cross-entropy loss.\\nUltimately, the refined retriever can be directly applied to enhance unseen target LMs, resulting in improved performance\\nin the target task. Additionally, it is suggested that LLMs\\nmay have a preference for focusing on readable rather than\\ninformation-rich documents.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 43454}),\n",
       " Document(page_content='REPLUG [Shi et al., 2023] utilizes a retriever and an LLM\\nto calculate the probability distributions of the retrieved documents and then performs supervised training by computing\\nthe KL divergence. This straightforward and effective training method enhances the performance of the retrieval model\\nby using an LM as the supervisory signal, eliminating the\\nneed for specific cross-attention mechanisms.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 43785}),\n",
       " Document(page_content='UPRISE [Cheng et al., 2023a] also employs frozen LLMs\\nto fine-tune the prompt retriever. Both the LLM and the retriever take prompt-input pairs as inputs and utilize the scores\\nprovided by the LLM to supervise the retriever’s training, effectively treating the LLM as a dataset labeler. In addition,\\nAtlas [Izacard et al., 2022] proposes four methods of supervised fine-tuning embedding models:', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 44187}),\n",
       " Document(page_content='• Attention Distillation. This approach employs crossattention scores generated by the LLM during output to\\ndistill the model’s knowledge.\\n\\n• EMDR2. By using the Expectation-Maximization algorithm, this method trains the model with retrieved documents as latent variables.\\n\\n• Perplexity Distillation directly trains the model using the\\n\\nperplexity of generated tokens as an indicator.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 44583}),\n",
       " Document(page_content='perplexity of generated tokens as an indicator.\\n\\n\\x0c• LOOP. This method presents a novel loss function based\\non the impact of document deletion on LLM prediction,\\noffering an efficient training strategy to better adapt the\\nmodel to specific tasks.\\n\\nThese approaches aim to improve the synergy between the\\nretriever and the LLM, leading to enhanced retrieval performance and more accurate responses to user inquiries.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 44920}),\n",
       " Document(page_content='Adapters\\nFine-tuning models may present challenges, such as integrating functionality through an API or addressing constraints\\narising from limited local computational resources. Consequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 45336}),\n",
       " Document(page_content='PRCA trains the adapter through a context extraction\\nThe retriever’s outphase and a reward-driven phase.\\nput\\nis then optimized using a token-based autoregressive strategy [Yang et al., 2023b]. The token filtering approach employs cross-attention scores to efficiently filter tokens, selecting only the highest-scoring input\\ntokens [Berchansky et al., 2023].RECOMP introduces both extractive and generative compressors for summary generation.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 45606}),\n",
       " Document(page_content='These compressors either select relevant sentences or synthesize document information, creating summaries tailored to\\nmulti-document queries [Xu et al., 2023a].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 46048}),\n",
       " Document(page_content='Furthermore, PKG introduces an innovative method for integrating knowledge into white-box models via directive finetuning [Luo et al., 2023]. In this approach, the retriever module is directly substituted to generate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 46210}),\n",
       " Document(page_content='5 Generation\\nA crucial component of RAG is its generator, which is responsible for converting retrieved information into coherent\\nand fluent text. Unlike traditional language models, RAG’s\\ngenerator sets itself apart by improving accuracy and relevance via the incorporation of retrieved data. In RAG, the\\ngenerator’s input encompasses not only typical contextual information but also relevant text segments obtained through\\nthe retriever. This comprehensive input enables the generator', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 46595}),\n",
       " Document(page_content='the retriever. This comprehensive input enables the generator\\nto gain a deep understanding of the question’s context, resulting in more informative and contextually relevant responses.\\nFurthermore, the generator is guided by the retrieved text to\\nensure coherence between the generated content and the obtained information. The diverse input data has led to targeted\\nefforts during the generation phase, all aimed at refining the\\nadaptation of the large model to the input data derived from', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 47020}),\n",
       " Document(page_content='adaptation of the large model to the input data derived from\\nqueries and documents. In the following subsections, we will\\nexplore the introduction of the generator by delving into aspects of post-retrieval processing and fine-tuning.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 47450}),\n",
       " Document(page_content='5.1 Post-retrieval with Frozen LLM\\nIn the realm of untunable LLMs , many studies rely on wellestablished models like GPT-4 [OpenAI, 2023] to harness\\ntheir comprehensive internal knowledge for systematically\\nsynthesizing retrieved information from various documents.\\n\\nHowever, challenges persist with these large models, including limitations on context length and susceptibility to redundant information. To tackle these issues, certain research endeavors have turned their focus to post-retrieval processing.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 47685}),\n",
       " Document(page_content='Post-retrieval processing involves treating, filtering, or optimizing the relevant information retrieved by the retriever\\nfrom a large document database. Its main goal is to enhance\\nthe quality of retrieval results, aligning them more closely\\nwith user needs or subsequent tasks. It can be viewed as a\\nreprocessing of the documents obtained during the retrieval\\nphase. Common operations in post-retrieval processing typically include information compression and result reranking.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 48196}),\n",
       " Document(page_content='Information Compression\\nThe retriever excels at retrieving relevant information from a\\nvast knowledge base, but managing the substantial amount of\\ninformation within retrieval documents is a challenge. Ongoing research aims to extend the context length of large language models to tackle this issue. However, current large\\nmodels still struggle with context limitations. Therefore,\\nthere are scenarios where condensing information becomes', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 48677}),\n",
       " Document(page_content='there are scenarios where condensing information becomes\\nnecessary. Information condensation is significant for reducing noise, addressing context length restrictions, and enhancing generation effects.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 49059}),\n",
       " Document(page_content='PRCA tackled this issue by training an information extractor [Yang et al., 2023b]. In the context extraction phase,\\nwhen provided with an input text Sinput, it is capable of\\nproducing an output sequence Cextracted that represents the\\ncondensed context from the input document. The training process is designed to minimize the difference between\\nCextracted and the actual context Ctruth.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 49262}),\n",
       " Document(page_content='Similarly, RECOMP adopts a comparable approach by\\ntraining an information condenser using contrastive learning [Xu et al., 2023a]. Each training data point consists of\\none positive sample and five negative samples, and the encoder undergoes training using contrastive loss throughout\\nthis process [Karpukhin et al., 2020] .', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 49650}),\n",
       " Document(page_content='Another study has taken a different approach by aiming to reduce the number of documents in order to imIn the study\\nprove the accuracy of the model’s answers.\\nby [Ma et al., 2023b],\\nthey propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and Small\\nLanguage Models (SLMs). In this paradigm, SLMs serve as', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 49975}),\n",
       " Document(page_content='Language Models (SLMs). In this paradigm, SLMs serve as\\nfilters, while LLMs function as reordering agents. The research shows that instructing LLMs to rearrange challenging samples identified by SLMs leads to significant improvements in various Information Extraction (IE) tasks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 50249}),\n",
       " Document(page_content='Reranking\\nThe re-ranking model is pivotal in optimizing the document\\nset retrieved from the retriever. Language models often face\\nperformance declines when additional context is introduced,\\nand re-ranking effectively addresses this issue. The core concept involves rearranging document records to prioritize the\\nmost relevant items at the top, thereby limiting the total number of documents. This not only resolves the challenge of\\ncontext window expansion during retrieval but also enhances', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 50530}),\n",
       " Document(page_content='context window expansion during retrieval but also enhances\\nretrieval efficiency and responsiveness.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 50962}),\n",
       " Document(page_content='The re-ranking model assumes a dual role throughout\\nthe information retrieval process, functioning as both an\\n\\n\\x0coptimizer and a refiner.\\naccurate input for subsequent\\ning [Zhuang et al., 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 51064}),\n",
       " Document(page_content='It provides more effective and\\nlanguage model process\\nContextual compression is incorporated into the reordering process to offer more precise retrieval information. This\\nmethod entails reducing the content of individual documents\\nand filtering the entire document, with the ultimate goal of\\npresenting the most relevant information in the search results\\nfor a more focused and accurate display of pertinent content.\\n\\n5.2 Fine-tuning LLM for RAG', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 51259}),\n",
       " Document(page_content='5.2 Fine-tuning LLM for RAG\\n\\nOptimizing the generator within the RAG model is a critical\\naspect of its architecture. The generator’s role is to take the\\nretrieved information and produce relevant text, forming the\\nfinal output of the model. The optimization of the generator\\naims to ensure that the generated text is both natural and effectively leverages the retrieved documents to better meet the\\nuser’s query needs.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 51677}),\n",
       " Document(page_content='In standard LLMs generation tasks, the input typically\\nconsists of a query. RAG stands out by incorporating not\\nonly a query but also various retrieved documents (structured/unstructured) by the retriever into the input. This additional information can significantly influence the model’s\\nunderstanding, particularly for smaller models. In such cases,\\nfine-tuning the model to adapt to the input of both query and\\nretrieved documents becomes crucial. Before presenting the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 52097}),\n",
       " Document(page_content='retrieved documents becomes crucial. Before presenting the\\ninput to the fine-tuned model, post-retrieval processing usually occurs for the documents retrieved by the retriever. It is\\nessential to note that the fine-tuning method for the generator in RAG aligns with the general fine-tuning approach for\\nLLMs. In the following, we will briefly describe some representative works involving data (formatted/unformatted) and\\noptimization functions.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 52511}),\n",
       " Document(page_content='General Optimization Process\\nAs part of the general optimization process, the training data\\ntypically consists of input-output pairs, aiming to train the\\nmodel to produce the output y given the input x. In the work\\nof Self-Mem [Cheng et al., 2023b], a traditional\\ntraining\\nprocess is employed, where given the input x, relevant\\ndocuments z are retrieved (selecting Top-1 in the paper), and\\nafter integrating (x, z), the model generates the output y.\\nThe paper utilizes two common paradigms for fine-tuning,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 52957}),\n",
       " Document(page_content='The paper utilizes two common paradigms for fine-tuning,\\nnamely Joint-Encoder and Dual-Encoder [Arora et al., 2023,\\nWang et al., 2022b,\\nLewis et al., 2020, Xia et al., 2019,\\nCai et al., 2021, Cheng et al., 2022].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 53407}),\n",
       " Document(page_content='In the Joint-Encoder paradigm, a standard model based on\\nan encoder-decoder is used. Here, the encoder initially encodes the input, and the decoder, through attention mechanisms, combines the encoded results to generate tokens in', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 53621}),\n",
       " Document(page_content='an autoregressive manner. On the other hand, in the DualEncoder paradigm, the system sets up two independent encoders, with each encoder encoding the input (query, context) and the document, respectively. The resulting outputs undergo bidirectional cross-attention processing by the\\ndecoder in sequence. Both architectures utilize the Transformer [Vaswani et al., 2017] as the foundational block and\\noptimize with Negative Log-Likelihood loss.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 53851}),\n",
       " Document(page_content='Utilizing Contrastive Learning\\nIn the phase of preparing training data for language models, interaction pairs of input and output are usually created.\\nThis traditional method can lead to ”exposure bias,” where\\nthe model is only trained on individual, correct output examples, thus restricting its exposure to a range of possible\\noutputs citesequence. This limitation can hinder the model’s', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 54296}),\n",
       " Document(page_content='outputs citesequence. This limitation can hinder the model’s\\nreal-world performance by causing it to overfit to the particular examples in the training set, thereby reducing its ability\\nto generalize across various contexts.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 54625}),\n",
       " Document(page_content='To mitigate exposure bias, SURGE [Kang et al., 2023]\\nproposes the use of graph-text contrastive learning. This\\nmethod includes a contrastive learning objective that prompts\\nthe model to produce a range of plausible and coherent responses, expanding beyond the instances encountered in the\\ntraining data. This approach is crucial in reducing overfitting\\nand strengthening the model’s ability to generalize.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 54851}),\n",
       " Document(page_content='For retrieval tasks that engage with structured data, the\\nSANTA framework [Li et al., 2023d] implements a tripartite\\ntraining regimen to effectively encapsulate both structural and\\nsemantic nuances. The initial phase focuses on the retriever,\\nwhere contrastive learning is harnessed to refine the query\\nand document embeddings.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 55258}),\n",
       " Document(page_content='Subsequently, the generator’s preliminary training stage\\nemploys contrastive learning to align the structured data with\\nits unstructured document descriptions. In a further stage of\\ngenerator training, the model acknowledges the critical role\\nof entity semantics in the representation learning of textual\\ndata for retrieval, as highlighted by [Sciavolino et al., 2021,\\nZhang et al., 2019]. This process commences with the identification of entities within the structured data, followed by the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 55587}),\n",
       " Document(page_content='application of masks over these entities within the generator’s\\ninput data, thus setting the stage for the model to anticipate\\nand predict these masked elements.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 56080}),\n",
       " Document(page_content='The training regimen progresses with the model learning\\nto reconstruct the masked entities by leveraging contextual\\ninformation. This exercise cultivates the model’s comprehension of the textual data’s structural semantics and facilitates\\nthe alignment of pertinent entities within the structured data.\\nThe overarching optimization goal is to train the language\\nmodel to accurately restore the obscured spans, thereby enriching its understanding of entity semantics [Ye et al., 2020].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 56243}),\n",
       " Document(page_content='6 Augmentation in RAG\\nThis section is structured around three key aspects: the augmentation stage, sources of augmentation data, and the augmentation process. These facets elucidate the critical technologies pivotal to RAG’s development. A taxonomy of\\nRAG’s core components is presented in Figure 4.\\n\\n6.1 RAG in Augmentation Stages\\nRAG, a knowledge-intensive endeavor, incorporates a variety of technical methodologies across the pre-training, finetuning, and inference stages of language model training.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 56729}),\n",
       " Document(page_content='Pre-training Stage\\nDuring the pre-training stage, researchers have investigated\\nmethods to bolster PTMs for open-domain QA through\\n\\n\\x0cFigure 4: Taxonomy of RAG’s core components\\n\\nretrieval-based strategies. The REALM model adopts a structured, interpretable method for knowledge embedding, framing pre-training, and fine-tuning as a retrieve-then-predict\\nworkflow within the masked language model (MLM) framework [Arora et al., 2023] .', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 57235}),\n",
       " Document(page_content='RETRO [Borgeaud et al., 2022] leverages retrieval augmentation for large-scale pre-training from scratch, achieving\\na reduction in model parameters while surpassing standard\\nGPT models in terms of perplexity. RETRO distinguishes itself with an additional encoder designed to process features\\nof entities retrieved from an external knowledge base, building on the foundational structure of GPT models.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 57671}),\n",
       " Document(page_content='Atlas[Izacard et al., 2022] also incorporates a retrieval\\nmechanism into the T5 architecture [Raffel et al., 2020] in\\nboth the pre-training and fine-tuning stages.\\nIt uses a pretrained T5 to initialize the encoder-decoder language model\\nand a pre-trained Contriever for the dense retriever, improving its efficiency for complex language modeling tasks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 58073}),\n",
       " Document(page_content='Furthermore, COG [Lan et al., 2022] introduces a novel\\ntext generation methodology that emulates copying text fragments from pre-existing collections. Utilizing efficient vector\\nsearch tools, COG computes and indexes contextually meaningful representations of text fragments, demonstrating superior performance in domains such as question-answering and\\ndomain adaptation when compared to RETRO.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 58427}),\n",
       " Document(page_content='The advent of scaling laws has catalyzed the growth of\\nmodel parameters, propelling autoregressive models into the\\nmainstream. Researchers are expanding the RAG approach to\\npretrained larger models, with RETRO++ exemplifying this\\ntrend by scaling up the model parameters while preserving or\\nenhancing performance [Wang et al., 2023b].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 58823}),\n",
       " Document(page_content='Empirical evidence underscores marked improvements in\\ntext generation quality, factual accuracy, reduced toxicity,\\nand downstream task proficiency, especially in knowledgeintensive applications like open-domain QA. These results\\nimply that integrating retrieval mechanisms into the pre\\n\\x0ctraining of autoregressive language models constitutes a\\npromising avenue, marrying sophisticated retrieval\\ntechniques with expansive language models to yield more precise\\nand efficient language generation.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 59159}),\n",
       " Document(page_content='The benefits of augmented pre-training include a robust\\nfoundational model that outperforms standard GPT models\\nin perplexity, text generation quality, and task-specific performance, all while utilizing fewer parameters. This method\\nis particularly adept at handling knowledge-intensive tasks\\nand facilitates the development of domain-specific models\\nthrough training on specialized corpora.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 59654}),\n",
       " Document(page_content='Nonetheless, this approach faces challenges such as the\\nnecessity for extensive pre-training datasets and resources,\\nas well as diminished update frequencies with increasing\\nmodel sizes. Despite these hurdles,\\nthe approach offers\\nsignificant advantages in model resilience. Once trained,\\nretrieval-enhanced models can operate independently of external libraries, enhancing generation speed and operational', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 60047}),\n",
       " Document(page_content='efficiency. The potential gains identified render this methodology a compelling subject for ongoing investigation and innovation in artificial intelligence and machine learning.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 60453}),\n",
       " Document(page_content='Fine-tuning Stage\\nRAG and Fine-tuning are powerful\\ntools for enhancing\\nLLMs, and combining the two can meet the needs of more\\nspecific scenarios. On one hand, fine-tuning allows for the\\nretrieval of documents with a unique style, achieving better semantic expression and aligning the differences between\\nqueries and documents. This ensures that the output of the\\nretriever is more aptly suited to the scenario at hand. On\\nthe other hand, fine-tuning can fulfill the generation needs of', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 60632}),\n",
       " Document(page_content='the other hand, fine-tuning can fulfill the generation needs of\\nmaking stylized and targeted adjustments. Furthermore, finetuning can also be used to align the retriever and generator for\\nimproved model synergy.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 61054}),\n",
       " Document(page_content='The main goal of fine-tuning the retriever is to improve\\nthe quality of semantic representations, achieved by directly\\nfine-tuning the Embedding model using a corpus [Liu, 2023].\\nBy aligning the retriever’s capabilities with the preferences of the LLMs through feedback signals, both can\\nbe better coordinated [Yu et al., 2023b, Izacard et al., 2022,\\nYang et al., 2023b, Shi et al., 2023]. Fine-tuning the retriever', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 61267}),\n",
       " Document(page_content='for specific downstream tasks can lead to improved adaptability [cite]. The introduction of task-agnostic fine-tuning\\naims to enhance the retriever’s versatility in multi-task scenarios [Cheng et al., 2023a].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 61683}),\n",
       " Document(page_content='Fine-tuning generator can result', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 61893}),\n",
       " Document(page_content='in outputs that are\\nmore stylized and customized. On one hand,\\nit allows\\nfor specialized adaptation to different input data formats.\\nFor example, fine-tuning LLMs to fit\\nthe structure of\\nknowledge graphs [Kang et al., 2023], the structure of text\\npairs [Kang et al., 2023, Cheng et al., 2023b], and other specific structures [Li et al., 2023d]. On the other hand, by constructing directive datasets, one can demand LLMs to generate specific formats content. For instance, in adaptive or', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 61927}),\n",
       " Document(page_content='iterative retrieval scenarios, LLMs are fine-tuned to generate\\ncontent that will help determine the timing for the next step\\nof action [Jiang et al., 2023b, Asai et al., 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 62414}),\n",
       " Document(page_content='By synergistically fine-tuning both the retriever and the\\ngenerator, we can enhance the model’s generalization capa\\nbilities and avoid overfitting that may arise from training them\\nseparately. However, joint fine-tuning also leads to increased\\nresource consumption. RA-DIT [Lin et al., 2023] presents\\na lightweight, dual-instruction tuning framework that can\\neffectively add retrieval capabilities to any LLMs. The\\nretrieval-enhanced directive fine-tuning updates the LLM,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 62592}),\n",
       " Document(page_content='retrieval-enhanced directive fine-tuning updates the LLM,\\nguiding it to make more efficient use of the information retrieved and to disregard distracting content.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 63007}),\n",
       " Document(page_content='Despite its advantages, fine-tuning has limitations, including the need for specialized datasets for RAG fine-tuning\\nand the requirement for significant computational resources.\\nHowever, this stage allows for customizing models to specific\\nneeds and data formats, potentially reducing resource usage\\ncompared to the pre-training phase while still being able to\\nfine-tune the model’s output style.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 63171}),\n",
       " Document(page_content='In summary, the fine-tuning stage is essential for the adaptation of RAG models to specific tasks, enabling the refinement of both retrievers and generators. This stage enhances\\nthe model’s versatility and adaptability to various tasks, despite the challenges presented by resource and dataset requirements. The strategic fine-tuning of RAG models is\\ntherefore a critical component in the development of efficient\\nand effective retrieval-augmented systems.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 63569}),\n",
       " Document(page_content='Inference Stage\\nThe inference stage in RAG models is crucial, as it involves extensive integration with LLMs. Traditional RAG\\napproaches, also known as Naive RAG, involve incorporating\\nretrieval content at this stage to guide the generation process.\\nTo overcome the limitations of Naive RAG, advanced techniques introduce more contextually rich information during inference. The DSP framework [Khattab et al., 2022]\\nutilizes a sophisticated exchange of natural language text', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 64027}),\n",
       " Document(page_content='utilizes a sophisticated exchange of natural language text\\nbetween fronzen LMs and retrieval models (RMs), enriching the context and thereby improving generation outcomes. The PKG [Luo et al., 2023] method equips LLMs\\nwith a knowledge-guided module that allows for the retrieval\\nof pertinent information without modifying the LMs’ parameters, enabling more complex task execution. CREAICL [Li et al., 2023b] employs a synchronous retrieval of', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 64443}),\n",
       " Document(page_content='cross-lingual knowledge to enhance context, while RECITE [Sun et al., 2022] generates context by sampling paragraphs directly from LLMs.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 64886}),\n",
       " Document(page_content='Further refinement of the RAG process during inference is seen in approaches that cater to tasks necessiITRG [Feng et al., 2023] ittating multi-step reasoning.\\neratively retrieves information to identify the correct reaITERsoning paths, thereby improving task adaptability.\\nRETGEN [Shao et al., 2023]\\nfollows an iterative strategy, merging retrieval and generation in a cyclical process that alternates between “retrieval-enhanced generation”', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 65024}),\n",
       " Document(page_content='and “generation-enhanced retrieval”. For non-knowledgeintensive (NKI) tasks, PGRA [Guo et al., 2023] proposes a\\ntwo-stage framework, starting with a task-agnostic retriever\\nfollowed by a prompt-guided reranker to select and prioritize evidence. In contrast, IRCOT [Trivedi et al., 2022] combines RAG with Chain of Thought (CoT) methodologies, alternating CoT-guided retrievals with retrieval-informed CoT\\nprocesses, significantly boosting GPT-3’s performance across', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 65467}),\n",
       " Document(page_content='various question-answering tasks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 65935}),\n",
       " Document(page_content='In essence, these inference-stage enhancements provide\\nlightweight, cost-effective alternatives that leverage the capabilities of pre-trained models without necessitating further\\ntraining. The principal advantage is maintaining static LLM\\nparameters while supplying contextually relevant information\\nto meet specific task demands. Nevertheless, this approach is\\nnot without limitations, as it requires meticulous data processing and optimization, and is bound by the foundational', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 65970}),\n",
       " Document(page_content='model’s intrinsic capabilities. To address diverse task requirements effectively, this method is often paired with procedural\\noptimization techniques such as step-wise reasoning, iterative\\nretrieval, and adaptive retrieval strategies.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 66450}),\n",
       " Document(page_content='6.2 Augmentation Source\\nThe effectiveness of RAG models is heavily impacted by the\\nselection of data sources for augmentation. Different levels of\\nknowledge and dimensions require distinct processing techniques. They are categorized as unstructured data, structured\\ndata, and content generated by LLMs. The technology tree\\nof representative RAG research with different augmentation\\naspects is depicted in Figure 5. The leaves, colored in three\\ndifferent shades, represent enhancements using various types', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 66686}),\n",
       " Document(page_content='different shades, represent enhancements using various types\\nof data: unstructured data, structured data, and content generated by LLMs. The diagram clearly shows that initially, augmentation was mainly achieved through unstructured data,\\nsuch as pure text. This approach later expanded to include\\nthe use of structured data (e.g. knowledge graph) for further\\nimprovement. More recently, there has been a growing trend', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 67130}),\n",
       " Document(page_content='improvement. More recently, there has been a growing trend\\nin research that utilizes content generated by the LLMs themselves for retrieval and augmentation purposes.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 67490}),\n",
       " Document(page_content='Augmented with Unstructured Data\\nUnstructured text, is gathered from corpora, such as prompt\\ndata for fine-tuning large models [Cheng et al., 2023a] and\\ncross-lingual data [Li et al., 2023b]. Retrieval units vary from\\ntokens (e.g., kNN-LM [Khandelwal et al., 2019]) to phrases\\n(e.g., NPM, COG [Lee et al., 2020, Lan et al., 2022]) and\\ndocument paragraphs, with finer granularities offering precision at the cost of increased retrieval complexity.\\n\\nFLARE [Jiang et al., 2023b]', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 67658}),\n",
       " Document(page_content='introduces an active retrieval approach, triggered by the LM’s generation of lowprobability words. It creates a temporary sentence for document retrieval, then regenerates the sentence with the retrieved context to predict subsequent sentences. RETRO uses\\nthe previous chunk to retrieve the nearest neighbor at the\\nchunk level, combined with the previous chunk’s context, it', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 68135}),\n",
       " Document(page_content='chunk level, combined with the previous chunk’s context, it\\nguides the generation of the next chunk. To preserve causality, the generation of the next block Ci only utilizes the nearest neighbor of the previous block N (Ci−1) and not N (Ci).', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 68450}),\n",
       " Document(page_content='Augmented with Structured Data\\nStructured data, such as knowledge graphs (KGs), provide high-quality context and mitigate model hallucinaRET-LLMs [Modarressi et al., 2023] constructs a\\ntions.\\nknowledge graph memory from past dialogues for future reference. SUGRE [Kang et al., 2023] employs Graph Neural Networks (GNNs) to encode relevant KG subgraphs,\\nensuring consistency between retrieved facts and generated text through multi-modal contrastive learning. Knowl', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 68693}),\n",
       " Document(page_content='edGPT [Wang et al., 2023d] generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness and contextuality.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 69158}),\n",
       " Document(page_content='LLMs-Generated Content in RAG\\nAddressing the limitations of external auxiliary information\\nin RAG, some research has focused on exploiting LLMs’ internal knowledge. SKR [Wang et al., 2023e] classifies questions as known or unknown, applying retrieval enhancement\\nselectively. GenRead [Yu et al., 2022] replaces the retriever', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 69323}),\n",
       " Document(page_content='selectively. GenRead [Yu et al., 2022] replaces the retriever\\nwith an LLM generator, finding that LLM-generated contexts often contain more accurate answers due to better alignment with the pre-training objectives of causal language modeling. Selfmem [Cheng et al., 2023b] iteratively creates an\\nunbounded memory pool with a retrieval-enhanced generator, using a memory selector to choose outputs that serve as\\ndual problems to the original question, thus self-enhancing\\nthe generative model.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 69586}),\n",
       " Document(page_content='These methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model performance and task effectiveness.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 70080}),\n",
       " Document(page_content='6.3 Augmentation Process\\nIn the domain of RAG, the standard practice often involves\\na singular retrieval step followed by generation, which can\\ntermed the “lost\\nlead to inefficiencies. A notable issue,\\nin the middle” phenomenon, arises when a single retrieval\\nyields redundant content that may dilute or contradict essential information, thereby degrading the generation quality [Liu et al., 2023a]. Furthermore, such singular retrieval is', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 70232}),\n",
       " Document(page_content='typically insufficient for complex problems demanding multistep reasoning, as it provides a limited scope of information [Yoran et al., 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 70672}),\n",
       " Document(page_content='As illustrated in Figure 5, to circumvent these challenges,\\ncontemporary research has proposed methods for refining the\\nretrieval process:\\niterative retrieval, recursive retrieval and\\nadaptive retrieval. Iterative retrieval allows the model to engage in multiple retrieval cycles, enhancing the depth and\\nrelevance of the information obtained. Recursive retrieval\\nprocess where the results of one retrieval operation are used\\nas the input for the subsequent retrieval.\\nIt helps to delve', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 70816}),\n",
       " Document(page_content='as the input for the subsequent retrieval.\\nIt helps to delve\\ndeeper into relevant information, particularly when dealing\\nwith complex or multi-step queries. Recursive retrieval is often used in scenarios where a gradual approach is needed to\\nconverge on a final answer, such as in academic research, legal case analysis, or certain types of data mining tasks. Adaptive retrieval, on the other hand, offers a dynamic adjustment', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 71242}),\n",
       " Document(page_content='mechanism, tailoring the retrieval process to the specific demands of varying tasks and contexts.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 71669}),\n",
       " Document(page_content='Iterative Retrieval\\nIterative retrieval in RAG models is a process where documents are repeatedly collected based on the initial query\\nand the text generated thus far, providing a more comprehensive knowledge base for LLMs [Borgeaud et al., 2022,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 71768}),\n",
       " Document(page_content='Arora et al., 2023]. This approach has been shown to enhance the robustness of subsequent answer generation by offering additional contextual references through multiple retrieval iterations. However, it may suffer from semantic discontinuity and the accumulation of irrelevant information, as', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 72015}),\n",
       " Document(page_content='Figure 5: Technology tree of representative RAG research with different augmentation aspects', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 72311}),\n",
       " Document(page_content='it typically relies on a sequence of n tokens to demarcate the\\nboundaries between generated text and retrieved documents.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized. Recursive retrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing sections of a document or lengthy PDF before performing a retrieval based on this summary. Subsequently, a', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 72405}),\n",
       " Document(page_content='secondary retrieval within the document refines the search,\\nembodying the recursive nature of the process. In contrast,\\nmulti-hop retrieval is designed to delve deeper into graphstructured data sources, extracting interconnected information [Li et al., 2023c].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 72872}),\n",
       " Document(page_content='Additionally, some methodologies integrate the steps of reITER-RETGEN [Shao et al., 2023]\\ntrieval and generation.\\nemploys a synergistic approach that leverages “retrievalenhanced generation” alongside “generation-enhanced retrieval” for tasks that necessitate the reproduction of specific\\ninformation. The model harnesses the content required to address the input task as a contextual basis for retrieving pertinent knowledge, which in turn facilitates the generation of', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 73134}),\n",
       " Document(page_content='improved responses in subsequent iterations.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 73605}),\n",
       " Document(page_content='Recursive Retrieval\\nRecursive Retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 73651}),\n",
       " Document(page_content='The process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradually converging on the most pertinent information through a\\nIRCoT [Trivedi et al., 2022] uses chain-offeedback loop.\\nthought to guide the retrieval process and refines the CoT\\nwith the obtained retrieval results. ToC [Kim et al., 2023]\\ncreates a clarification tree that systematically optimizes the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 73793}),\n",
       " Document(page_content='creates a clarification tree that systematically optimizes the\\nambiguous parts in the Query. It can be particularly useful in\\ncomplex search scenarios where the user’s needs are not entirely clear from the outset or where the information sought\\nis highly specialized or nuanced. The recursive nature of the\\nprocess allows for continuous learning and adaptation to the\\nuser’s requirements, often resulting in improved satisfaction\\nwith the search outcomes.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 74206}),\n",
       " Document(page_content='Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare and SelfRAG [Jiang et al., 2023b, Asai et al., 2023], refine the RAG\\nframework by enabling LLMs to actively determine the optimal moments and content for retrieval, thus enhancing the\\nefficiency and relevance of the information sourced.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 74663}),\n",
       " Document(page_content='These methods are part of a broader trend wherein\\nin their operations, as\\nLLMs employ active judgment\\nseen in model agents like AutoGPT, Toolformer, and\\n[Yang et al., 2023c, Schick et al., 2023,\\nGraph-Toolformer', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 74969}),\n",
       " Document(page_content='Zhang, 2023]. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs proactively use\\nretrievers, apply Self-Ask techniques, and employ few-shot\\nprompts to initiate search queries. This proactive stance allows LLMs to decide when to search for necessary information, akin to how an agent utilizes tools.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 75183}),\n",
       " Document(page_content='WebGPT [Nakano et al., 2021] integrates a reinforcement\\nlearning framework to train the GPT-3 model\\nin autonomously using a search engine during text generation.\\nIt navigates this process using special tokens that facilitate actions such as search engine queries, browsing results,\\nand citing references, thereby expanding GPT-3’s capabilities\\nthrough the use of external search engines.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 75525}),\n",
       " Document(page_content='Flare automates timing retrieval by monitoring the confidence of the generation process, as indicated by the probability of generated terms [Jiang et al., 2023b]. When the probability falls below a certain threshold would activates the retrieval system to collect relevant information, thus optimizing\\nthe retrieval cycle.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 75914}),\n",
       " Document(page_content='Self-RAG [Asai et al., 2023] introduces “reflection tokens” that allow the model to introspect its outputs. These\\ntokens come in two varieties: “retrieve” and “critic”. The\\nmodel autonomously decides when to activate retrieval, or\\nalternatively, a predefined threshold may trigger the process. During retrieval, the generator conducts a fragmentlevel beam search across multiple paragraphs to derive the\\nmost coherent sequence. Critic scores are used to update the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 76238}),\n",
       " Document(page_content='most coherent sequence. Critic scores are used to update the\\nsubdivision scores, with the flexibility to adjust these weights\\nduring inference, tailoring the model’s behavior. Self-RAG’s\\ndesign obviates the need for additional classifiers or reliance\\non Natural Language Inference (NLI) models, thus streamlining the decision-making process for when to engage retrieval mechanisms and improving the model’s autonomous\\njudgment capabilities in generating accurate responses.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 76642}),\n",
       " Document(page_content='LLM optimization has received significant attention due to\\nits increasing prevalence. Techniques such as prompt engineering, Fine-Tuning (FT), and RAG each have distinct characteristics, visually represented in Figure 6. While prompt\\nengineering leverages a model’s inherent capabilities, optimizing LLMs often requires the application of both RAG and\\nFT methods. The choice between RAG and FT should be', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 77117}),\n",
       " Document(page_content='FT methods. The choice between RAG and FT should be\\nbased on the specific requirements of the scenario and the inherent properties of each approach. A detailed comparison of\\nRAG and FT is presented in Table 1.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 77469}),\n",
       " Document(page_content='6.4 RAG vs Fine-Tuning\\n\\nRAG is like giving a model a textbook for tailored information retrieval, perfect for specific queries. On the other hand,\\nFT is like a student internalizing knowledge over time, better for replicating specific structures, styles, or formats. FT\\ncan improve model performance and efficiency by reinforcing base model knowledge, adjusting outputs, and teaching\\ncomplex instructions. However, it is not as good for integrating new knowledge or rapidly iterating new use cases.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 77680}),\n",
       " Document(page_content='The two methods, RAG and FT, are not mutually exclusive\\nand can be complementary, augmenting a model’s capabilities at different levels. In some cases, their combined use\\nmay yield optimal performance. The optimization process\\n\\ninvolving RAG and FT can necessitate multiple iterations to\\nachieve satisfactory results.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 78180}),\n",
       " Document(page_content='7 RAG Evaluation\\nThe rapid advancement and growing adoption of RAG in the\\nfield of Natural Language Processing (NLP) have propelled\\nthe evaluation of RAG models to the forefront of research in\\nthe LLMs community. The primary objective of this evaluation is to comprehend and optimize the performance of RAG\\nmodels across diverse application scenarios.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 78499}),\n",
       " Document(page_content='Historically, RAG models assessments have centered\\non their execution in specific downstream tasks. These\\nevaluations employ established metrics suitable to the tasks\\nat hand.\\nFor instance, question answering evaluations\\nrely on EM and F1 scores [Wang et al., 2023a,\\nmight\\nShi et al., 2023, Feng et al., 2023, Ma et al., 2023a], whereas\\nfact-checking tasks often hinge on accuracy as the priIzacard et al., 2022,\\nmary metric', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 78852}),\n",
       " Document(page_content='mary metric\\nShao et al., 2023]. Tools like RALLE, designed for the automatic evaluation of RAG applications, similarly base their assessments on these task-specific metrics [Hoshi et al., 2023].\\nDespite this, there is a notable paucity of research dedicated\\nto evaluating the distinct characteristics of RAG models, with\\nonly a handful of related studies.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 79265}),\n",
       " Document(page_content='[Lewis et al., 2020,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 79622}),\n",
       " Document(page_content='The following section shifts the focus from task-specific\\nevaluation methods and metrics to provide a synthesis of the\\nexisting literature based on their unique attributes. This exploration covers the objectives of RAG evaluation, the aspects\\nalong which these models are assessed, and the benchmarks\\nand tools available for such evaluations. The aim is to offer a\\ncomprehensive overview of RAG model evaluation, outlining\\nthe methodologies that specifically address the unique aspects', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 79644}),\n",
       " Document(page_content='the methodologies that specifically address the unique aspects\\nof these advanced generative systems.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 80067}),\n",
       " Document(page_content='7.1 Evaluation Targets\\nThe assessment of RAG models mainly revolves around two\\nkey components: the retrieval and generation modules. This\\ndivision ensures a thorough evaluation of both the quality of\\ncontext provided and the quality of content produced.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 80169}),\n",
       " Document(page_content='Retrieval Quality\\nEvaluating the retrieval quality is crucial for determining the\\neffectiveness of the context sourced by the retriever component. Standard metrics from the domains of search engines, recommendation systems, and information retrieval\\nsystems are employed to measure the performance of the\\nRAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [Liu, 2023,\\nNguyen, 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 80424}),\n",
       " Document(page_content='Generation Quality\\nThe assessment of generation quality centers on the generator’s capacity to synthesize coherent and relevant answers\\nfrom the retrieved context. This evaluation can be categorized based on the content’s objectives: unlabeled and labeled content. For unlabeled content, the evaluation encompasses the faithfulness, relevance, and non-harmfulness of the\\ngenerated answers. In contrast, for labeled content, the focus is on the accuracy of the information produced by the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 80858}),\n",
       " Document(page_content='Table 1: Comparison between RAG and Fine-Tuning\\n\\nFeature Comparison\\n\\nRAG\\n\\nFine-Tuning\\n\\nKnowledge Updates\\n\\nDirectly updating the retrieval knowledge\\nbase ensures that the information remains\\ncurrent without the need for frequent retraining, making it well-suited for dynamic data\\nenvironments.\\n\\nExternal Knowledge\\n\\nProficient in leveraging external resources,\\nparticularly suitable for accessing documents\\nor other structured/unstructured databases.\\n\\nData Processing', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 81348}),\n",
       " Document(page_content='Data Processing\\n\\nInvolves minimal data processing and handling.\\n\\nStores static data, requiring retraining for\\nknowledge and data updates.\\n\\nCan be utilized to align the externally acquired knowledge from pretraining with large\\nlanguage models, but may be less practical\\nfor frequently changing data sources.\\n\\nDepends on the creation of high-quality\\ndatasets, and limited datasets may not result\\nin significant performance improvements.\\n\\nModel Customization', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 81798}),\n",
       " Document(page_content='Model Customization\\n\\nFocuses on information retrieval and integrating external knowledge but may not fully\\ncustomize model behavior or writing style.\\n\\nAllows adjustments of LLM behavior, writing style, or specific domain knowledge\\nbased on specific tones or terms.\\n\\nInterpretability\\n\\nResponses can be traced back to specific data\\nsources, providing higher interpretability and\\ntraceability.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 82234}),\n",
       " Document(page_content='Similar to a black box, it is not always clear\\nwhy the model reacts a certain way, resulting\\nin relatively lower interpretability.\\n\\nComputational Resources\\n\\nDepends on computational resources to support retrieval strategies and technologies related to databases. Additionally, it requires\\nthe maintenance of external data source integration and updates.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 82626}),\n",
       " Document(page_content='The preparation and curation of high-quality\\ntraining datasets, defining fine-tuning objectives, and providing corresponding computational resources are necessary.\\n\\nLatency Requirements\\n\\nInvolves data retrieval, which may lead to\\nhigher latency.\\n\\nLLM after fine-tuning can respond without\\nretrieval, resulting in lower latency.\\n\\nReducing Hallucinations\\n\\nInherently less prone to hallucinations as\\neach answer is grounded in retrieved evidence.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 82981}),\n",
       " Document(page_content='Can help reduce hallucinations by training\\nthe model based on specific domain data but\\nmay still exhibit hallucinations when faced\\nwith unfamiliar input.\\n\\nEthical and Privacy Issues\\n\\nEthical and privacy concerns arise from the\\nstorage and retrieval of text from external\\ndatabases.\\n\\nEthical and privacy concerns may arise due\\nto sensitive content in the training data.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 83426}),\n",
       " Document(page_content='model [Liu, 2023]. Additionally, both retrieval and generation quality assessments can be conducted through manual\\nor automatic evaluation methods [Liu, 2023, Lan et al., 2022,\\nLeng et al., 2023].\\n\\n7.2 Evaluation Aspects\\nContemporary evaluation practices of RAG models emphasize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 83796}),\n",
       " Document(page_content='Quality Scores\\nQuality scores include context relevance, answer faithThese quality scores\\nfulness, and answer\\n\\nrelevance.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 84249}),\n",
       " Document(page_content='relevance.\\n\\nevaluate the efficiency of the RAG model from different perspectives in the process of information retrieval\\nand generation [Es et al., 2023, Saad-Falcon et al., 2023,\\nJarvis and Allard, 2023]. The quality scores—context relevance, answer faithfulness, and answer relevance—assess the\\nRAG model’s efficiency from various angles throughout the\\ninformation retrieval and generation process [Es et al., 2023,\\nSaad-Falcon et al., 2023, Jarvis and Allard, 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 84360}),\n",
       " Document(page_content='Context Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\n\\nAnswer Faithfulness ensures that the generated answers remain true to the retrieved context, maintaining consistency\\n\\n\\x0cFigure 6: RAG compared with other model optimization methods\\n\\nand avoiding contradictions.\\n\\nquality of generation.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 84831}),\n",
       " Document(page_content='and avoiding contradictions.\\n\\nquality of generation.\\n\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively addressing the core inquiry.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 85184}),\n",
       " Document(page_content='Required Abilities\\nRAG evaluation also encompasses four abilities indicative of\\nits adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness [Chen et al., 2023b, Liu et al., 2023b]. These abilities\\nare critical for the model’s performance under various challenges and complex scenarios, impacting the quality scores.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 85380}),\n",
       " Document(page_content='Noise Robustness appraises the model’s capability to manage noise documents that are question-related but lack substantive information.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 85762}),\n",
       " Document(page_content='Negative Rejection assesses the model’s discernment in refraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\n\\nInformation Integration evaluates the model’s proficiency\\nin synthesizing information from multiple documents to address complex questions.\\n\\nCounterfactual Robustness tests the model’s ability to recognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 85899}),\n",
       " Document(page_content='Context relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 86383}),\n",
       " Document(page_content='The specific metrics for each evaluation aspect are summarized in Table 2. It is essential to recognize that these metrics,\\nderived from related work, are traditional measures and do\\nnot yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to\\nthe nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\n\\n7.3 Evaluation Benchmarks and Tools', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 86631}),\n",
       " Document(page_content='This section delineates the evaluation framework for RAG\\nmodels, comprising benchmark tests and automated evaluation tools. These instruments furnish quantitative metrics that not only gauge RAG model performance but also\\nenhance comprehension of the model’s capabilities across\\nvarious evaluation aspects. Prominent benchmarks such as\\nRGB and RECALL [Chen et al., 2023b, Liu et al., 2023b]', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 87077}),\n",
       " Document(page_content='RGB and RECALL [Chen et al., 2023b, Liu et al., 2023b]\\nfocus on appraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS [Es et al., 2023], ARES [Saad-Falcon et al., 2023], and\\nTruLens8 employ LLMs to adjudicate the quality scores.\\nThese tools and benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in Table 3.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 87413}),\n",
       " Document(page_content='8https://www.trulens.org/trulens eval/core concepts rag triad/\\n\\n\\x0cTable 2: Summary of metrics applicable for evaluation aspects of RAG\\n\\nContext\\nRelevance\\n\\nFaithfulness\\n\\nAnswer\\nRelevance\\n\\nNoise\\nRobustness\\n\\nNegative\\nRejection\\n\\nInformation\\nIntegration\\n\\nCounterfactual\\nRobustness\\n\\nAccuracy\\nEM\\nRecall\\nPrecision\\nR-Rate\\nCosine Similarity\\nHit Rate\\nMRR\\nNDCG\\n\\n✓\\n\\n✓\\n✓\\n\\n✓\\n✓\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n✓\\n\\n✓\\n\\n✓\\n\\n✓\\n\\nTable 3: Summary of evaluation frameworks\\n\\nEvaluation Framework\\n\\nEvaluation Targets\\n\\nEvaluation Aspects', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 87832}),\n",
       " Document(page_content='Evaluation Framework\\n\\nEvaluation Targets\\n\\nEvaluation Aspects\\n\\nQuantitative Metrics\\n\\nRGB†\\n\\nRECALL†\\n\\nRAGAS‡\\n\\nARES‡\\n\\nTruLens‡\\n\\nRetrieval Quality\\nGeneration Quality\\n\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\n\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\n\\nGeneration Quality Counterfactual Robustness R-Rate (Reappearance Rate)\\n\\nRetrieval Quality\\nGeneration Quality\\n\\nRetrieval Quality\\nGeneration Quality\\n\\nRetrieval Quality\\nGeneration Quality', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 88268}),\n",
       " Document(page_content='Retrieval Quality\\nGeneration Quality\\n\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n\\n*\\n*\\nCosine Similarity\\n\\nAccuracy\\nAccuracy\\nAccuracy\\n\\n*\\n*\\n*', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 88697}),\n",
       " Document(page_content='*\\n*\\nCosine Similarity\\n\\nAccuracy\\nAccuracy\\nAccuracy\\n\\n*\\n*\\n*\\n\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\n\\n8 Future Prospects\\n\\nThis section explores three future prospects for RAG: future\\nchallenges, modality expansion, and the RAG ecosystem.\\n\\n8.1 Future Challenges of RAG', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 88882}),\n",
       " Document(page_content='8.1 Future Challenges of RAG\\n\\nDespite the considerable progress in RAG technology, several\\nchallenges persist that warrant in-depth research:', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 89352}),\n",
       " Document(page_content='Context Length. RAG’s efficacy is limited by the context\\nwindow size of Large Language Models (LLMs). Balancing\\nthe trade-off between a window that is too short, risking insufficient information, and one that is too long, risking information dilution, is crucial. With ongoing efforts to expand LLM\\ncontext windows to virtually unlimited sizes, the adaptation\\nof RAG to these changes presents a significant research question [Xu et al., 2023c, Packer et al., 2023, Xiao et al., 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 89495}),\n",
       " Document(page_content='Robustness. The presence of noise or contradictory information during retrieval can detrimentally affect RAG’s out\\nput quality. This situation is figuratively referred to as “Misinformation can be worse than no information at all”.\\nImproving RAG’s resistance to such adversarial or counterfactual inputs is gaining research momentum and has become a\\nkey performance metric [Yu et al., 2023a, Glass et al., 2021,\\nBaek et al., 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 89980}),\n",
       " Document(page_content='Hybrid Approaches (RAG+FT). Combining RAG with\\nfine-tuning is emerging as a leading strategy. Determining the\\noptimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to-end joint training—and\\nhow to harness both parameterized and non-parameterized\\nadvantages are areas ripe for exploration [Lin et al., 2023].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 90413}),\n",
       " Document(page_content='Expanding LLM Roles. Beyond generating final answers,\\nLLMs are leveraged for retrieval and evaluation within RAG\\nframeworks. Identifying ways to further unlock LLMs potential in RAG systems is a growing research direction.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 90759}),\n",
       " Document(page_content='Scaling Laws. While scaling laws [Kaplan et al., 2020] are\\nestablished for LLMs, their applicability to RAG remains un\\n\\x0ccertain. Initial studies [Wang et al., 2023b] have begun to address this, yet the parameter count in RAG models still lags\\nbehind that of LLMs. The possibility of an Inverse Scaling\\nLaw9, where smaller models outperform larger ones, is particularly intriguing and merits further investigation.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 90983}),\n",
       " Document(page_content='Production-Ready RAG. RAG’s practicality and alignment\\nwith engineering requirements have facilitated its adoption.\\nHowever, enhancing retrieval efficiency, improving document\\nrecall in large knowledge bases, and ensuring data security—such as preventing inadvertent disclosure of document\\nsources or metadata by LLMs—are critical engineering challenges that remain to be addressed [Alon et al., 2022].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 91398}),\n",
       " Document(page_content='Modality Extension of RAG\\nRAG has\\ntext-based questiontranscended its\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\n\\ninitial', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 91802}),\n",
       " Document(page_content='initial\\n\\nImage. RA-CM3 [Yasunaga et al., 2022] stands as a pioneering multimodal model of both retrieving and generating\\ntext and images. BLIP-2 [Li et al., 2023a] leverages frozen\\nimage encoders alongside LLMs for efficient visual language\\npre-training, enabling zero-shot image-to-text conversions.\\nThe “Visualize Before You Write” method [Zhu et al., 2022]\\nemploys image generation to steer the LM’s text generation,\\nshowing promise in open-ended text generation tasks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 92041}),\n",
       " Document(page_content='Audio and Video. The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [Zhao et al., 2022]. UEOP marks\\na significant advancement in end-to-end automatic speech\\nrecognition by incorporating external, offline strategies for\\nvoice-to-text conversion [Chan et al., 2023]. Additionally,\\nKNN-based attention fusion leverages audio embeddings and\\nsemantically related text embeddings to refine ASR, thereby', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 92515}),\n",
       " Document(page_content='semantically related text embeddings to refine ASR, thereby\\naccelerating domain adaptation. Vid2Seq augments language\\nmodels with specialized temporal markers, facilitating the\\nprediction of event boundaries and textual descriptions within\\na unified output sequence [Yang et al., 2023a].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 92923}),\n",
       " Document(page_content='Code. RBPS [Nashid et al., 2023] excels in small-scale\\nlearning tasks by retrieving code examples that align with developers’ objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as\\ntest assertion generation and program repair. For structured\\nknowledge, the CoK method [Li et al., 2023c] first extracts\\nfacts pertinent to the input query from a knowledge graph,\\nthen integrates these facts as hints within the input, enhancing', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 93212}),\n",
       " Document(page_content='performance in knowledge graph question-answering tasks.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 93692}),\n",
       " Document(page_content='8.2 Ecosystem of RAG\\nDownstream Tasks and Evaluation\\nRAG has shown considerable promise in enriching language\\nmodels with the capacity to handle intricate queries and produce detailed responses by leveraging extensive knowledge\\nbases. Empirical evidence suggests that RAG excels in a\\nvariety of downstream tasks, including open-ended question\\nanswering and fact verification. The integration of RAG not\\nonly bolsters the precision and relevance of responses but also\\ntheir diversity and depth.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 93750}),\n",
       " Document(page_content='9https://github.com/inverse-scaling/prize\\n\\nThe scalability and versatility of RAG across multiple domains warrant further investigation, particularly in specialized fields such as medicine, law, and education. In these areas, RAG could potentially reduce training costs and enhance\\nperformance compared to traditional fine-tuning approaches\\nin professional domain knowledge question answering.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 94245}),\n",
       " Document(page_content='Concurrently, refining the evaluation framework for RAG\\nis essential to maximize its efficacy and utility across different\\ntasks. This entails the development of nuanced metrics and\\nassessment tools that can gauge aspects such as contextual\\nrelevance, creativity of content, and non-maleficence.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 94640}),\n",
       " Document(page_content='Furthermore, improving the interpretability of RAG-driven\\nmodels continues to be a key goal. Doing so would allow\\nusers to understand the reasoning behind the responses generated by the model, thereby promoting trust and transparency\\nin the use of RAG applications.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 94937}),\n",
       " Document(page_content='Technical Stack\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAGrelated APIs and becoming essential in the realm of LLMs.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 95204}),\n",
       " Document(page_content='Emerging technical stacks, while not as feature-rich as\\nLangChain and LLamaIndex, distinguish themselves with\\nspecialized offerings. For instance, Flowise AI10 prioritizes a\\nlow-code approach, enabling users to deploy AI applications,\\nincluding RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano11, and Cohere Coral12 are also gaining attention for their unique contributions to the field.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 95504}),\n",
       " Document(page_content='In addition to AI-focused providers, traditional software\\nand cloud service providers are expanding their offerings to\\ninclude RAG-centric services. Verba13 from Weaviate is designed for personal assistant applications, while Amazon’s\\nKendra14 provides an intelligent enterprise search service, allowing users to navigate through various content repositories\\nusing built-in connectors. During the evolution of the RAG', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 95940}),\n",
       " Document(page_content='using built-in connectors. During the evolution of the RAG\\ntechnology landscape, there has been a clear divergence towards different specializations, such as: 1) Customization.\\nTailoring RAG to meet a specific requirements. 2) Simplification. Making RAG easier to use, thereby reducing the initial learning curve. 3) Specialization. Refining RAG to serve\\nproduction environments more effectively.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 96299}),\n",
       " Document(page_content='The mutual growth of RAG models and their technical\\nstack is evident; technological advancements consistently establish new standards for the existing infrastructure. In turn,\\nenhancements to the technical stack drive the evolution of\\nRAG capabilities. The RAG toolkit is converging into a foundational technical stack, laying the groundwork for advanced\\nenterprise applications. However, the concept of a fully integrated, comprehensive platform remains on the horizon,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 96697}),\n",
       " Document(page_content='pending further innovation and development.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 97168}),\n",
       " Document(page_content='10https://flowiseai.com\\n11https://meltano.com\\n12https://cohere.com/coral\\n13https://github.com/weaviate/Verba\\n14https://aws.amazon.com/cn/kendra/\\n\\n\\x0cFigure 7: Summary of RAG ecosystem', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 97213}),\n",
       " Document(page_content='9 Conclusion\\nThe summary of this paper, as depicted in Figure 7, highlights RAG’s significant advancement in enhancing the capabilities of LLMs through the integration of parameterized knowledge from language models with extensive nonparameterized data from external knowledge bases. Our survey illustrates the evolution of RAG technologies and their', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 97396}),\n",
       " Document(page_content='impact on knowledge-intensive tasks. Our analysis delineates three developmental paradigms within the RAG framework: Naive, Advanced, and Modular RAG, each marking\\na progressive enhancement over its predecessors. The Advanced RAG paradigm extends beyond the Naive approach\\nby incorporating sophisticated architectural elements, including query rewriting, chunk reranking, and prompt summarization. These innovations have led to a more nuanced and modular architecture that enhances both the performance and the', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 97747}),\n",
       " Document(page_content='interpretability of LLMs. RAG’s technical integration with\\nother AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. In content retrieval, a hybrid methodology that leverages both structured and unstructured data sources is emerging as a trend,\\nproviding a more enriched retrieval process. Cutting-edge research within the RAG framework is exploring novel concepts such as self-retrieval from LLMs and the dynamic timing of information retrieval.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 98258}),\n",
       " Document(page_content='Despite the strides made in RAG technology, research opportunities abound in improving its robustness and its ability to manage extended contexts. RAG’s application scope is\\nalso widening into multimodal domains, adapting its princi\\nples to interpret and process diverse data forms such as images, videos, and code. This expansion underscores RAG’s\\nsignificant practical implications for AI deployment, attracting interest from both academic and industrial sectors. The', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 98759}),\n",
       " Document(page_content='growing ecosystem of RAG is underscored by an increase in\\nRAG-centric AI applications and the ongoing development\\nof supportive tools. However, as RAG’s application landscape expands, there is an imperative need to refine evaluation\\nmethodologies to keep pace with its evolution. Ensuring that\\nperformance assessments remain accurate and representative\\nis crucial for capturing the full extent of RAG’s contributions\\nto the AI research and development community.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 99229}),\n",
       " Document(page_content='References\\n\\n[Alon et al., 2022] Uri Alon, Frank Xu, Junxian He, Sudipta\\nSengupta, Dan Roth, and Graham Neubig.\\nNeurosymbolic language modeling with automaton-augmented\\nretrieval. In International Conference on Machine Learning, pages 468–485. PMLR, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 99693}),\n",
       " Document(page_content='[Anderson et al., 2022] Nathan Anderson, Caleb Wilson,\\nand Stephen D. Richardson. Lingua: Addressing scenarios for live interpretation and automatic dubbing. In Janice Campbell, Stephen Larocca, Jay Marciano, Konstantin\\nSavenkov, and Alex Yanishevsky, editors, Proceedings of\\nthe 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and\\nProviders Track and Government Track), pages 202–209,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 99949}),\n",
       " Document(page_content='Orlando, USA, September 2022. Association for Machine\\nTranslation in the Americas.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 100390}),\n",
       " Document(page_content='[Arora et al., 2023] Daman Arora, Anush Kini, Sayak Ray\\nChowdhury, Nagarajan Natarajan, Gaurav Sinha, and\\nAmit Sharma. Gar-meets-rag paradigm for zero-shot information retrieval. arXiv preprint arXiv:2310.20158, 2023.\\n[Asai et al., 2023] Akari Asai, Zeqiu Wu, Yizhong Wang,\\nAvirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning\\nto retrieve, generate, and critique through self-reflection.\\narXiv preprint arXiv:2310.11511, 2023.\\n\\n[BAAI, 2023] BAAI. Flagembedding. https://github.com/', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 100474}),\n",
       " Document(page_content='[BAAI, 2023] BAAI. Flagembedding. https://github.com/\\n\\nFlagOpen/FlagEmbedding, 2023.\\n\\n[Baek et al., 2023] Jinheon Baek, Soyeong Jeong, Minki\\nKang, Jong C Park, and Sung Ju Hwang. Knowledgeaugmented language model verification. arXiv preprint\\narXiv:2310.12836, 2023.\\n\\n[Berchansky et al., 2023] Moshe Berchansky, Peter Izsak,\\nAvi Caciularu, Ido Dagan, and Moshe Wasserblat. Optimizing retrieval-augmented reader models via token elimination. arXiv preprint arXiv:2310.13682, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 100905}),\n",
       " Document(page_content='[Blagojevi, 2023] Vladimir Blagojevi.\\n\\npipelines in haystack:\\nlostinthemiddleranker.\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5,\\n2023.\\n\\nEnhancing rag\\nIntroducing diversityranker and\\nhttps://towardsdatascience.com/', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 101385}),\n",
       " Document(page_content='[Borgeaud et al., 2022] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\\nMillican, George Bm Van Den Driessche, Jean-Baptiste\\nLespiau, Bogdan Damoc, Aidan Clark, et al. Improving\\nlanguage models by retrieving from trillions of tokens.\\nIn International conference on machine learning, pages\\n2206–2240. PMLR, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 101606}),\n",
       " Document(page_content='[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, et al. Language models are few-shot\\nlearners. Advances in neural information processing systems, 33:1877–1901, 2020.\\n\\n[Cai et al., 2021] Deng Cai, Yan Wang, Huayang Li, Wai\\nNeural machine translation\\narXiv preprint\\n\\nLam, and Lemao Liu.\\nwith monolingual translation memory.\\narXiv:2105.11269, 2021.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 101959}),\n",
       " Document(page_content='[Chan et al., 2023] David M Chan, Shalini Ghosh, Ariya\\nRastrow, and Bj¨orn Hoffmeister. Using external offpolicy speech-to-text mappings in contextual end-toarXiv preprint\\nend automated speech recognition.\\narXiv:2301.02736, 2023.\\n\\n[Chen et al., 2023a] Howard Chen, Ramakanth Pasunuru,\\nJason Weston, and Asli Celikyilmaz. Walking down the\\nmemory maze: Beyond context limit through interactive\\nreading. arXiv preprint arXiv:2310.05029, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 102432}),\n",
       " Document(page_content='[Chen et al., 2023b] Jiawei Chen, Hongyu Lin, Xianpei\\nHan, and Le Sun. Benchmarking large language modarXiv preprint\\nels in retrieval-augmented generation.\\narXiv:2309.01431, 2023.\\n\\n[Cheng et al., 2022] Xin Cheng, Shen Gao, Lemao Liu,\\nDongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories. arXiv preprint\\narXiv:2212.03140, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 102873}),\n",
       " Document(page_content='[Cheng et al., 2023a] Daixuan Cheng, Shaohan Huang,\\nJunyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao\\nSun, Furu Wei, Denvy Deng, and Qi Zhang. Uprise: Universal prompt retrieval for improving zero-shot evaluation.\\narXiv preprint arXiv:2303.08518, 2023.\\n\\n[Cheng et al., 2023b] Xin Cheng, Di Luo, Xiuying Chen,\\nLemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself\\nup: Retrieval-augmented text generation with self memory. arXiv preprint arXiv:2305.02437, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 103240}),\n",
       " Document(page_content='[Cohere, 2023] Cohere. Say goodbye to irrelevant search\\nhttps://txt.cohere.com/\\n\\nresults: Cohere rerank is here.\\nrerank/, 2023.\\n\\n[Dai et al., 2022] Zhuyun Dai, Vincent Y Zhao, Ji Ma,\\nYi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin\\nGuu, Keith B Hall, and Ming-Wei Chang. Promptagator:\\nFew-shot dense retrieval from 8 examples. arXiv preprint\\narXiv:2209.11755, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 103704}),\n",
       " Document(page_content='[Es et al., 2023] Shahul Es, Jithin James, Luis EspinosaAnke, and Steven Schockaert. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint\\narXiv:2309.15217, 2023.\\n\\n[Feng et al., 2023] Zhangyin Feng, Xiaocheng Feng, Dezhi\\nZhao, Maojin Yang, and Bing Qin. Retrieval-generation\\nsynergy augmented large language models. arXiv preprint\\narXiv:2310.05149, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 104073}),\n",
       " Document(page_content='[Gao et al., 2022] Luyu Gao, Xueguang Ma, Jimmy Lin, and\\nJamie Callan. Precise zero-shot dense retrieval without\\nrelevance labels. arXiv preprint arXiv:2212.10496, 2022.\\n[Glass et al., 2021] Michael Glass, Gaetano Rossiello,\\nMd Faisal Mahbub Chowdhury, and Alfio Gliozzo.\\nRobust retrieval augmented generation for zero-shot slot\\nfilling. arXiv preprint arXiv:2108.13934, 2021.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 104453}),\n",
       " Document(page_content='[Google, 2023] Google. Gemini: A family of highly capable\\nmultimodal models. https://goo.gle/GeminiPaper, 2023.\\n[Guo et al., 2023] Zhicheng Guo, Sijie Cheng, Yile Wang,\\nPeng Li, and Yang Liu. Prompt-guided retrieval augmentation for non-knowledge-intensive tasks. arXiv preprint\\narXiv:2305.17653, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 104831}),\n",
       " Document(page_content='[Hendrycks et al., 2020] Dan Hendrycks, Collin Burns,\\nSteven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\\nand Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300,\\n2020.\\n\\n[Hoshi et al., 2023] Yasuto Hoshi, Daisuke Miyashita,\\nYouyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu\\nTorii, and Jun Deguchi. Ralle: A framework for developing and evaluating retrieval-augmented large language\\nmodels. arXiv preprint arXiv:2308.10633, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 105135}),\n",
       " Document(page_content='[Huang et al., 2023] Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan\\n\\n\\x0cCatanzaro. Raven: In-context learning with retrieval augmented encoder-decoder language models. arXiv preprint\\narXiv:2308.07922, 2023.\\n[ILIN, 2023] IVAN ILIN.\\n\\nAdvanced rag techniques:\\nhttps://pub.towardsai.net/\\n\\nan illustrated overview.\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n\\nFew-shot', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 105616}),\n",
       " Document(page_content='Few-shot\\n\\n[Izacard et al., 2022] Gautier Izacard, Patrick Lewis, Maria\\nLomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,\\nlearning with reand Edouard Grave.\\narXiv preprint\\ntrieval augmented language models.\\narXiv:2208.03299, 2022.\\n[Jarvis and Allard, 2023] Colin\\n\\nand\\nJohn AlJarvis\\ntechniques\\nfor maximizing\\nhttps://community.openai.com/', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 106028}),\n",
       " Document(page_content='lard.\\nllm performance.\\nt/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2,\\n2023.\\n\\nA survey of\\n\\n[Jiang et al., 2023a] Huiqiang Jiang, Qianhui Wu, Chin-Yew\\nLin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing\\nprompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 106418}),\n",
       " Document(page_content='[Jiang et al., 2023b] Zhengbao Jiang, Frank F Xu, Luyu\\nGao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming\\nYang, Jamie Callan, and Graham Neubig. Active retrieval\\naugmented generation. arXiv preprint arXiv:2305.06983,\\n2023.\\n\\n[Kandpal et al., 2023] Nikhil Kandpal, Haikang Deng,\\nAdam Roberts, Eric Wallace, and Colin Raffel. Large\\nlanguage models struggle to learn long-tail knowledge.\\nIn International Conference on Machine Learning, pages\\n15696–15707. PMLR, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 106780}),\n",
       " Document(page_content='[Kang et al., 2023] Minki Kang, Jin Myung Kwak, Jinheon\\nBaek, and Sung Ju Hwang. Knowledge graph-augmented\\nlanguage models for knowledge-grounded dialogue generation. arXiv preprint arXiv:2305.18846, 2023.\\n\\n[Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, Tom\\nHenighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\\nScaling laws for neural language models. arXiv preprint\\narXiv:2001.08361, 2020.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 107246}),\n",
       " Document(page_content='[Karpukhin et al., 2020] Vladimir Karpukhin, Barlas O˘guz,\\nSewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval\\narXiv preprint\\nfor open-domain question answering.\\narXiv:2004.04906, 2020.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 107698}),\n",
       " Document(page_content='[Khandelwal et al., 2019] Urvashi Khandelwal, Omer Levy,\\nDan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.\\n[Khattab et al., 2022] Omar Khattab, Keshav Santhanam,\\nXiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\\nnlp. arXiv preprint arXiv:2212.14024, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 107938}),\n",
       " Document(page_content='[Kim et al., 2023] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang.\\nTree\\nof clarifications: Answering ambiguous questions with\\nretrieval-augmented large language models. arXiv preprint\\narXiv:2310.14696, 2023.\\n\\n[Lan et al., 2022] Tian Lan, Deng Cai, Yan Wang, Heyan\\nIn\\nHuang, and Xian-Ling Mao. Copy is all you need.\\nThe Eleventh International Conference on Learning Representations, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 108423}),\n",
       " Document(page_content='[Lee et al., 2020] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang,\\nLearning dense representations of\\nand Danqi Chen.\\nphrases at scale. arXiv preprint arXiv:2012.12624, 2020.\\n[Leng et al., 2023] Quinn Leng, Kasey Uhlenhuth, and\\nllm evaluation\\nhttps://www.databricks.com/blog/\\n\\nAlkis Polyzotis.\\nof rag applications.\\nLLM-auto-eval-best-practices-RAG, 2023.\\n\\nBest practices for', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 108838}),\n",
       " Document(page_content='Best practices for\\n\\n[Lewis et al., 2020] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt¨aschel, et al. Retrieval-augmented generation for\\nknowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474, 2020.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 109184}),\n",
       " Document(page_content='[Li and Li, 2023] Xianming Li and Jing Li. Angle-optimized\\ntext embeddings. arXiv preprint arXiv:2309.12871, 2023.\\n[Li et al., 2023a] Junnan Li, Dongxu Li, Silvio Savarese, and\\nSteven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language\\nmodels. arXiv preprint arXiv:2301.12597, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 109527}),\n",
       " Document(page_content='[Li et al., 2023b] Xiaoqian Li, Ercong Nie, and Sheng\\nLiang. From classification to generation: Insights into\\narXiv preprint\\ncrosslingual retrieval augmented icl.\\narXiv:2311.06595, 2023.\\n\\n[Li et al., 2023c] Xingxuan Li, Ruochen Zhao, Yew Ken\\nChia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Soujanya Poria. Chain of knowledge: A framework for\\ngrounding large language models with structured knowledge bases. arXiv preprint arXiv:2305.13269, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 109859}),\n",
       " Document(page_content='[Li et al., 2023d] Xinze Li, Zhenghao Liu, Chenyan Xiong,\\nShi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware\\nlanguage model pretraining improves dense retrieval on\\nstructured data. arXiv preprint arXiv:2305.19912, 2023.\\n[Liang et al., 2023] Han Liang, Wenqian Zhang, Wenxuan\\nLi, Jingyi Yu, and Lan Xu.\\nIntergen: Diffusion-based\\nmulti-human motion generation under complex interactions. arXiv preprint arXiv:2304.05684, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 110308}),\n",
       " Document(page_content='[Lin et al., 2023] Xi Victoria Lin, Xilun Chen, Mingda\\nChen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al.\\nRa-dit: Retrieval-augmented dual instruction tuning. arXiv\\npreprint arXiv:2310.01352, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 110738}),\n",
       " Document(page_content='[Litman et al., 2020] Ron Litman, Oron Anschel, Shahar\\nTsiper, Roee Litman, Shai Mazor, and R Manmatha. Scatter: selective context attentional scene text recognizer. In\\nproceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11962–11972, 2020.\\n\\n\\x0c[Liu et al., 2023a] Nelson F Liu, Kevin Lin, John Hewitt,\\nAshwin Paranjape, Michele Bevilacqua, Fabio Petroni,\\nand Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172,\\n2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 110996}),\n",
       " Document(page_content='[Liu et al., 2023b] Yi Liu, Lianzhe Huang, Shicheng Li,\\nSishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and\\nXu Sun.\\nRecall: A benchmark for llms robustness\\nagainst external counterfactual knowledge. arXiv preprint\\narXiv:2311.08147, 2023.\\n\\n[Liu, 2023] Jerry Liu.\\n\\nBuilding production-ready rag\\nhttps://www.ai.engineer/summit/schedule/\\n\\napplications.\\nbuilding-production-ready-rag-applications, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 111502}),\n",
       " Document(page_content='[Luo et al., 2023] Ziyang Luo, Can Xu, Pu Zhao, Xiubo\\nGeng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin\\nJiang. Augmented large language models with parametric knowledge guiding. arXiv preprint arXiv:2305.04757,\\n2023.\\n\\n[Ma et al., 2023a] Xinbei Ma, Yeyun Gong, Pengcheng\\nHe, Hai Zhao, and Nan Duan. Query rewriting for\\nretrieval-augmented large language models. arXiv preprint\\narXiv:2305.14283, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 111901}),\n",
       " Document(page_content='[Ma et al., 2023b] Yubo Ma, Yixin Cao, YongChing Hong,\\nand Aixin Sun. Large language model is not a good fewshot information extractor, but a good reranker for hard\\nsamples! ArXiv, abs/2303.08559, 2023.\\n\\n[Modarressi et al., 2023] Ali Modarressi, Ayyoob Imani,\\nMohsen Fayyaz, and Hinrich Sch¨utze. Ret-llm: Towards\\na general read-write memory for large language models.\\narXiv preprint arXiv:2305.14322, 2023.\\n[Nakano et al., 2021] Reiichiro Nakano,', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 112305}),\n",
       " Document(page_content='Jacob Hilton,\\nSuchir Balaji, Jeff Wu, Long Ouyang, Christina Kim,\\nChristopher Hesse, Shantanu Jain, Vineet Kosaraju,\\nWilliam Saunders, et al. Webgpt: Browser-assisted\\nquestion-answering with human feedback. arXiv preprint\\narXiv:2112.09332, 2021.\\n\\n[Nashid et al., 2023] Noor Nashid, Mifta Sintaha, and Ali\\nMesbah. Retrieval-based prompt selection for code-related\\nfew-shot learning. In 2023 IEEE/ACM 45th International\\nConference on Software Engineering (ICSE), pages 2450–\\n2462, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 112754}),\n",
       " Document(page_content='[Nguyen, 2023] Isabelle Nguyen. Evaluating rag part i: How\\nto evaluate document retrieval. https://www.deepset.ai/\\nblog/rag-evaluation-retrieval, 2023.\\n\\n[Nishikawa et al., 2022] Sosuke Nishikawa, Ryokan Ri,\\nIkuya Yamada, Yoshimasa Tsuruoka, and Isao Echizen.\\nEase: Entity-aware contrastive learning of sentence embedding. arXiv preprint arXiv:2205.04260, 2022.\\n\\n[OpenAI, 2023] OpenAI. Gpt-4 technical report. https://cdn.\\n\\nopenai.com/papers/gpt-4.pdf, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 113240}),\n",
       " Document(page_content='openai.com/papers/gpt-4.pdf, 2023.\\n\\n[Packer et al., 2023] Charles Packer, Vivian Fang, Shishir G\\nPatil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards llms as operating systems. arXiv\\npreprint arXiv:2310.08560, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 113663}),\n",
       " Document(page_content='[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam\\nRoberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits\\nof transfer learning with a unified text-to-text transformer.\\nThe Journal of Machine Learning Research, 21(1):5485–\\n5551, 2020.\\n\\n[Ram et al., 2023] Ori Ram, Yoav Levine, Itay Dalmedigos,\\nDor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham.\\nIn-context retrieval-augmented language\\nmodels. arXiv preprint arXiv:2302.00083, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 113899}),\n",
       " Document(page_content='[Raudaschl, 2023] Adrian H. Raudaschl. Forget rag, the\\nfuture is rag-fusion.\\nhttps://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[Saad-Falcon et al., 2023] Jon Saad-Falcon, Omar Khattab,\\nChristopher Potts, and Matei Zaharia. Ares: An automated\\nevaluation framework for retrieval-augmented generation\\nsystems. arXiv preprint arXiv:2311.09476, 2023.\\n\\nSchick,\\n\\n[Schick et al., 2023] Timo', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 114407}),\n",
       " Document(page_content='Schick,\\n\\n[Schick et al., 2023] Timo\\n\\nJane Dwivedi-Yu,\\nRoberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke\\nZettlemoyer, Nicola Cancedda, and Thomas Scialom.\\nToolformer: Language models can teach themselves to\\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\\n\\n[Sciavolino et al., 2021] Christopher Sciavolino, Zexuan\\nZhong, Jinhyuk Lee, and Danqi Chen. Simple entityarXiv\\ncentric questions challenge dense retrievers.\\npreprint arXiv:2109.08535, 2021.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 114792}),\n",
       " Document(page_content='[Shao et al., 2023] Zhihong Shao, Yeyun Gong, Yelong\\nShen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with\\narXiv preprint\\niterative retrieval-generation synergy.\\narXiv:2305.15294, 2023.\\n\\n[Shi et al., 2023] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\\nZettlemoyer, and Wen-tau Yih.\\nReplug: Retrievalaugmented black-box language models. arXiv preprint\\narXiv:2301.12652, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 115244}),\n",
       " Document(page_content='[Srivastava et al., 2022] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya\\nGupta, Adri`a Garriga-Alonso, et al. Beyond the imitation\\ngame: Quantifying and extrapolating the capabilities of\\nlanguage models. arXiv preprint arXiv:2206.04615, 2022.\\n[Sun et al., 2022] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models. arXiv preprint arXiv:2210.01296, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 115709}),\n",
       " Document(page_content='[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin\\nStone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\\nShruti Bhosale, et al. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288,\\n2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 116195}),\n",
       " Document(page_content='[Trivedi et al., 2022] Harsh Trivedi, Niranjan BalasubramaInternian, Tushar Khot, and Ashish Sabharwal.\\nreasoning for\\nleaving retrieval with chain-of-thought\\nknowledge-intensive multi-step questions. arXiv preprint\\narXiv:2212.10509, 2022.\\n\\n\\x0c[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. Advances in neural information processing systems,\\n30, 2017.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 116474}),\n",
       " Document(page_content='[VoyageAI, 2023] VoyageAI. Voyage’s embedding models.\\n\\nhttps://docs.voyageai.com/embeddings/, 2023.\\n\\n[Wang et al., 2019] Alex Wang, Yada Pruksachatkun, Nikita\\nNangia, Amanpreet Singh, Julian Michael, Felix Hill,\\nOmer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing\\nsystems, 32, 2019.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 116952}),\n",
       " Document(page_content='[Wang et al., 2022a] Shuohang Wang, Yichong Xu, Yuwei\\nFang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu,\\nand Michael Zeng. Training data is more valuable than you\\nthink: A simple and effective method by retrieving from\\ntraining data. arXiv preprint arXiv:2203.08773, 2022.\\n[Wang et al., 2022b] Shuohang Wang, Yichong Xu, Yuwei\\nFang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu,\\nand Michael Zeng. Training data is more valuable than\\nyou think: A simple and effective method by retrieving from training data.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 117340}),\n",
       " Document(page_content='In Smaranda Muresan, Preslav\\nNakov, and Aline Villavicencio, editors, Proceedings of\\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3170–\\n3179, Dublin, Ireland, May 2022. Association for Computational Linguistics.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 117851}),\n",
       " Document(page_content='[Wang et al., 2023a] Boxin Wang, Wei Ping, Lawrence\\nMcAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan\\nCatanzaro. Instructretro: Instruction tuning post retrievalaugmented pretraining. arXiv preprint arXiv:2310.07713,\\n2023.\\n\\n[Wang et al., 2023b] Boxin Wang, Wei Ping, Peng Xu,\\nLawrence McAfee, Zihan Liu, Mohammad Shoeybi,\\nYi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao,\\net al. Shall we pretrain autoregressive language models\\nwith retrieval? a comprehensive study. arXiv preprint\\narXiv:2304.06762, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 118123}),\n",
       " Document(page_content='[Wang et al., 2023c] Liang Wang, Nan Yang, and Furu Wei.\\nQuery2doc: Query expansion with large language models.\\narXiv preprint arXiv:2303.07678, 2023.\\n\\n[Wang et al., 2023d] Xintao Wang, Qianwen Yang, Yongting\\nQiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua\\nXiao, and Wei Wang. Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases. arXiv preprint arXiv:2308.11761, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 118630}),\n",
       " Document(page_content='[Wang et al., 2023e] Yile Wang, Peng Li, Maosong Sun,\\nSelf-knowledge guided retrieval augarXiv preprint\\n\\nand Yang Liu.\\nmentation for large language models.\\narXiv:2310.05002, 2023.\\n\\n[Xia et al., 2019] Mengzhou Xia, Guoping Huang, Lemao\\nLiu, and Shuming Shi. Graph based translation memIn Proceedings of\\nory for neural machine translation.\\nthe AAAI conference on artificial intelligence, volume 33,\\npages 7297–7304, 2019.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 119049}),\n",
       " Document(page_content='[Xiao et al., 2023] Guangxuan Xiao, Yuandong Tian, Beidi\\nChen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint\\narXiv:2309.17453, 2023.\\n\\n[Xu et al., 2023a] Fangyuan Xu, Weijia Shi, and Eunsol\\nChoi. Recomp: Improving retrieval-augmented lms with\\ncompression and selective augmentation. arXiv preprint\\narXiv:2310.04408, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 119470}),\n",
       " Document(page_content='[Xu et al., 2023b] Peng Xu, Wei Ping, Xianchao Wu,\\nLawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and\\nBryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023.\\n[Xu et al., 2023c] Peng Xu, Wei Ping, Xianchao Wu,\\nLawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 119845}),\n",
       " Document(page_content='Bryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 120257}),\n",
       " Document(page_content='[Yang et al., 2023a] Antoine Yang,\\n\\nArsha Nagrani,\\nPaul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset,\\nIvan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq:\\nLarge-scale pretraining of a visual language model for\\ndense video captioning. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition,\\npages 10714–10726, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 120366}),\n",
       " Document(page_content='[Yang et al., 2023b] Haoyan Yang, Zhitao Li, Yong Zhang,\\nJianzong Wang, Ning Cheng, Ming Li, and Jing Xiao.\\nPrca: Fitting black-box large language models for retrieval\\nquestion answering via pluggable reward-driven contextual adapter. arXiv preprint arXiv:2310.18347, 2023.\\n[Yang et al., 2023c] Hui Yang, Sifu Yue, and Yunzhong He.\\nAuto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 120716}),\n",
       " Document(page_content='[Yasunaga et al., 2022] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy\\nLiang, Mike Lewis, Luke Zettlemoyer, and Wen-tau\\nYih. Retrieval-augmented multimodal language modeling.\\narXiv preprint arXiv:2211.12561, 2022.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 121160}),\n",
       " Document(page_content='[Ye et al., 2020] Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. Coreferential reasoning learning for language representation.\\narXiv preprint arXiv:2004.06870, 2020.\\n\\n[Yoran et al., 2023] Ori Yoran, Tomer Wolfson, Ori Ram,\\nand Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint\\narXiv:2310.01558, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 121411}),\n",
       " Document(page_content='[Yu et al., 2022] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\\nMichael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063, 2022.\\n\\n[Yu et al., 2023a] Wenhao Yu, Hongming Zhang, Xiaoman\\nPan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chainof-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 121807}),\n",
       " Document(page_content='[Yu et al., 2023b] Zichun Yu, Chenyan Xiong, Shi Yu, and\\nZhiyuan Liu. Augmentation-adapted retriever improves\\ngeneralization of language models as generic plug-in.\\narXiv preprint arXiv:2305.17331, 2023.\\n\\n[Zhang et al., 2019] Zhengyan Zhang, Xu Han, Zhiyuan Liu,\\nXin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced\\nlanguage representation with informative entities. arXiv\\npreprint arXiv:1905.07129, 2019.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 122283}),\n",
       " Document(page_content='[Zhang et al., 2023a] Peitian Zhang, Shitao Xiao, Zheng\\nLiu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to augment large language models. arXiv preprint\\narXiv:2310.07554, 2023.\\n\\n[Zhang et al., 2023b] Yue Zhang, Yafu Li, Leyang Cui, Deng\\nCai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao,\\nYu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:\\nA survey on hallucination in large language models. arXiv\\npreprint arXiv:2309.01219, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 122689}),\n",
       " Document(page_content='[Zhang, 2023] Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augarXiv preprint arXiv:2304.11116,\\nmented by chatgpt.\\n2023.\\n\\n[Zhao et al., 2022] Jinming Zhao, Gholamreza Haffar, and\\nGenerating synthetic speech from\\narXiv preprint\\n\\nEhsan Shareghi.\\nspokenvocab for speech translation.\\narXiv:2210.08174, 2022.\\n[Zheng et al., 2023] Huaixiu', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 123140}),\n",
       " Document(page_content='Swaroop\\nMishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi,\\nQuoc V Le, and Denny Zhou. Take a step back: Evoking\\nreasoning via abstraction in large language models. arXiv\\npreprint arXiv:2310.06117, 2023.\\n\\nSteven Zheng,\\n\\n[Zhu et al., 2022] Wanrong Zhu, An Yan, Yujie Lu, Wenda\\nXu, Xin Eric Wang, Miguel Eckstein, and William Yang\\nImaginationWang.\\narXiv preprint\\nguided open-ended text generation.\\narXiv:2210.03765, 2022.\\n\\nVisualize before you write:', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 123516}),\n",
       " Document(page_content='Visualize before you write:\\n\\n[Zhuang et al., 2023] Shengyao Zhuang, Bing Liu, Bevan\\nKoopman, and Guido Zuccon.\\nOpen-source large\\nlanguage models are strong zero-shot query likeliarXiv preprint\\nhood models for document ranking.\\narXiv:2310.13243, 2023.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 123932})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PDFMinerLoader(\"../documents/2312.10997.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "def clean_text(text):\n",
    "  #text = text.lower()  # Convert to lowercase\n",
    "  #text = re.sub(r'[^a-z0-9\\s-]', '', text)  # Remove non-alphanumeric characters (except space and dash)\n",
    "  text = re.sub(r'-\\n', '', text)  # Remove hyphens at line breaks\n",
    "  return text\n",
    "def preprocess_documents(documents):\n",
    "  for doc in documents:\n",
    "    # Apply basic cleaning\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    # Optionally remove stop words\n",
    "    # cleaned_text = remove_stop_words(cleaned_text)  # Uncomment if you want stop word removal\n",
    "    doc.page_content = cleaned_text\n",
    "  return documents\n",
    "pre_documents = preprocess_documents(documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, \n",
    "                                               chunk_overlap=64, \n",
    "                                               add_start_index=True,\n",
    "                                               length_function = len,\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \"(?<=[\\.?])\", \"(?<=[\\,;])\", \" \"])\n",
    "# 正则表达式正向回顾后发断言 (?<=...) 分割.,?;\n",
    "all_splits = text_splitter.split_documents(pre_documents)\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_loader = PDFMinerLoader(\"../documents/barlowtwins-CXR.pdf\")\n",
    "#documents = pdf_loader.load()\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32, add_start_index=False)\n",
    "#all_splits = text_splitter.split_documents(documents)\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") #e5-small-v2 or base v2 NEED TRY\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\") \n",
    "vectordb = Chroma.from_documents(all_splits, embeddings) \n",
    "retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='3.1 Naive RAG\\nThe Naive RAG research paradigm represents the earliest\\nmethodology, which gained prominence shortly after the\\nwidespread adoption of ChatGPT. The Naive RAG follows a\\ntraditional process that includes indexing, retrieval, and generation. It is also characterized as a “Retrieve-Read” framework [Ma et al., 2023a].', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 12575}),\n",
       " Document(page_content='upon the foundation of Naive RAG by adding “Rewrite” and\\n“Rerank” modules. However, on the whole, modular RAG\\nenjoys greater diversity and flexibility.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 27158}),\n",
       " Document(page_content='Drawbacks in Naive RAG\\nNaive RAG faces significant challenges in three key areas:\\n“Retrieval,” “Generation,” and “Augmentation”.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 14950}),\n",
       " Document(page_content='3 RAG Framework\\nThe RAG research paradigm is continuously evolving, and\\nthis section primarily delineates its progression. We categorize it into three types: Naive RAG, Advanced RAG, and\\nModular RAG. While RAG were cost-effective and surpassed\\nthe performance of the native LLM, they also exhibited several limitations. The development of Advanced RAG and\\nModular RAG was a response to these specific shortcomings\\nin Naive RAG.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 12146}),\n",
       " Document(page_content='4https://www.llamaindex.ai\\n5https://www.langchain.com/\\n6https://haystack.deepset.ai/blog/\\nenhancing-rag-pipelines-in-haystack\\n\\n7https://huggingface.co/BAAI/bge-reranker-large\\n\\n\\x0cFigure 3: Comparison between the three paradigms of RAG\\n\\nis depicted in Figure 3. However, Modular RAG is not standalone. Advanced RAG is a specialized form of modular\\nRAG, and further, Naive RAG itself is a special case of Advanced RAG. The relationship among the three paradigms is\\none of inheritance and development.', metadata={'source': '../documents/2312.10997.pdf', 'start_index': 22733})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is naive rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning\\n\\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸cois Samson3,\\nDianbo Liu2,4\\n\\n1*D´epartement d’informatique et de recherche op´erationnelle, Universit´e\\nde Montr´eal, 2920 chemin de la Tour, Montr´eal, H3T 1J4, QC, Canada.\\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´eal, H2S\\n3H1, QC, Canada.\\n3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ˆIle-de-Montr´eal, 400 Blvd. De Maisonneuve Ouest,\\nMontr´eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National\\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.\\n\\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;\\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\\n\\nAbstract\\n\\nBackground: Chest X-ray imaging based abnormality localization, essential in\\ndiagnosing various diseases, faces significant clinical challenges due to complex\\ninterpretations and the growing workload of radiologists. Recent advances in deep\\nlearning, especially self-supervised learning, offer promising solutions to enhance\\nimage analysis efficiency, accuracy and reliability.\\nThis study aims to improve autonomic abnormality localization performance of\\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\\nsupervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n\\n1\\n\\n\\x0cbackbone pre-trained on ImageNet. This was followed by supervised fine-tuning\\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.\\nResults: Our experiments showed a significant improvement in model perfor-\\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\\nracy of chest X-ray image base abnormality localization, outperforming tradi-\\ntional transfer learning methods. Its ability to adapt to various imaging conditions\\nand regional variations demonstrates the potential of self-supervised learning in\\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\\nparticularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.\\n\\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.\\n\\n1 Introduction\\n\\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\\ntriaging patients and prioritizing the care order due to its convenience and flexibility.\\nEffective mitigation addresses the lack of availability of computed tomography and\\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\\nplex interpretation often requires a highly qualified radiologist to make an accurate\\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\\ning and remote areas, qualified radiologists are insufficient to cope with the increased\\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\\npopulation [4]. This situation necessitates urgently developing and introducing auto-\\nmated technologies like AI-based image analysis tools to aid radiologists in quicker\\nand more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.\\n\\nIn recent years, deep learning models have rapidly advanced in various medi-\\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\\nysis because it can identify and precisely locate the types of anomalies in the images,\\n\\n2\\n\\n\\x0cproviding doctors with more specific and valuable information. However, training\\nthese models requires a large amount of annotated data. These annotations must be\\nperformed by experienced radiologists for CXR images, as well as for most medi-\\ncal images, making such annotated data not only costly, but also rare, with only a\\nvery limited number of public datasets including bounding box information. Although\\ntransfer learning is widely regarded as an effective method to solve the problem\\nof scarce labelling data, its application in medical image analysis still faces limita-\\ntions. This is mainly due to the significant difference in feature distribution between\\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\\ndatasets. This disparity suggests that directly applying these pre-trained weights to\\nmedical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].\\n\\nTo fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\\nemploying a dual-phase training process to enhance CXR image analysis. The first\\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\\napplied to fine-tune the model. Our experiments show that such a training strategy\\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\\nmodel performance. We observed a 3% increase in model accuracy on the mean\\nAverage Precision benchmark, surpassing the results achieved by directly performing\\nconventional transfer learning from ImageNet pre-trained weights.\\n\\nThis study extends the application of self-supervised learning to chest X-ray abnor-\\nmality localization. It demonstrates the potential of self-supervised learning in medical\\nimaging analysis, especially in the absence of annotated data. By effectively improving\\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\\nresents a significant advancement in the field of CXR abnormality localization, paving\\nthe way for more efficient and accurate diagnostic methods in the future.\\n\\n2 Related Work\\n\\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\\nmodels trained with vast amounts of labelled data are capable of identifying features\\nof various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\\nimages [13]. This achievement is attributed to the advanced image recognition and\\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\\narchitecture which incorporates attention mechanisms, as proposed by Guszt´av Ga´al\\net al. [14], has made significant strides in accurately segmenting lung structures, thus\\naiding in detailed analysis and diagnosis of diseases.\\n\\n3\\n\\n\\x0cSelf-supervised learning has recently gained popularity in the field of medical imag-\\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\\nuseful feature representations by training deep networks on unsupervised data [16].\\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\\ntionships in data by designing innovative pretext tasks, such as image reconstruction\\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\\net al. used large-scale images for self-supervised learning to improve accuracy and con-\\nvergence speed significantly in downstream tasks, achieving better performance than\\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model\\nbased on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].\\n\\nIn terms of transfer learning, applying models trained in one domain to another has\\nled to notable success in medical image analysis. Research indicates that well-processed\\ntransfer results from ImageNet can improve model performance in the medical imag-\\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\\nthe significant difference in feature distribution between medical and natural images,\\nfeatures learned from natural images may not always be broadly applicable to med-\\nical images [23]. Various cross-domain adaptive transfer learning methods have been\\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\\ning and sequential domain adaptation techniques. By tuning model parameters, these\\nmethods can be better adapted to the characteristics of medical images, improving\\nthe performance and accuracy of models in medical image analysis [22].\\n\\n3 Methods\\n\\n3.1 Dataset Selection\\n\\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\\n14 diseases with image-level annotations, including disease location annotations in\\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\\n\\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.\\n\\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\\ning sets. The training set, consisting of 15,000 images, was annotated independently\\nby three radiologists per image. In contrast, the test set, comprising 3,000 images,\\nunderwent a more rigorous annotation process. Initially, each image was independently\\nannotated by three radiologists. This is followed by a secondary review phase where\\nthese initial annotations are reviewed by two other more experienced radiologists, they\\ncommunicated with each other to resolve any disagreements and reach a consensus on\\n\\n4\\n\\n\\x0cFig. 1 Image-level label distribution of the NIH-CXR dataset.\\n\\nthe final labelling. This meticulous process for the test set created a potential dispar-\\nity in data distribution compared to the training set. To eliminate any bias it might\\nintroduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.\\n\\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\\ncalculating the weighted average of each set of duplicate bounding boxes to create a\\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-\\ntion redundancy and improving target area representation in the dataset. Figure 2\\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\\n\\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\\nable dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.\\n\\n3.2 Dual-Phase Training Process\\n\\nOur training encompasses two primary phases: self-supervised pre-training and sub-\\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\\napproach refined the ImageNet pre-trained model by updating its backbone weights.\\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step\\naims to further improve the model’s task-specific performance, explicitly enhancing\\nits capabilities in localized diseases in CXR images.\\n\\n5\\n\\nCountdisease/abnormalityNo FindingInfiltrationEffusionAtelectasisNoduleMassPneumothoraxConsolidationPleural_ThickeningCardiomegalyEmphysemaEdemaFibrosisPneumoniaHernia010000200003000040000500006000070000\\x0cFig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\\npreprocessing.\\n\\n3.2.1 Self-Supervised Pre-training\\n\\nFor the first stage of training, we employed the original version of the Barlow Twins\\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\\nconventional contrastive learning, introducing a self-supervised learning framework\\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\\na straightforward yet potent principle: it learns distinctive features by reducing the\\nrepresentational differences between two differently distorted images from the same\\nsource as processed by the network. This strategy is instrumental in enabling the model\\nto identify unique and rich features in each image while concurrently minimizing the\\noverlap in features. The process involves generating two distinct variants of an image\\nthrough data augmentation, followed by their simultaneous processing via two deep\\n\\n6\\n\\na)b)\\x0cFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with\\nmulti-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\\nfor object detection tasks.\\n\\nneural networks that share identical weights. The objective is to align the network’s\\nweights to enhance the similarity in the high-level representations of these image pairs\\nyet ensure that the individual features remain distinct and independent.\\n\\nThe Barlow Twins method might be particularly useful for medical\\n\\nimaging\\nbecause it extracts features by minimizing the redundancy between representations of\\nperturbed images. In CXR imaging, subtle differences might indicate important health\\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\\nimportant features. In contrast to other contrastive learning algorithms like MoCo\\n[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\\nTwins works at the feature dimension. It aims to assign an independent meaning to\\neach feature dimension. This could lead to a richer feature representation, potentially\\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\\n\\n7\\n\\nInputimageC2C3C4C5P2P3P4P5Resnet50 Backboneconvrpn_clsrpn_regRoI poolingRegion Proposal NetworkFCFCFCFeature mapFeature VectorFeature mapClassBoxFeaturePyramidNetworkXY AY BImagesDistortedimagesZ AZ BResnet 50Resnet 50NetEmbeddingsEmporocalcross-corr.Targetcross-corr.ILBTfeature dimensionBarlow twinsFaster R-CNN\\x0cpathological states). Moreover, compared to self-supervised learning methods requir-\\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\\noffers a more straightforward training framework, which is particularly important in\\nsituations with limited computational resources.\\n\\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\\nfrom the Torchvision library, this step brings no additional cost. We used images from\\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.\\n\\n3.2.2 Fine-tuning Phase\\n\\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with\\nFeature Pyramid Network (FPN) [28] as our object detector and trained it on the\\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\\nR-CNN detector employs these regions to detect and classify targets. This architecture\\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\\nPyramid Network (FPN), an architecture frequently employed in object detection,\\nparticularly enhances performance with multi-scale targets. It integrates high-level\\nsemantic information from deeper layers with detailed information from shallower lay-\\ners, producing feature maps of varied scales that effectively detect differently sized\\ntargets.\\n\\nWe employed the MMdetection [29] machine learning toolbox as the platform for\\nFaster R-CNN, utilizing a number of classical image augmentation techniques and\\nmaintaining consistent hyperparameters across all experiments. Two different input\\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\\non the model’s performance with the pre-trained models. In addition, for comparison,\\nwe also conducted experiments using ImageNet pre-trained weights directly.\\n\\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\\ncomprehensively evaluate the self-supervised learning model’s performance in medical\\nimaging. This method examines the model’s feature transfer capability - its ability to\\nadapt learned representations to new tasks. We first resplit the test set of the NIH\\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\\nand the remaining 20% as an evaluation test set for assessing model performance.\\n\\nWe adopted two distinct strategies during the evaluation: freezing the backbone\\nweights or fine-tuning the weights. In the freezing backbone strategy, we kept the\\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\\nthe entire network, encompassing both the self-supervised trained feature extraction\\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\\n\\n8\\n\\n\\x0cevaluation training set data for training the linear classifier, allowing us to assess the\\nmodel’s performance across different scales of training data.\\n\\nWhen evaluating the representation transfer ability of a self-supervised learning\\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\\ntest sets is consistent. We used the Iterative stratification for the multi-label data\\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\\nand test sets were roughly similar. This helped prevent biases due to uneven label\\ndistribution, making our evaluation results more reliable and convincing.\\n\\n3.3 Results Analysis Process\\n\\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\\nof our object detection models. mAP is a widely recognized and effective metric in\\nobject detection, calculated by averaging precision scores across various object detec-\\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\\nscores for each class. The proportion of correct predictions relative to all predictions\\nfor a specific class across different detection confidence thresholds determines the pre-\\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\\nfor a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.\\n\\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\\nprecision and recall, making it an especially appropriate performance indicator for\\nthis field. The AUC metric represents the area under the Receiver Operating Char-\\nacteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\\nfor imbalanced datasets as it does not rely directly on classification thresholds.\\n\\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\\nlutional layer and observes the impact on the output class scores. This process reveals\\nthe most influential regions for the model’s decision-making. The resulting heat maps\\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.\\n\\n4 Results\\n\\n4.1 Transfer Learning on VinDr Abnormality Localization\\n\\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\\n\\n9\\n\\n\\x0cusing two different input resolutions. Consistent hyperparameter settings were main-\\ntained across all experiments, ensuring that the performance changes were attributable\\nonly to the merits of the pretraining method itself. We visualized the performance of\\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\\non the validation set in Figure 4, and tabulated the corresponding mAP performance\\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\\nperformance baseline without pre-training benefits.\\n\\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\\nbetween the lowest and highest value.\\n\\nTable 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.\\n\\nBackBone weight\\n\\nInput size mAP50 (val set)\\n\\nmAP50 (test set)\\n\\nbaseline nopretrained\\nImageNet pretrained\\nBarlow twins\\nBarlow twins from ImageNet\\n\\n224\\n\\n0.1388 (0.1352,0.1424)\\n0.2245 (0.2204,0.2286)\\n0.2555 (0.2485,0.2626)\\n0.2625 (0.2568,0.2682)\\n\\n0.1342 (0.1306,0.1378)\\n0.2210 (0.2194,0.2226)\\n0.2448 (0.2414,0.2482)\\n0.2502 (0.2476,0.2528)\\n\\nImageNet pretrained\\nBarlow twins from ImageNet\\n\\n640\\n\\n0.2973 (0.2913,0.3033)\\n0.3102 (0.3080,0.3125)\\n\\n0.280 (0.2757,0.2848)\\n0.289 (0.2826,0.2954)\\n\\n1Scores are presented with 95% confidence intervals.\\n\\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\\npre-training in feature representation across disparate image domains.\\n\\n10\\n\\na)b)\\x0cMore strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\n\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\n\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features\\nwith BarlowTwins-CXR, potentially offering more targeted information for clinical\\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\\nderived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.\\n\\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\\nlow Twin-CXR weighted models saw performance improvements due to the increased\\ndetail in the CXR images. Nonetheless, the performance differential between the\\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.\\n\\nThis points to intriguing future research avenues, such as refining image resolu-\\ntion parameters during pre-training and fine-tuning phases and investigating whether\\nhigher-resolution pre-training could elevate model performance. It also accentuates the\\nnecessity of tailoring deep learning model design to specific tasks, considering factors\\nlike image resolution and feature granularity.\\n\\nOverall,\\n\\nimplementing the Barlow Twins-CXR method on the VinDr dataset\\nresulted in substantial gains despite its data limitations and the inherent challenges\\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\\nimage weights for better applicability in CXR image analysis and beyond in medical\\nimaging.\\n\\n4.2 Linear Evaluation Protocol\\n\\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\\ntraditional ImageNet pre-training on the linear classification performance within the\\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone\\nof the linear classifier and updating only the final linear layer’s weights. This approach\\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\\nthese experiments are presented in Figure 6 and Table 2.\\n\\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\\n\\n11\\n\\n\\x0cFig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\\nbelow the heatmaps refer to the image numbers in the dataset.\\n\\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\\nmethods at 224 and 640 input resolutions.\\n\\nModel\\n\\n1%\\n\\n10%\\n\\n100%\\n\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616)\\n0.5932 (0.5913, 0.5951)\\nImage-Net\\n\\n0.7773 (0.7756, 0.7790)\\n0.6855 (0.6822, 0.6889)\\n\\n0.8031 (0.8027, 0.8035)\\n0.7098 (0.7089, 0.7107)\\n\\n1Scores are presented with 95% confidence intervals.\\n\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\n\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\n\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\\nimage analysis, particularly when annotated data is scarce.\\n\\n12\\n\\nGround Truth BboxImageNet WeightsOur method20e27597c972c6e7fdb4d1e7638e227e03431b577d1ccf075e930c4c4913c079fd810298e165ef0b9a88bb25fda7a34bGround Truth BboxImageNet WeightsOur method9eba0d101f410f9cdfae46cb094ae2a687a8df2f22475c7200ebe891d0f25b88ad86f42123384e2441cce36347aa7d1aa)b)\\x0cFig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.\\n\\n4.3 End-to-End Finetuning\\n\\nIn our end-to-end experiments, where we permitted updates to all model layers, the\\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\\nImageNet pre-trained equivalent across all training set sizes. The results of these\\nexperiments are presented in Figure 7 and Table 3.\\n\\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various\\ndataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.\\n\\n13\\n\\n\\x0cTable 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\\npre-training methods at 224 and 640 input resolutions.\\n\\nModel\\n\\n1%\\n\\n10%\\n\\n100%\\n\\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627)\\n0.6163 (0.6110, 0.6216)\\nImage-Net\\n\\n0.7756 (0.7745, 0.7768)\\n0.7168 (0.7093, 0.7243)\\n\\n0.8107 (0.8098, 0.8116)\\n0.7866 (0.7843, 0.7889)\\n\\n1Scores are presented with 95% confidence intervals.\\n\\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\n\\nAUC than the ImageNet counterpart.\\n\\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\\nover the frozen backbone setup was less marked, suggesting that the wealth of features\\nlearned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.\\n\\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning\\nacross all model layers may elevate the risk of overfitting, particularly when data is\\nscarce. The narrowing performance differential between the two pre-training strategies\\nwith increasing data volume indicates that the distinction between domain-specific\\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\\ntial with larger datasets. This trend implies that the influence of the pre-training\\nstrategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.\\n\\n5 Discussion\\n\\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\\nunannotated CXR images for learning valuable representations and enhances trans-\\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\\nThis leads to quicker training and improved performance on tasks like abnormality\\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input\\nresolutions, outshining models pre-trained on ImageNet.\\n\\nOne of the primary limitations of our study is the scarcity of CXR datasets with\\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\\nmay limit the generalizability of our findings. Additionally, the computational cost\\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\\nimages with an image size of 224*224 pixels, the training process required two days\\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\\nability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.\\n\\n6 Future Work\\n\\nOur future endeavours include developing a demo interactive system for deployment\\nand testing in emergency rooms. It will allow practical evaluation of the model’s\\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\\n\\n14\\n\\n\\x0cAdditionally, we plan to explore more advanced self-supervised learning methods,\\nobject detection frameworks, and backbone networks to refine our approach further.\\nThe continuous evolution of these technologies promises to address some of the current\\nlimitations and expand the applicability and accuracy of our model in medical image\\nanalysis.\\n\\n7 Conclusions\\n\\nThe results of this study provide strong support for the application of self-supervised\\nlearning in the field of abnormality detection, especially valuable in environments\\nwhere radiologists face high workloads but the corresponding data labelling resources\\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\\nCXR image, attributable to differences in imaging equipment, patient demographics,\\nand other locale-specific factors [34][35]. Such variations often impede the cross-\\nregional applicability of a model, thus limiting its generalizability. By employing the\\nBarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\\nnetworks to local datasets tailored to the unique characteristics of their regional data.\\nOur findings might also have significant implications for clinical practice, suggest-\\ning that this strategy could be a game-changer in aiding radiologists to interpret\\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\\ntially increasing patients’ throughput and improving the overall quality of care.\\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\\nholds particular promise in areas where standardization of medical imaging presents\\nchallenges.\\n\\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.\\n\\n8 Abbreviations\\n\\nAP: anterior-posterior\\nAUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping\\nCIUSSS: Centre int´egr´e universitaire de sant´e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer\\nFPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic\\nROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate\\n\\n15\\n\\n\\x0cWBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n\\n9 Declarations\\n\\n9.1 Ethics approval and consent to participate\\n\\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\\nrations of Helsinki). The studies reported in this manuscript used reputable public\\ndatasets and did not require any additional data involving human participants, human\\ndata, or human tissue.\\n\\n9.2 Consent for publication\\n\\nNot applicable\\n\\n9.3 Availability of data and materials\\n\\nThe datasets generated and/or analysed during the current study are available in the\\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\\n\\n9.4 Competing interests\\n\\nThe authors declare that they have no competing interests\\n\\n9.5 Funding\\n\\nNo external funding was associated with this research study.\\n\\n9.6 Authors’ contributions\\n\\nHS designed the research methodology, analyzed data, was responsible for experiments\\nand results visualization, and participated in manuscript drafting and revision. LM\\nassisted in developing the research methodology and contributed to the drafting and\\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\\nin statistical analysis. DL contributed to the study design, offered statistical analysis\\nexpertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.\\n\\nAll authors read and approved the final manuscript.\\n\\n9.7 Acknowledgements\\n\\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆıle-de-\\nmontr´eal for the computational resources and support provided, which were essential\\nfor the research conducted as part of the graduate internship program. We are espe-\\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\\nTheir expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.\\n\\n16\\n\\n\\x0cReferences\\n\\n[1] Satia, I., Bashagha, S., Bibi, A., et al.: Assessing the accuracy and certainty in\\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502\\n\\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al.: The role of chest imaging\\nin patient management during the covid-19 pandemic: a multinational consensus\\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978\\n\\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\\nTrend in radiologist workload compared to number of admissions in the emer-\\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337\\n\\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\\n\\ndate-of-access (2022)\\n\\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al.: Effect of a comprehensive\\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:\\na retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054\\n\\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n\\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al.: Transfer learning for medical\\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7\\n\\n[8] Zbontar, J., Jing, L., Misra, I., et al.: Barlow twins: Self-supervised learning\\nvia redundancy reduction. In: Proceedings of the International Conference on\\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230\\n\\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\\n\\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al.: Vindr-cxr: An open dataset of chest\\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w\\n\\n17\\n\\n\\x0c[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90\\n\\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225\\n\\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\\n\\n[14] Ga´al, G., Maga, B., Luk´acs, A.: Attention u-net based adversarial architectures\\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304\\n\\n[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045\\n\\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al.: Greedy layer-wise training of deep\\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476\\n\\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al.: Context encoders: Feature\\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278\\n\\n[18] Chen, T., Kornblith, S., Norouzi, M., et al.: A simple framework for contrastive\\nlearning of visual representations. In: Proceedings of the International Conference\\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.\\n3525087\\n\\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728\\n\\n[20] Azizi, S., Mustafa, B., Ryan, F., et al.: Big self-supervised models advance medical\\nimage classification. In: Proceedings of the IEEE/CVF International Conference\\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346\\n\\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n\\n18\\n\\n\\x0crepresentation and transferability of chest x-ray models. In: Medical Imaging with\\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\\n\\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n\\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\\ntransfer learning work for medical images: Feature reuse & other factors. In:\\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901\\n\\n[24] Wang, X., Peng, Y., Lu, L., et al.: Chestx-ray8: Hospital-scale chest x-ray\\ndatabase and benchmarks on weakly-supervised classification and localization\\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\\n\\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\\nfrom different object detection models. Image and Vision Computing 107, 104117\\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\\n\\n[26] He, K., Fan, H., Wu, Y., et al.: Momentum contrast for unsupervised visual\\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975\\n\\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169\\n\\n[28] Lin, T. Y., Doll´ar, P., Girshick, R., et al.: Feature pyramid networks for object\\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106\\n\\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155\\n\\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\\nmizing mutual information across views. In: Proceedings of the 33rd International\\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\\n\\n19\\n\\n\\x0c[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277\\n\\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)\\n\\n[33] Szyma´nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\\nlabel data. Proceedings of the First International Workshop on Learning with\\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756\\n\\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979\\n\\n[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\\n\\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n\\n20\\n\\n\\x0c', metadata={'source': '../documents/barlowtwins-CXR.pdf'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 0}),\n",
       " Document(page_content='Haoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸cois Samson3,\\nDianbo Liu2,4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 101}),\n",
       " Document(page_content='1*D´epartement d’informatique et de recherche op´erationnelle, Universit´e', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 174}),\n",
       " Document(page_content='de Montr´eal, 2920 chemin de la Tour, Montr´eal, H3T 1J4, QC, Canada.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 249}),\n",
       " Document(page_content='2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´eal, H2S\\n3H1, QC, Canada.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 319}),\n",
       " Document(page_content='3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ˆIle-de-Montr´eal, 400 Blvd. De Maisonneuve Ouest,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 403}),\n",
       " Document(page_content='Montr´eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 525}),\n",
       " Document(page_content='University of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 625}),\n",
       " Document(page_content='*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 709}),\n",
       " Document(page_content='jean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 819}),\n",
       " Document(page_content='Abstract', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 884}),\n",
       " Document(page_content='Background: Chest X-ray imaging based abnormality localization, essential in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 894}),\n",
       " Document(page_content='diagnosing various diseases, faces significant clinical challenges due to complex', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 971}),\n",
       " Document(page_content='interpretations and the growing workload of radiologists. Recent advances in deep', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1053}),\n",
       " Document(page_content='learning, especially self-supervised learning, offer promising solutions to enhance', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1135}),\n",
       " Document(page_content='image analysis efficiency, accuracy and reliability.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1219}),\n",
       " Document(page_content='This study aims to improve autonomic abnormality localization performance of', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1272}),\n",
       " Document(page_content='chest X-ray image analysis, particularly in detecting abnormalities, using a self-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1349}),\n",
       " Document(page_content='supervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1432}),\n",
       " Document(page_content='Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1557}),\n",
       " Document(page_content='a two-stage training process. Initially, self-supervised pre-training was performed', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1630}),\n",
       " Document(page_content='using an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1714}),\n",
       " Document(page_content='1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1791}),\n",
       " Document(page_content='backbone pre-trained on ImageNet. This was followed by supervised fine-tuning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1795}),\n",
       " Document(page_content='on the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1873}),\n",
       " Document(page_content='(FPN). The study employed mean Average Precision (mAP) at an Intersection', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 1946}),\n",
       " Document(page_content='over Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2020}),\n",
       " Document(page_content='evaluation.\\nResults: Our experiments showed a significant improvement in model perfor-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2091}),\n",
       " Document(page_content='mance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2178}),\n",
       " Document(page_content='accuracy compared to traditional ImageNet pre-trained models. In addition, the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2251}),\n",
       " Document(page_content='Ablation CAM method revealed enhanced precision in localizing chest abnormal-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2330}),\n",
       " Document(page_content='ities. The study involved 112,120 images from the NIH dataset and 18,000 images', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2408}),\n",
       " Document(page_content='from the VinDr-CXR dataset, indicating robust training and testing samples.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2488}),\n",
       " Document(page_content='Conclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2564}),\n",
       " Document(page_content='racy of chest X-ray image base abnormality localization, outperforming tradi-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2640}),\n",
       " Document(page_content='tional transfer learning methods. Its ability to adapt to various imaging conditions', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2718}),\n",
       " Document(page_content='and regional variations demonstrates the potential of self-supervised learning in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2803}),\n",
       " Document(page_content='medical diagnostics. This approach can be instrumental in aiding radiologists,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2885}),\n",
       " Document(page_content='particularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 2964}),\n",
       " Document(page_content='Keywords: medical image analysis; chest x-ray; abnormality localization; deep', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3083}),\n",
       " Document(page_content='learning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3161}),\n",
       " Document(page_content='1 Introduction', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3287}),\n",
       " Document(page_content='Chest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3303}),\n",
       " Document(page_content='nosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3386}),\n",
       " Document(page_content='and diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3480}),\n",
       " Document(page_content='triaging patients and prioritizing the care order due to its convenience and flexibility.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3561}),\n",
       " Document(page_content='Effective mitigation addresses the lack of availability of computed tomography and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3651}),\n",
       " Document(page_content='reduces the risk of transmission in the room with the CT scanner [2]. However, its com-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3734}),\n",
       " Document(page_content='plex interpretation often requires a highly qualified radiologist to make an accurate', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3822}),\n",
       " Document(page_content='diagnosis [1]. As the demand for healthcare increases, the workload of radiologists has', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3908}),\n",
       " Document(page_content='significantly increased [3]. It results in less time to analyze each radiographic image,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 3996}),\n",
       " Document(page_content='potentially increasing the risk of diagnostic error. In many areas, especially in develop-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4085}),\n",
       " Document(page_content='ing and remote areas, qualified radiologists are insufficient to cope with the increased', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4176}),\n",
       " Document(page_content='demand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4265}),\n",
       " Document(page_content='while the United Kingdom has 8.5, and Malaysia has approximately 30 per million', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4349}),\n",
       " Document(page_content='population [4]. This situation necessitates urgently developing and introducing auto-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4429}),\n",
       " Document(page_content='mated technologies like AI-based image analysis tools to aid radiologists in quicker', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4515}),\n",
       " Document(page_content='and more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4600}),\n",
       " Document(page_content='In recent years, deep learning models have rapidly advanced in various medi-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4720}),\n",
       " Document(page_content='cal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4797}),\n",
       " Document(page_content='human experts [5]. Object detection plays a more critical role in medical image anal-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4879}),\n",
       " Document(page_content='ysis because it can identify and precisely locate the types of anomalies in the images,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 4965}),\n",
       " Document(page_content='2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5054}),\n",
       " Document(page_content='providing doctors with more specific and valuable information. However, training', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5058}),\n",
       " Document(page_content='these models requires a large amount of annotated data. These annotations must be', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5139}),\n",
       " Document(page_content='performed by experienced radiologists for CXR images, as well as for most medi-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5221}),\n",
       " Document(page_content='cal images, making such annotated data not only costly, but also rare, with only a', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5301}),\n",
       " Document(page_content='very limited number of public datasets including bounding box information. Although', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5384}),\n",
       " Document(page_content='transfer learning is widely regarded as an effective method to solve the problem', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5468}),\n",
       " Document(page_content='of scarce labelling data, its application in medical image analysis still faces limita-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5549}),\n",
       " Document(page_content='tions. This is mainly due to the significant difference in feature distribution between', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5637}),\n",
       " Document(page_content='large datasets (such as ImageNet) used for pre-training models and medical imaging', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5725}),\n",
       " Document(page_content='datasets. This disparity suggests that directly applying these pre-trained weights to', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5808}),\n",
       " Document(page_content='medical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 5894}),\n",
       " Document(page_content='To fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6022}),\n",
       " Document(page_content='employing a dual-phase training process to enhance CXR image analysis. The first', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6101}),\n",
       " Document(page_content='phase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6182}),\n",
       " Document(page_content='images without annotation, starting with an ImageNet [9] pre-trained model as the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6265}),\n",
       " Document(page_content='foundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6347}),\n",
       " Document(page_content='applied to fine-tune the model. Our experiments show that such a training strategy', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6431}),\n",
       " Document(page_content='combining self-supervised pre-training and supervised fine-tuning is particularly effec-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6514}),\n",
       " Document(page_content='tive. In our experiments, while employing ResNet50 [11] as the backbone architecture,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6603}),\n",
       " Document(page_content='we observed that implementing the BarlowTwins-CXR strategy significantly improved', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6689}),\n",
       " Document(page_content='model performance. We observed a 3% increase in model accuracy on the mean', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6771}),\n",
       " Document(page_content='Average Precision benchmark, surpassing the results achieved by directly performing', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6846}),\n",
       " Document(page_content='conventional transfer learning from ImageNet pre-trained weights.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6930}),\n",
       " Document(page_content='This study extends the application of self-supervised learning to chest X-ray abnor-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 6997}),\n",
       " Document(page_content='mality localization. It demonstrates the potential of self-supervised learning in medical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7082}),\n",
       " Document(page_content='imaging analysis, especially in the absence of annotated data. By effectively improving', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7172}),\n",
       " Document(page_content='detection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7260}),\n",
       " Document(page_content='resents a significant advancement in the field of CXR abnormality localization, paving', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7343}),\n",
       " Document(page_content='the way for more efficient and accurate diagnostic methods in the future.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7430}),\n",
       " Document(page_content='2 Related Work', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7505}),\n",
       " Document(page_content='In recent years, deep learning techniques have excelled in the field of medical imaging,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7521}),\n",
       " Document(page_content='particularly in analyzing CXR images. For example, in terms of disease classifica-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7610}),\n",
       " Document(page_content='tion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7693}),\n",
       " Document(page_content='in detecting chest diseases, when benchmarked against the F1 score. Neural network', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7774}),\n",
       " Document(page_content='models trained with vast amounts of labelled data are capable of identifying features', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7857}),\n",
       " Document(page_content='of various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 7943}),\n",
       " Document(page_content='YOLOv7 object detection framework to effectively identify and locate lesions in CXR', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8026}),\n",
       " Document(page_content='images [13]. This achievement is attributed to the advanced image recognition and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8110}),\n",
       " Document(page_content='feature extraction capabilities of neural networks. Additionally, the modified U-net', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8192}),\n",
       " Document(page_content='architecture which incorporates attention mechanisms, as proposed by Guszt´av Ga´al', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8277}),\n",
       " Document(page_content='et al. [14], has made significant strides in accurately segmenting lung structures, thus', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8361}),\n",
       " Document(page_content='aiding in detailed analysis and diagnosis of diseases.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8450}),\n",
       " Document(page_content='3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8506}),\n",
       " Document(page_content='Self-supervised learning has recently gained popularity in the field of medical imag-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8510}),\n",
       " Document(page_content='ing [15] and provides an efficient method for utilizing unlabeled data. Initially proposed', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8596}),\n",
       " Document(page_content='by Bengio et al., this approach allows models to learn from unlabeled data and extract', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8687}),\n",
       " Document(page_content='useful feature representations by training deep networks on unsupervised data [16].', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8774}),\n",
       " Document(page_content='Such learning strategy promotes models to capture the intrinsic structure and rela-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8858}),\n",
       " Document(page_content='tionships in data by designing innovative pretext tasks, such as image reconstruction', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 8942}),\n",
       " Document(page_content='(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9028}),\n",
       " Document(page_content='tasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9114}),\n",
       " Document(page_content='et al. used large-scale images for self-supervised learning to improve accuracy and con-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9203}),\n",
       " Document(page_content='vergence speed significantly in downstream tasks, achieving better performance than', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9292}),\n",
       " Document(page_content='models pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9376}),\n",
       " Document(page_content='based on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9462}),\n",
       " Document(page_content='In terms of transfer learning, applying models trained in one domain to another has', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9560}),\n",
       " Document(page_content='led to notable success in medical image analysis. Research indicates that well-processed', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9644}),\n",
       " Document(page_content='transfer results from ImageNet can improve model performance in the medical imag-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9733}),\n",
       " Document(page_content='ing domain [22]. However, studies by Christos Matsoukas et al. have shown that due to', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9815}),\n",
       " Document(page_content='the significant difference in feature distribution between medical and natural images,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9901}),\n",
       " Document(page_content='features learned from natural images may not always be broadly applicable to med-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 9988}),\n",
       " Document(page_content='ical images [23]. Various cross-domain adaptive transfer learning methods have been', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10070}),\n",
       " Document(page_content='developed to address these challenges, such as unsupervised and semi-supervised learn-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10154}),\n",
       " Document(page_content='ing and sequential domain adaptation techniques. By tuning model parameters, these', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10241}),\n",
       " Document(page_content='methods can be better adapted to the characteristics of medical images, improving', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10324}),\n",
       " Document(page_content='the performance and accuracy of models in medical image analysis [22].', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10406}),\n",
       " Document(page_content='3 Methods\\n\\n3.1 Dataset Selection', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10478}),\n",
       " Document(page_content='This study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10512}),\n",
       " Document(page_content='dataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10593}),\n",
       " Document(page_content='anterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10673}),\n",
       " Document(page_content='14 diseases with image-level annotations, including disease location annotations in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10756}),\n",
       " Document(page_content='some images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10840}),\n",
       " Document(page_content='Meanwhile, the VinDr-CXR dataset is the largest publicly available dataset for', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 10922}),\n",
       " Document(page_content='adult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11001}),\n",
       " Document(page_content='pass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11084}),\n",
       " Document(page_content='The VinDr-CXR dataset exhibits a distinct labelling process for its test and train-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11200}),\n",
       " Document(page_content='ing sets. The training set, consisting of 15,000 images, was annotated independently', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11284}),\n",
       " Document(page_content='by three radiologists per image. In contrast, the test set, comprising 3,000 images,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11369}),\n",
       " Document(page_content='underwent a more rigorous annotation process. Initially, each image was independently', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11454}),\n",
       " Document(page_content='annotated by three radiologists. This is followed by a secondary review phase where', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11540}),\n",
       " Document(page_content='these initial annotations are reviewed by two other more experienced radiologists, they', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11624}),\n",
       " Document(page_content='communicated with each other to resolve any disagreements and reach a consensus on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11712}),\n",
       " Document(page_content='4\\n\\n\\x0cFig. 1 Image-level label distribution of the NIH-CXR dataset.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11796}),\n",
       " Document(page_content='the final labelling. This meticulous process for the test set created a potential dispar-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11863}),\n",
       " Document(page_content='ity in data distribution compared to the training set. To eliminate any bias it might', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 11953}),\n",
       " Document(page_content='introduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12039}),\n",
       " Document(page_content='To improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12167}),\n",
       " Document(page_content='processing technique was applied to the VinDr-CXR training set. The WBF involves', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12250}),\n",
       " Document(page_content='calculating the weighted average of each set of duplicate bounding boxes to create a', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12331}),\n",
       " Document(page_content='single fused bounding box. Such a preprocessing step is crucial for reducing annota-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12416}),\n",
       " Document(page_content='tion redundancy and improving target area representation in the dataset. Figure 2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12501}),\n",
       " Document(page_content='shows the data distribution of VinDr-CXR before and after WBF preprocessing.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12583}),\n",
       " Document(page_content='We chose the VinDr-CXR dataset not only because it is the largest publicly avail-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12661}),\n",
       " Document(page_content='able dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12743}),\n",
       " Document(page_content='3.2 Dual-Phase Training Process', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12863}),\n",
       " Document(page_content='Our training encompasses two primary phases: self-supervised pre-training and sub-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12896}),\n",
       " Document(page_content='sequent supervised fine-tuning. Initially, we commenced with a Resnet50 model', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 12979}),\n",
       " Document(page_content='pre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13057}),\n",
       " Document(page_content='phase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13140}),\n",
       " Document(page_content='approach refined the ImageNet pre-trained model by updating its backbone weights.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13218}),\n",
       " Document(page_content='Subsequently, in the supervised fine-tuning phase, we utilize this refined backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13300}),\n",
       " Document(page_content='within a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13384}),\n",
       " Document(page_content='aims to further improve the model’s task-specific performance, explicitly enhancing', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13467}),\n",
       " Document(page_content='its capabilities in localized diseases in CXR images.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13551}),\n",
       " Document(page_content='5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13606}),\n",
       " Document(page_content='Countdisease/abnormalityNo', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13609}),\n",
       " Document(page_content='FindingInfiltrationEffusionAtelectasisNoduleMassPneumothoraxConsolidationPleural_ThickeningCardiomegalyEmphysemaEdemaFibrosisPn', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13636}),\n",
       " Document(page_content='aEdemaFibrosisPneumoniaHernia010000200003000040000500006000070000\\x0cFig.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13747}),\n",
       " Document(page_content='2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13818}),\n",
       " Document(page_content='preprocessing.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13907}),\n",
       " Document(page_content='3.2.1 Self-Supervised Pre-training', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13923}),\n",
       " Document(page_content='For the first stage of training, we employed the original version of the Barlow Twins', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 13959}),\n",
       " Document(page_content='method, as mentioned in Zbontar et al. [8] This approach represents a shift from', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14045}),\n",
       " Document(page_content='conventional contrastive learning, introducing a self-supervised learning framework', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14126}),\n",
       " Document(page_content='primarily focused on diminishing redundancy. The Barlow Twins method operates on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14210}),\n",
       " Document(page_content='a straightforward yet potent principle: it learns distinctive features by reducing the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14291}),\n",
       " Document(page_content='representational differences between two differently distorted images from the same', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14378}),\n",
       " Document(page_content='source as processed by the network. This strategy is instrumental in enabling the model', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14462}),\n",
       " Document(page_content='to identify unique and rich features in each image while concurrently minimizing the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14550}),\n",
       " Document(page_content='overlap in features. The process involves generating two distinct variants of an image', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14635}),\n",
       " Document(page_content='through data augmentation, followed by their simultaneous processing via two deep', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14722}),\n",
       " Document(page_content='6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14805}),\n",
       " Document(page_content='a)b)\\x0cFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14808}),\n",
       " Document(page_content='the Barlow Twins method in Phase One, where pairs of distorted images are processed through', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14905}),\n",
       " Document(page_content='a shared ResNet50 network to produce embeddings. These are then compared using an empirical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 14997}),\n",
       " Document(page_content='cross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15089}),\n",
       " Document(page_content='dimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15186}),\n",
       " Document(page_content='ResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15281}),\n",
       " Document(page_content='multi-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15377}),\n",
       " Document(page_content='Proposal Network (RPN) that generates object region proposals. The features are then pooled and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15474}),\n",
       " Document(page_content='processed by fully connected (FC) layers to output the final class labels and bounding box coordinates', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15570}),\n",
       " Document(page_content='for object detection tasks.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15673}),\n",
       " Document(page_content='neural networks that share identical weights. The objective is to align the network’s', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15702}),\n",
       " Document(page_content='weights to enhance the similarity in the high-level representations of these image pairs', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15788}),\n",
       " Document(page_content='yet ensure that the individual features remain distinct and independent.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15877}),\n",
       " Document(page_content='The Barlow Twins method might be particularly useful for medical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 15951}),\n",
       " Document(page_content='imaging\\nbecause it extracts features by minimizing the redundancy between representations of', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16017}),\n",
       " Document(page_content='perturbed images. In CXR imaging, subtle differences might indicate important health', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16110}),\n",
       " Document(page_content='information, and the Barlow Twins can effectively capture these subtle but clinically', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16195}),\n",
       " Document(page_content='important features. In contrast to other contrastive learning algorithms like MoCo', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16281}),\n",
       " Document(page_content='[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16364}),\n",
       " Document(page_content='Twins works at the feature dimension. It aims to assign an independent meaning to', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16448}),\n",
       " Document(page_content='each feature dimension. This could lead to a richer feature representation, potentially', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16530}),\n",
       " Document(page_content='better adapted to variations in CXR images (e.g., different imaging conditions and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16618}),\n",
       " Document(page_content='7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16702}),\n",
       " Document(page_content='InputimageC2C3C4C5P2P3P4P5Resnet50 Backboneconvrpn_clsrpn_regRoI poolingRegion Proposal NetworkFCFCFCFeature mapFeature', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16705}),\n",
       " Document(page_content='mapFeature VectorFeature mapClassBoxFeaturePyramidNetworkXY AY BImagesDistortedimagesZ AZ BResnet 50Resnet', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16814}),\n",
       " Document(page_content='50Resnet 50NetEmbeddingsEmporocalcross-corr.Targetcross-corr.ILBTfeature dimensionBarlow twinsFaster R-CNN\\x0cpathological', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 16912}),\n",
       " Document(page_content='states). Moreover, compared to self-supervised learning methods requir-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17032}),\n",
       " Document(page_content='ing negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17104}),\n",
       " Document(page_content='offers a more straightforward training framework, which is particularly important in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17185}),\n",
       " Document(page_content='situations with limited computational resources.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17270}),\n",
       " Document(page_content='We chose to apply Barlow Twins pre-training on the ImageNet pretrained', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17320}),\n",
       " Document(page_content='ResNet50model. Since the ImageNet pre-trained model weights can be easily obtained', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17391}),\n",
       " Document(page_content='from the Torchvision library, this step brings no additional cost. We used images from', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17474}),\n",
       " Document(page_content='the training set portion of the NIH-CXR dataset for this training phase, with the input', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17561}),\n",
       " Document(page_content='image size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17649}),\n",
       " Document(page_content='GPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17731}),\n",
       " Document(page_content='3.2.2 Fine-tuning Phase', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17849}),\n",
       " Document(page_content='In our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17874}),\n",
       " Document(page_content='Feature Pyramid Network (FPN) [28] as our object detector and trained it on the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 17957}),\n",
       " Document(page_content='VinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18037}),\n",
       " Document(page_content='prises two main components: the Region Proposal Network (RPN) [28] and the Fast', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18117}),\n",
       " Document(page_content='R-CNN detector. First, RPN generates candidate regions for objects, and then the Fast', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18197}),\n",
       " Document(page_content='R-CNN detector employs these regions to detect and classify targets. This architecture', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18283}),\n",
       " Document(page_content='renders Faster R-CNN particularly efficient in processing complex images. The Feature', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18370}),\n",
       " Document(page_content='Pyramid Network (FPN), an architecture frequently employed in object detection,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18456}),\n",
       " Document(page_content='particularly enhances performance with multi-scale targets. It integrates high-level', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18536}),\n",
       " Document(page_content='semantic information from deeper layers with detailed information from shallower lay-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18621}),\n",
       " Document(page_content='ers, producing feature maps of varied scales that effectively detect differently sized\\ntargets.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18707}),\n",
       " Document(page_content='We employed the MMdetection [29] machine learning toolbox as the platform for', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18804}),\n",
       " Document(page_content='Faster R-CNN, utilizing a number of classical image augmentation techniques and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18882}),\n",
       " Document(page_content='maintaining consistent hyperparameters across all experiments. Two different input', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 18962}),\n",
       " Document(page_content='sizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19045}),\n",
       " Document(page_content='on the model’s performance with the pre-trained models. In addition, for comparison,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19134}),\n",
       " Document(page_content='we also conducted experiments using ImageNet pre-trained weights directly.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19219}),\n",
       " Document(page_content='We implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19295}),\n",
       " Document(page_content='comprehensively evaluate the self-supervised learning model’s performance in medical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19374}),\n",
       " Document(page_content='imaging. This method examines the model’s feature transfer capability - its ability to', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19459}),\n",
       " Document(page_content='adapt learned representations to new tasks. We first resplit the test set of the NIH', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19546}),\n",
       " Document(page_content='dataset into two parts: 80% as an evaluation training set for training a linear classifier', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19631}),\n",
       " Document(page_content='and the remaining 20% as an evaluation test set for assessing model performance.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19722}),\n",
       " Document(page_content='We adopted two distinct strategies during the evaluation: freezing the backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19804}),\n",
       " Document(page_content='weights or fine-tuning the weights. In the freezing backbone strategy, we kept the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19884}),\n",
       " Document(page_content='parameters of the backbone network (i.e., the feature extraction layers) obtained from', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 19967}),\n",
       " Document(page_content='self-supervised pretraining unchanged. We updated only the weights of the final lin-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20054}),\n",
       " Document(page_content='ear layer. Conversely, under the fine-tuning strategy, we updated parameters across', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20139}),\n",
       " Document(page_content='the entire network, encompassing both the self-supervised trained feature extraction', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20223}),\n",
       " Document(page_content='layers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20308}),\n",
       " Document(page_content='8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20394}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier, allowing us to assess the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20398}),\n",
       " Document(page_content='model’s performance across different scales of training data.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20489}),\n",
       " Document(page_content='When evaluating the representation transfer ability of a self-supervised learning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20552}),\n",
       " Document(page_content='model, it is necessary to ensure that the ratio of individual labels in the training and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20634}),\n",
       " Document(page_content='test sets is consistent. We used the Iterative stratification for the multi-label data', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20723}),\n",
       " Document(page_content='method [32][33] to ensure that the proportions of each label in the evaluation training', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20810}),\n",
       " Document(page_content='and test sets were roughly similar. This helped prevent biases due to uneven label', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20898}),\n",
       " Document(page_content='distribution, making our evaluation results more reliable and convincing.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 20981}),\n",
       " Document(page_content='3.3 Results Analysis Process', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21056}),\n",
       " Document(page_content='For the analysis of results, we employed the mean Average Precision (mAP) at an', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21086}),\n",
       " Document(page_content='Intersection over Union (IoU) of 50% as the benchmark for evaluating the performance', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21166}),\n",
       " Document(page_content='of our object detection models. mAP is a widely recognized and effective metric in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21251}),\n",
       " Document(page_content='object detection, calculated by averaging precision scores across various object detec-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21334}),\n",
       " Document(page_content='tion confidence thresholds. Specifically, mAP is the mean of the average precision', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21422}),\n",
       " Document(page_content='scores for each class. The proportion of correct predictions relative to all predictions', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21505}),\n",
       " Document(page_content='for a specific class across different detection confidence thresholds determines the pre-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21594}),\n",
       " Document(page_content='cision score. In the context of CXR abnormality localization, utilizing mAP at an IoU', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21684}),\n",
       " Document(page_content='of 50% is beneficial for capturing clinically significant lesion detections while allowing', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21770}),\n",
       " Document(page_content='for a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21861}),\n",
       " Document(page_content='Moreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 21964}),\n",
       " Document(page_content='ear evaluation protocol. AUC, a standard metric in medical image analysis, balances', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22042}),\n",
       " Document(page_content='precision and recall, making it an especially appropriate performance indicator for', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22126}),\n",
       " Document(page_content='this field. The AUC metric represents the area under the Receiver Operating Char-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22210}),\n",
       " Document(page_content='acteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22292}),\n",
       " Document(page_content='False Positive Rate (FPR) at various thresholds. This assessment method balances', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22372}),\n",
       " Document(page_content='the model’s sensitivity and specificity, enhancing detection rates while controlling false', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22453}),\n",
       " Document(page_content='positives. Medical image analysis often deals with imbalanced data, and AUC is robust', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22544}),\n",
       " Document(page_content='for imbalanced datasets as it does not rely directly on classification thresholds.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22630}),\n",
       " Document(page_content='Beyond using mAP and AUC for quantitative analysis, our study also utilized the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22714}),\n",
       " Document(page_content='Ablation CAM (Class Activation Mapping) method to create heat maps for qualitative', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22794}),\n",
       " Document(page_content='evaluation. Ablation CAM systematically abates features in the model’s final convo-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22877}),\n",
       " Document(page_content='lutional layer and observes the impact on the output class scores. This process reveals', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 22961}),\n",
       " Document(page_content='the most influential regions for the model’s decision-making. The resulting heat maps', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23049}),\n",
       " Document(page_content='delineate areas of interest in CXR images, providing intuitive visual evidence of how', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23135}),\n",
       " Document(page_content='our BarlowTwins-CXR model focuses on and recognizes abnormalities.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23221}),\n",
       " Document(page_content='4 Results\\n\\n4.1 Transfer Learning on VinDr Abnormality Localization', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23289}),\n",
       " Document(page_content='In this experiment, we examined the efficacy of the ResNet backbone pre-trained by the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23357}),\n",
       " Document(page_content='Barlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23444}),\n",
       " Document(page_content='9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23524}),\n",
       " Document(page_content='using two different input resolutions. Consistent hyperparameter settings were main-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23528}),\n",
       " Document(page_content='tained across all experiments, ensuring that the performance changes were attributable', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23613}),\n",
       " Document(page_content='only to the merits of the pretraining method itself. We visualized the performance of', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23700}),\n",
       " Document(page_content='different models such as Barlow twins-CXR pre-training and ImageNet pre-training', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23786}),\n",
       " Document(page_content='on the validation set in Figure 4, and tabulated the corresponding mAP performance', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23867}),\n",
       " Document(page_content='in Table 1. As depicted in the figure, the baseline model with an untrained ResNet50', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 23950}),\n",
       " Document(page_content='backbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24035}),\n",
       " Document(page_content='performance baseline without pre-training benefits.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24116}),\n",
       " Document(page_content='Fig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24169}),\n",
       " Document(page_content='dataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24259}),\n",
       " Document(page_content='of four(left) and five(right) trials with different random seeds, with shaded areas indicating the range', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24360}),\n",
       " Document(page_content='between the lowest and highest value.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24465}),\n",
       " Document(page_content='Table 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24504}),\n",
       " Document(page_content='BackBone weight\\n\\nInput size mAP50 (val set)\\n\\nmAP50 (test set)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24631}),\n",
       " Document(page_content='baseline nopretrained\\nImageNet pretrained\\nBarlow twins\\nBarlow twins from ImageNet\\n\\n224', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24694}),\n",
       " Document(page_content='224\\n\\n0.1388 (0.1352,0.1424)\\n0.2245 (0.2204,0.2286)\\n0.2555 (0.2485,0.2626)\\n0.2625 (0.2568,0.2682)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24777}),\n",
       " Document(page_content='0.1342 (0.1306,0.1378)\\n0.2210 (0.2194,0.2226)\\n0.2448 (0.2414,0.2482)\\n0.2502 (0.2476,0.2528)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24875}),\n",
       " Document(page_content='ImageNet pretrained\\nBarlow twins from ImageNet\\n\\n640\\n\\n0.2973 (0.2913,0.3033)\\n0.3102 (0.3080,0.3125)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 24968}),\n",
       " Document(page_content='0.280 (0.2757,0.2848)\\n0.289 (0.2826,0.2954)\\n\\n1Scores are presented with 95% confidence intervals.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25068}),\n",
       " Document(page_content='A significant advancement was observed with the ImageNet pre-trained ResNet50,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25167}),\n",
       " Document(page_content='which attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25246}),\n",
       " Document(page_content='pre-training in feature representation across disparate image domains.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25329}),\n",
       " Document(page_content='10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25401}),\n",
       " Document(page_content='a)b)\\x0cMore strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25405}),\n",
       " Document(page_content='formance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25491}),\n",
       " Document(page_content='expedited training trajectory and a significant increase in detection performance.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25573}),\n",
       " Document(page_content='When further enhanced by pre-training from ImageNet, the Barlow Twins-CXR', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25657}),\n",
       " Document(page_content='approach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25731}),\n",
       " Document(page_content='0.2528), evidencing the synergetic effect of combining pre-training methodologies.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25811}),\n",
       " Document(page_content='The heat maps generated from the study present a compelling visualization of the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25895}),\n",
       " Document(page_content='performance of the BarlowTwins-CXR method compared to the traditional ImageNet', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 25976}),\n",
       " Document(page_content='weights approach. We generated heat maps of the first few CXR images of the train-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26055}),\n",
       " Document(page_content='ing and test sets in Figure 5. In each image, our method’s heat maps show a more', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26138}),\n",
       " Document(page_content='focused alignment with the actual lesion areas marked by the Ground Truth Bbox.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26219}),\n",
       " Document(page_content='This indicates a higher precision in localizing and identifying pathological features', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26299}),\n",
       " Document(page_content='with BarlowTwins-CXR, potentially offering more targeted information for clinical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26385}),\n",
       " Document(page_content='diagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26467}),\n",
       " Document(page_content='localization of the heatmaps from BarlowTwins-CXR are visibly superior to those', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26552}),\n",
       " Document(page_content='derived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26632}),\n",
       " Document(page_content='Upon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26744}),\n",
       " Document(page_content='low Twin-CXR weighted models saw performance improvements due to the increased', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26825}),\n",
       " Document(page_content='detail in the CXR images. Nonetheless, the performance differential between the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26904}),\n",
       " Document(page_content='two narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 26984}),\n",
       " Document(page_content='This points to intriguing future research avenues, such as refining image resolu-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27113}),\n",
       " Document(page_content='tion parameters during pre-training and fine-tuning phases and investigating whether', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27195}),\n",
       " Document(page_content='higher-resolution pre-training could elevate model performance. It also accentuates the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27280}),\n",
       " Document(page_content='necessity of tailoring deep learning model design to specific tasks, considering factors', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27368}),\n",
       " Document(page_content='like image resolution and feature granularity.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27457}),\n",
       " Document(page_content='Overall,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27505}),\n",
       " Document(page_content='implementing the Barlow Twins-CXR method on the VinDr dataset', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27515}),\n",
       " Document(page_content='resulted in substantial gains despite its data limitations and the inherent challenges', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27577}),\n",
       " Document(page_content='of CXR abnormality localization. An 11.5% performance enhancement over the base-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27664}),\n",
       " Document(page_content='line and a 2.8% increment over ImageNet pre-trained models were observed on the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27745}),\n",
       " Document(page_content='mAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27825}),\n",
       " Document(page_content='prowess in addressing domain inconsistencies, thereby fine-tuning naturally derived', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27904}),\n",
       " Document(page_content='image weights for better applicability in CXR image analysis and beyond in medical\\nimaging.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 27988}),\n",
       " Document(page_content='4.2 Linear Evaluation Protocol', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28081}),\n",
       " Document(page_content='In this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28113}),\n",
       " Document(page_content='traditional ImageNet pre-training on the linear classification performance within the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28197}),\n",
       " Document(page_content='NIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28283}),\n",
       " Document(page_content='of the linear classifier and updating only the final linear layer’s weights. This approach', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28368}),\n",
       " Document(page_content='was applied across training datasets of varying sizes - 1%, 10%, and 100%, results of', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28459}),\n",
       " Document(page_content='these experiments are presented in Figure 6 and Table 2.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28545}),\n",
       " Document(page_content='The results show that at a training data size of 1%, the Barlow Twins-CXR pre-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28603}),\n",
       " Document(page_content='trained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28682}),\n",
       " Document(page_content='11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28767}),\n",
       " Document(page_content='Fig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28772}),\n",
       " Document(page_content='indicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28874}),\n",
       " Document(page_content='to one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 28968}),\n",
       " Document(page_content='below the heatmaps refer to the image numbers in the dataset.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29068}),\n",
       " Document(page_content='Table 2 AUC scores in validation and test sets for of linear models with varying pre-training', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29131}),\n",
       " Document(page_content='methods at 224 and 640 input resolutions.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29225}),\n",
       " Document(page_content='Model\\n\\n1%\\n\\n10%\\n\\n100%\\n\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616)\\n0.5932 (0.5913, 0.5951)\\nImage-Net', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29268}),\n",
       " Document(page_content='0.7773 (0.7756, 0.7790)\\n0.6855 (0.6822, 0.6889)\\n\\n0.8031 (0.8027, 0.8035)\\n0.7098 (0.7089, 0.7107)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29364}),\n",
       " Document(page_content='1Scores are presented with 95% confidence intervals.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29462}),\n",
       " Document(page_content='CI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29516}),\n",
       " Document(page_content='trained model. As the training data size increased to 10% and 100%, the AUCs for', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29598}),\n",
       " Document(page_content='the Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29679}),\n",
       " Document(page_content='0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29760}),\n",
       " Document(page_content='scored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29842}),\n",
       " Document(page_content='Notably, the incremental gains for both pre-training methods diminished with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29915}),\n",
       " Document(page_content='larger data sizes, suggesting that the performance boost provided by additional data', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 29992}),\n",
       " Document(page_content='becomes marginal when only the linear layer is updated.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30077}),\n",
       " Document(page_content='These findings highlight the Barlow Twins-CXR pre-training method’s superiority', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30134}),\n",
       " Document(page_content='over ImageNet pre-training across various dataset sizes, especially in data-limited sce-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30214}),\n",
       " Document(page_content='narios. This demonstrates the promise of self-supervised learning in enhancing medical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30303}),\n",
       " Document(page_content='image analysis, particularly when annotated data is scarce.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30390}),\n",
       " Document(page_content='12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30451}),\n",
       " Document(page_content='Ground Truth BboxImageNet WeightsOur', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30455}),\n",
       " Document(page_content='WeightsOur method20e27597c972c6e7fdb4d1e7638e227e03431b577d1ccf075e930c4c4913c079fd810298e165ef0b9a88bb25fda7a34bGround Truth', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30481}),\n",
       " Document(page_content='Truth BboxImageNet WeightsOur', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30601}),\n",
       " Document(page_content='WeightsOur method9eba0d101f410f9cdfae46cb094ae2a687a8df2f22475c7200ebe891d0f25b88ad86f42123384e2441cce36347aa7d1aa)b)\\x0cFig. 6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30620}),\n",
       " Document(page_content='6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30743}),\n",
       " Document(page_content='scores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30830}),\n",
       " Document(page_content='(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 30929}),\n",
       " Document(page_content='outperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31024}),\n",
       " Document(page_content='4.3 End-to-End Finetuning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31136}),\n",
       " Document(page_content='In our end-to-end experiments, where we permitted updates to all model layers, the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31163}),\n",
       " Document(page_content='Barlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31246}),\n",
       " Document(page_content='ImageNet pre-trained equivalent across all training set sizes. The results of these', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31323}),\n",
       " Document(page_content='experiments are presented in Figure 7 and Table 3.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31407}),\n",
       " Document(page_content='Fig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31459}),\n",
       " Document(page_content='of models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31558}),\n",
       " Document(page_content='dataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31651}),\n",
       " Document(page_content='consistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31744}),\n",
       " Document(page_content='13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31869}),\n",
       " Document(page_content='Table 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31874}),\n",
       " Document(page_content='pre-training methods at 224 and 640 input resolutions.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 31967}),\n",
       " Document(page_content='Model\\n\\n1%\\n\\n10%\\n\\n100%\\n\\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627)\\n0.6163 (0.6110, 0.6216)\\nImage-Net', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32023}),\n",
       " Document(page_content='0.7756 (0.7745, 0.7768)\\n0.7168 (0.7093, 0.7243)\\n\\n0.8107 (0.8098, 0.8116)\\n0.7866 (0.7843, 0.7889)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32119}),\n",
       " Document(page_content='1Scores are presented with 95% confidence intervals.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32217}),\n",
       " Document(page_content='At a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\n\\nAUC than the ImageNet counterpart.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32271}),\n",
       " Document(page_content='With 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32386}),\n",
       " Document(page_content='approximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32463}),\n",
       " Document(page_content='over the frozen backbone setup was less marked, suggesting that the wealth of features', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32544}),\n",
       " Document(page_content='learned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32631}),\n",
       " Document(page_content='Overall, these end-to-end fine-tuning results suggest that comprehensive learning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32743}),\n",
       " Document(page_content='across all model layers may elevate the risk of overfitting, particularly when data is', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32825}),\n",
       " Document(page_content='scarce. The narrowing performance differential between the two pre-training strategies', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32912}),\n",
       " Document(page_content='with increasing data volume indicates that the distinction between domain-specific', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 32999}),\n",
       " Document(page_content='(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33082}),\n",
       " Document(page_content='tial with larger datasets. This trend implies that the influence of the pre-training', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33163}),\n",
       " Document(page_content='strategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33248}),\n",
       " Document(page_content='5 Discussion', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33354}),\n",
       " Document(page_content='Our study demonstrates that the BarlowTwins-CXR approach effectively utilizes', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33368}),\n",
       " Document(page_content='unannotated CXR images for learning valuable representations and enhances trans-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33446}),\n",
       " Document(page_content='fer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33527}),\n",
       " Document(page_content='This leads to quicker training and improved performance on tasks like abnormality', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33614}),\n",
       " Document(page_content='detection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33696}),\n",
       " Document(page_content='resolutions, outshining models pre-trained on ImageNet.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33777}),\n",
       " Document(page_content='One of the primary limitations of our study is the scarcity of CXR datasets with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33834}),\n",
       " Document(page_content='bounding box. Our reliance on public datasets, due to the absence of a private dataset,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 33915}),\n",
       " Document(page_content='may limit the generalizability of our findings. Additionally, the computational cost', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34003}),\n",
       " Document(page_content='of the BarlowTwins pre-training remains substantial. For a dataset size of 112,120', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34088}),\n",
       " Document(page_content='images with an image size of 224*224 pixels, the training process required two days', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34171}),\n",
       " Document(page_content='on an NVIDIA A100 80G GPU. This significant resource requirement constrained our', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34255}),\n",
       " Document(page_content='ability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34336}),\n",
       " Document(page_content='6 Future Work', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34447}),\n",
       " Document(page_content='Our future endeavours include developing a demo interactive system for deployment', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34462}),\n",
       " Document(page_content='and testing in emergency rooms. It will allow practical evaluation of the model’s', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34544}),\n",
       " Document(page_content='effectiveness in a clinical setting and facilitate the collection of a proprietary dataset.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34626}),\n",
       " Document(page_content='14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34719}),\n",
       " Document(page_content='Additionally, we plan to explore more advanced self-supervised learning methods,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34724}),\n",
       " Document(page_content='object detection frameworks, and backbone networks to refine our approach further.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34805}),\n",
       " Document(page_content='The continuous evolution of these technologies promises to address some of the current', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34888}),\n",
       " Document(page_content='limitations and expand the applicability and accuracy of our model in medical image\\nanalysis.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 34975}),\n",
       " Document(page_content='7 Conclusions', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35070}),\n",
       " Document(page_content='The results of this study provide strong support for the application of self-supervised', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35085}),\n",
       " Document(page_content='learning in the field of abnormality detection, especially valuable in environments', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35173}),\n",
       " Document(page_content='where radiologists face high workloads but the corresponding data labelling resources', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35257}),\n",
       " Document(page_content='are scarce. A critical aspect of this approach is its adaptability to regional variations in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35343}),\n",
       " Document(page_content='CXR image, attributable to differences in imaging equipment, patient demographics,', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35436}),\n",
       " Document(page_content='and other locale-specific factors [34][35]. Such variations often impede the cross-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35519}),\n",
       " Document(page_content='regional applicability of a model, thus limiting its generalizability. By employing the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35603}),\n",
       " Document(page_content='BarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35691}),\n",
       " Document(page_content='networks to local datasets tailored to the unique characteristics of their regional data.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35774}),\n",
       " Document(page_content='Our findings might also have significant implications for clinical practice, suggest-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35864}),\n",
       " Document(page_content='ing that this strategy could be a game-changer in aiding radiologists to interpret', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 35950}),\n",
       " Document(page_content='CXR images efficiently. This technology promises to reduce diagnostic times, poten-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36033}),\n",
       " Document(page_content='tially increasing patients’ throughput and improving the overall quality of care.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36117}),\n",
       " Document(page_content='Given its capacity for fine-tuning to specific regional characteristics, our approach', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36199}),\n",
       " Document(page_content='holds particular promise in areas where standardization of medical imaging presents\\nchallenges.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36285}),\n",
       " Document(page_content='In summary, the BarlowTwins-CXR approach demonstrates the potential of AI', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36382}),\n",
       " Document(page_content='to enhance healthcare delivery. By integrating cutting-edge technology with clini-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36456}),\n",
       " Document(page_content='cal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36539}),\n",
       " Document(page_content='8 Abbreviations', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36651}),\n",
       " Document(page_content='AP: anterior-posterior\\nAUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36668}),\n",
       " Document(page_content='CIUSSS: Centre int´egr´e universitaire de sant´e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36781}),\n",
       " Document(page_content='FPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 36908}),\n",
       " Document(page_content='ROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37030}),\n",
       " Document(page_content='15\\n\\n\\x0cWBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n\\n9 Declarations\\n\\n9.1 Ethics approval and consent to participate', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37130}),\n",
       " Document(page_content='All methods were performed under relevant guidelines and regulations (e.g., Decla-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37250}),\n",
       " Document(page_content='rations of Helsinki). The studies reported in this manuscript used reputable public', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37333}),\n",
       " Document(page_content='datasets and did not require any additional data involving human participants, human\\ndata, or human tissue.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37417}),\n",
       " Document(page_content='9.2 Consent for publication\\n\\nNot applicable\\n\\n9.3 Availability of data and materials', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37526}),\n",
       " Document(page_content='The datasets generated and/or analysed during the current study are available in the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37611}),\n",
       " Document(page_content='VinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37696}),\n",
       " Document(page_content='9.4 Competing interests\\n\\nThe authors declare that they have no competing interests\\n\\n9.5 Funding', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37763}),\n",
       " Document(page_content='9.5 Funding\\n\\nNo external funding was associated with this research study.\\n\\n9.6 Authors’ contributions', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37847}),\n",
       " Document(page_content='HS designed the research methodology, analyzed data, was responsible for experiments', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 37950}),\n",
       " Document(page_content='and results visualization, and participated in manuscript drafting and revision. LM', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38035}),\n",
       " Document(page_content='assisted in developing the research methodology and contributed to the drafting and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38119}),\n",
       " Document(page_content='revision of the manuscript. JFS collected and interpreted data, and provided expertise', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38203}),\n",
       " Document(page_content='in statistical analysis. DL contributed to the study design, offered statistical analysis', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38290}),\n",
       " Document(page_content='expertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38380}),\n",
       " Document(page_content='All authors read and approved the final manuscript.\\n\\n9.7 Acknowledgements', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38500}),\n",
       " Document(page_content='The authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆıle-de-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38575}),\n",
       " Document(page_content='montr´eal for the computational resources and support provided, which were essential', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38657}),\n",
       " Document(page_content='for the research conducted as part of the graduate internship program. We are espe-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38742}),\n",
       " Document(page_content='cially thankful to our department director, Mathieu Mailhot, for his mentorship and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38826}),\n",
       " Document(page_content='to Chen Cheng for his collaborative efforts and valuable contributions to this project.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38910}),\n",
       " Document(page_content='Their expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 38998}),\n",
       " Document(page_content='16\\n\\n\\x0cReferences', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39106}),\n",
       " Document(page_content='[1] Satia, I., Bashagha, S., Bibi, A., et al.: Assessing the accuracy and certainty in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39123}),\n",
       " Document(page_content='interpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39210}),\n",
       " Document(page_content='[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al.: The role of chest imaging', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39315}),\n",
       " Document(page_content='in patient management during the covid-19 pandemic: a multinational consensus', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39400}),\n",
       " Document(page_content='statement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39478}),\n",
       " Document(page_content='[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39564}),\n",
       " Document(page_content='Trend in radiologist workload compared to number of admissions in the emer-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39641}),\n",
       " Document(page_content='gency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39717}),\n",
       " Document(page_content='[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\\n\\ndate-of-access (2022)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39801}),\n",
       " Document(page_content='[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al.: Effect of a comprehensive', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39903}),\n",
       " Document(page_content='deep-learning model on the accuracy of chest x-ray interpretation by radiologists:', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 39989}),\n",
       " Document(page_content='a retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40072}),\n",
       " Document(page_content='[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40180}),\n",
       " Document(page_content='ing research on medical image analysis using imagenet. Computers in Biology', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40261}),\n",
       " Document(page_content='and Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40337}),\n",
       " Document(page_content='[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al.: Transfer learning for medical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40420}),\n",
       " Document(page_content='image classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40505}),\n",
       " Document(page_content='[8] Zbontar, J., Jing, L., Misra, I., et al.: Barlow twins: Self-supervised learning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40626}),\n",
       " Document(page_content='via redundancy reduction. In: Proceedings of the International Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40711}),\n",
       " Document(page_content='Machine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40788}),\n",
       " Document(page_content='[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40880}),\n",
       " Document(page_content='scale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 40968}),\n",
       " Document(page_content='Computer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41050}),\n",
       " Document(page_content='[10] Nguyen, H. Q., Lam, K., Le, L. T., et al.: Vindr-cxr: An open dataset of chest', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41176}),\n",
       " Document(page_content='x-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41260}),\n",
       " Document(page_content='17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41367}),\n",
       " Document(page_content='[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41372}),\n",
       " Document(page_content='In: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41460}),\n",
       " Document(page_content='Recognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41535}),\n",
       " Document(page_content='[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41634}),\n",
       " Document(page_content='detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41719}),\n",
       " Document(page_content='[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41848}),\n",
       " Document(page_content='yolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 41933}),\n",
       " Document(page_content='Bioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42012}),\n",
       " Document(page_content='[14] Ga´al, G., Maga, B., Luk´acs, A.: Attention u-net based adversarial architectures', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42141}),\n",
       " Document(page_content='for chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42228}),\n",
       " Document(page_content='[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42347}),\n",
       " Document(page_content='medical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42431}),\n",
       " Document(page_content='[16] Bengio, Y., Lamblin, P., Popovici, D., et al.: Greedy layer-wise training of deep', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42545}),\n",
       " Document(page_content='networks. In: Proceedings of the 19th International Conference on Neural Infor-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42632}),\n",
       " Document(page_content='mation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42712}),\n",
       " Document(page_content='[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al.: Context encoders: Feature', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42830}),\n",
       " Document(page_content='learning by inpainting. In: Proceedings of the IEEE Conference on Computer', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42910}),\n",
       " Document(page_content='Vision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 42985}),\n",
       " Document(page_content='[18] Chen, T., Kornblith, S., Norouzi, M., et al.: A simple framework for contrastive', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43098}),\n",
       " Document(page_content='learning of visual representations. In: Proceedings of the International Conference', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43184}),\n",
       " Document(page_content='on Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.\\n3525087', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43268}),\n",
       " Document(page_content='[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43353}),\n",
       " Document(page_content='predicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43437}),\n",
       " Document(page_content='[20] Azizi, S., Mustafa, B., Ryan, F., et al.: Big self-supervised models advance medical', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43549}),\n",
       " Document(page_content='image classification. In: Proceedings of the IEEE/CVF International Conference', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43639}),\n",
       " Document(page_content='on Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43718}),\n",
       " Document(page_content='[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n\\n18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43829}),\n",
       " Document(page_content='representation and transferability of chest x-ray models. In: Medical Imaging with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 43918}),\n",
       " Document(page_content='Deep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44001}),\n",
       " Document(page_content='[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44079}),\n",
       " Document(page_content='ing research on medical image analysis using imagenet. Computers in Biology', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44161}),\n",
       " Document(page_content='and Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44237}),\n",
       " Document(page_content='[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44320}),\n",
       " Document(page_content='transfer learning work for medical images: Feature reuse & other factors. In:', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44402}),\n",
       " Document(page_content='Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44480}),\n",
       " Document(page_content='Recognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44555}),\n",
       " Document(page_content='[24] Wang, X., Peng, Y., Lu, L., et al.: Chestx-ray8: Hospital-scale chest x-ray', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44666}),\n",
       " Document(page_content='database and benchmarks on weakly-supervised classification and localization', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44747}),\n",
       " Document(page_content='of common thorax diseases. In: Proceedings of the 2017 IEEE Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44824}),\n",
       " Document(page_content='Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 44898}),\n",
       " Document(page_content='[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45026}),\n",
       " Document(page_content='from different object detection models. Image and Vision Computing 107, 104117', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45110}),\n",
       " Document(page_content='(2021) https://doi.org/10.1016/j.imavis.2021.104117', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45189}),\n",
       " Document(page_content='[26] He, K., Fan, H., Wu, Y., et al.: Momentum contrast for unsupervised visual', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45242}),\n",
       " Document(page_content='representation learning. In: Proceedings of the IEEE/CVF Conference on Com-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45322}),\n",
       " Document(page_content='puter Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45398}),\n",
       " Document(page_content='[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45522}),\n",
       " Document(page_content='on Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45606}),\n",
       " Document(page_content='[28] Lin, T. Y., Doll´ar, P., Girshick, R., et al.: Feature pyramid networks for object', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45705}),\n",
       " Document(page_content='detection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45793}),\n",
       " Document(page_content='Pattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45871}),\n",
       " Document(page_content='[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 45978}),\n",
       " Document(page_content='tion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46061}),\n",
       " Document(page_content='[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46173}),\n",
       " Document(page_content='mizing mutual information across views. In: Proceedings of the 33rd International', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46255}),\n",
       " Document(page_content='Conference on Neural Information Processing Systems, Red Hook, NY, USA, pp.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46337}),\n",
       " Document(page_content='15535–15545 (2019). https://doi.org/10.5555/3454287.3455679', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46413}),\n",
       " Document(page_content='19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46474}),\n",
       " Document(page_content='[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46479}),\n",
       " Document(page_content='Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46569}),\n",
       " Document(page_content='Recognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46644}),\n",
       " Document(page_content='[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46749}),\n",
       " Document(page_content='data. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46834}),\n",
       " Document(page_content='Machine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 46909}),\n",
       " Document(page_content='[33] Szyma´nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 47002}),\n",
       " Document(page_content='label data. Proceedings of the First International Workshop on Learning with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 47090}),\n",
       " Document(page_content='Imbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 47167}),\n",
       " Document(page_content='[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 47263}),\n",
       " Document(page_content='physicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 47348}),\n",
       " Document(page_content='[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 47447}),\n",
       " Document(page_content='radiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n\\n20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 47530})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=16, add_start_index=True)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning\\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸ cois Samson3,\\nDianbo Liu2,4\\n1*D´ epartement d’informatique et de recherche op´ erationnelle, Universit´ e\\nde Montr´ eal, 2920 chemin de la Tour, Montr´ eal, H3T 1J4, QC, Canada.\\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´ eal, H2S\\n3H1, QC, Canada.\\n3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ ˆIle-de-Montr´ eal, 400 Blvd. De Maisonneuve Ouest,\\nMontr´ eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National\\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.\\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;\\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\\nAbstract\\nBackground: Chest X-ray imaging based abnormality localization, essential in\\ndiagnosing various diseases, faces significant clinical challenges due to complex\\ninterpretations and the growing workload of radiologists. Recent advances in deep\\nlearning, especially self-supervised learning, offer promising solutions to enhance\\nimage analysis efficiency, accuracy and reliability.\\nThis study aims to improve autonomic abnormality localization performance of\\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\\nsupervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0}),\n",
       " Document(page_content='backbone pre-trained on ImageNet. This was followed by supervised fine-tuning\\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.\\nResults: Our experiments showed a significant improvement in model perfor-\\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\\nracy of chest X-ray image base abnormality localization, outperforming tradi-\\ntional transfer learning methods. Its ability to adapt to various imaging conditions\\nand regional variations demonstrates the potential of self-supervised learning in\\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\\nparticularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.\\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.\\n1 Introduction\\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\\ntriaging patients and prioritizing the care order due to its convenience and flexibility.\\nEffective mitigation addresses the lack of availability of computed tomography and\\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\\nplex interpretation often requires a highly qualified radiologist to make an accurate\\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\\ning and remote areas, qualified radiologists are insufficient to cope with the increased\\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\\npopulation [4]. This situation necessitates urgently developing and introducing auto-\\nmated technologies like AI-based image analysis tools to aid radiologists in quicker\\nand more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.\\nIn recent years, deep learning models have rapidly advanced in various medi-\\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\\nysis because it can identify and precisely locate the types of anomalies in the images,\\n2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1}),\n",
       " Document(page_content='providing doctors with more specific and valuable information. However, training\\nthese models requires a large amount of annotated data. These annotations must be\\nperformed by experienced radiologists for CXR images, as well as for most medi-\\ncal images, making such annotated data not only costly, but also rare, with only a\\nvery limited number of public datasets including bounding box information. Although\\ntransfer learning is widely regarded as an effective method to solve the problem\\nof scarce labelling data, its application in medical image analysis still faces limita-\\ntions. This is mainly due to the significant difference in feature distribution between\\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\\ndatasets. This disparity suggests that directly applying these pre-trained weights to\\nmedical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].\\nTo fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\\nemploying a dual-phase training process to enhance CXR image analysis. The first\\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\\napplied to fine-tune the model. Our experiments show that such a training strategy\\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\\nmodel performance. We observed a 3% increase in model accuracy on the mean\\nAverage Precision benchmark, surpassing the results achieved by directly performing\\nconventional transfer learning from ImageNet pre-trained weights.\\nThis study extends the application of self-supervised learning to chest X-ray abnor-\\nmality localization. It demonstrates the potential of self-supervised learning in medical\\nimaging analysis, especially in the absence of annotated data. By effectively improving\\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\\nresents a significant advancement in the field of CXR abnormality localization, paving\\nthe way for more efficient and accurate diagnostic methods in the future.\\n2 Related Work\\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\\nmodels trained with vast amounts of labelled data are capable of identifying features\\nof various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\\nimages [13]. This achievement is attributed to the advanced image recognition and\\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\\narchitecture which incorporates attention mechanisms, as proposed by Guszt´ av Ga´ al\\net al. [14], has made significant strides in accurately segmenting lung structures, thus\\naiding in detailed analysis and diagnosis of diseases.\\n3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2}),\n",
       " Document(page_content='Self-supervised learning has recently gained popularity in the field of medical imag-\\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\\nuseful feature representations by training deep networks on unsupervised data [16].\\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\\ntionships in data by designing innovative pretext tasks, such as image reconstruction\\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\\net al. used large-scale images for self-supervised learning to improve accuracy and con-\\nvergence speed significantly in downstream tasks, achieving better performance than\\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model\\nbased on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].\\nIn terms of transfer learning, applying models trained in one domain to another has\\nled to notable success in medical image analysis. Research indicates that well-processed\\ntransfer results from ImageNet can improve model performance in the medical imag-\\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\\nthe significant difference in feature distribution between medical and natural images,\\nfeatures learned from natural images may not always be broadly applicable to med-\\nical images [23]. Various cross-domain adaptive transfer learning methods have been\\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\\ning and sequential domain adaptation techniques. By tuning model parameters, these\\nmethods can be better adapted to the characteristics of medical images, improving\\nthe performance and accuracy of models in medical image analysis [22].\\n3 Methods\\n3.1 Dataset Selection\\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\\n14 diseases with image-level annotations, including disease location annotations in\\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.\\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\\ning sets. The training set, consisting of 15,000 images, was annotated independently\\nby three radiologists per image. In contrast, the test set, comprising 3,000 images,\\nunderwent a more rigorous annotation process. Initially, each image was independently\\nannotated by three radiologists. This is followed by a secondary review phase where\\nthese initial annotations are reviewed by two other more experienced radiologists, they\\ncommunicated with each other to resolve any disagreements and reach a consensus on\\n4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3}),\n",
       " Document(page_content='Countdisease/abnormalityNo Finding\\nInfiltration\\nEffusion\\nAtelectasis\\nNodule\\nMass\\nPneumothorax\\nConsolidation\\nPleural_Thickening\\nCardiomegaly\\nEmphysema\\nEdema\\nFibrosis\\nPneumonia\\nHernia\\n0 10000 20000 30000 40000 50000 60000 70000Fig. 1 Image-level label distribution of the NIH-CXR dataset.\\nthe final labelling. This meticulous process for the test set created a potential dispar-\\nity in data distribution compared to the training set. To eliminate any bias it might\\nintroduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.\\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\\ncalculating the weighted average of each set of duplicate bounding boxes to create a\\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-\\ntion redundancy and improving target area representation in the dataset. Figure 2\\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\\nable dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.\\n3.2 Dual-Phase Training Process\\nOur training encompasses two primary phases: self-supervised pre-training and sub-\\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\\napproach refined the ImageNet pre-trained model by updating its backbone weights.\\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step\\naims to further improve the model’s task-specific performance, explicitly enhancing\\nits capabilities in localized diseases in CXR images.\\n5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4}),\n",
       " Document(page_content='a)\\nb)Fig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\\npreprocessing.\\n3.2.1 Self-Supervised Pre-training\\nFor the first stage of training, we employed the original version of the Barlow Twins\\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\\nconventional contrastive learning, introducing a self-supervised learning framework\\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\\na straightforward yet potent principle: it learns distinctive features by reducing the\\nrepresentational differences between two differently distorted images from the same\\nsource as processed by the network. This strategy is instrumental in enabling the model\\nto identify unique and rich features in each image while concurrently minimizing the\\noverlap in features. The process involves generating two distinct variants of an image\\nthrough data augmentation, followed by their simultaneous processing via two deep\\n6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 5}),\n",
       " Document(page_content='Input\\nimageC2 C3 C4 C5\\nP2 P3 P4 P5Resnet50 Backbone\\nconvrpn_cls\\nrpn_reg\\nRoI poolingRegion Proposal Network\\nFC FC FCFeature map\\nFeature VectorFeature mapClass\\nBoxFeature\\nPyramid\\nNetworkX\\nY AY B\\nImagesDistorted\\nimages\\nZ A\\nZ BResnet 50\\nResnet 50Net Embeddings\\nEmporocal\\ncross-corr .Target\\ncross-corr .\\nI\\nLBT\\nfeature dimensionBarlow twins\\nFaster  R-CNNFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with\\nmulti-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\\nfor object detection tasks.\\nneural networks that share identical weights. The objective is to align the network’s\\nweights to enhance the similarity in the high-level representations of these image pairs\\nyet ensure that the individual features remain distinct and independent.\\nThe Barlow Twins method might be particularly useful for medical imaging\\nbecause it extracts features by minimizing the redundancy between representations of\\nperturbed images. In CXR imaging, subtle differences might indicate important health\\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\\nimportant features. In contrast to other contrastive learning algorithms like MoCo\\n[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\\nTwins works at the feature dimension. It aims to assign an independent meaning to\\neach feature dimension. This could lead to a richer feature representation, potentially\\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\\n7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6}),\n",
       " Document(page_content='pathological states). Moreover, compared to self-supervised learning methods requir-\\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\\noffers a more straightforward training framework, which is particularly important in\\nsituations with limited computational resources.\\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\\nfrom the Torchvision library, this step brings no additional cost. We used images from\\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.\\n3.2.2 Fine-tuning Phase\\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with\\nFeature Pyramid Network (FPN) [28] as our object detector and trained it on the\\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\\nR-CNN detector employs these regions to detect and classify targets. This architecture\\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\\nPyramid Network (FPN), an architecture frequently employed in object detection,\\nparticularly enhances performance with multi-scale targets. It integrates high-level\\nsemantic information from deeper layers with detailed information from shallower lay-\\ners, producing feature maps of varied scales that effectively detect differently sized\\ntargets.\\nWe employed the MMdetection [29] machine learning toolbox as the platform for\\nFaster R-CNN, utilizing a number of classical image augmentation techniques and\\nmaintaining consistent hyperparameters across all experiments. Two different input\\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\\non the model’s performance with the pre-trained models. In addition, for comparison,\\nwe also conducted experiments using ImageNet pre-trained weights directly.\\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\\ncomprehensively evaluate the self-supervised learning model’s performance in medical\\nimaging. This method examines the model’s feature transfer capability - its ability to\\nadapt learned representations to new tasks. We first resplit the test set of the NIH\\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\\nand the remaining 20% as an evaluation test set for assessing model performance.\\nWe adopted two distinct strategies during the evaluation: freezing the backbone\\nweights or fine-tuning the weights. In the freezing backbone strategy, we kept the\\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\\nthe entire network, encompassing both the self-supervised trained feature extraction\\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\\n8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier, allowing us to assess the\\nmodel’s performance across different scales of training data.\\nWhen evaluating the representation transfer ability of a self-supervised learning\\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\\ntest sets is consistent. We used the Iterative stratification for the multi-label data\\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\\nand test sets were roughly similar. This helped prevent biases due to uneven label\\ndistribution, making our evaluation results more reliable and convincing.\\n3.3 Results Analysis Process\\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\\nof our object detection models. mAP is a widely recognized and effective metric in\\nobject detection, calculated by averaging precision scores across various object detec-\\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\\nscores for each class. The proportion of correct predictions relative to all predictions\\nfor a specific class across different detection confidence thresholds determines the pre-\\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\\nfor a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.\\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\\nprecision and recall, making it an especially appropriate performance indicator for\\nthis field. The AUC metric represents the area under the Receiver Operating Char-\\nacteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\\nfor imbalanced datasets as it does not rely directly on classification thresholds.\\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\\nlutional layer and observes the impact on the output class scores. This process reveals\\nthe most influential regions for the model’s decision-making. The resulting heat maps\\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.\\n4 Results\\n4.1 Transfer Learning on VinDr Abnormality Localization\\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\\n9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8}),\n",
       " Document(page_content='using two different input resolutions. Consistent hyperparameter settings were main-\\ntained across all experiments, ensuring that the performance changes were attributable\\nonly to the merits of the pretraining method itself. We visualized the performance of\\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\\non the validation set in Figure 4, and tabulated the corresponding mAP performance\\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\\nperformance baseline without pre-training benefits.\\na) b)\\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\\nbetween the lowest and highest value.\\nTable 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.\\nBackBone weight Input size mAP50 (val set) mAP50 (test set)\\nbaseline nopretrained 224 0.1388 (0.1352,0.1424) 0.1342 (0.1306,0.1378)\\nImageNet pretrained 0.2245 (0.2204,0.2286) 0.2210 (0.2194,0.2226)\\nBarlow twins 0.2555 (0.2485,0.2626) 0.2448 (0.2414,0.2482)\\nBarlow twins from ImageNet 0.2625 (0.2568,0.2682) 0.2502 (0.2476,0.2528)\\nImageNet pretrained 640 0.2973 (0.2913,0.3033) 0.280 (0.2757,0.2848)\\nBarlow twins from ImageNet 0.3102 (0.3080,0.3125) 0.289 (0.2826,0.2954)\\n1Scores are presented with 95% confidence intervals.\\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\\npre-training in feature representation across disparate image domains.\\n10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9}),\n",
       " Document(page_content='More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features\\nwith BarlowTwins-CXR, potentially offering more targeted information for clinical\\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\\nderived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.\\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\\nlow Twin-CXR weighted models saw performance improvements due to the increased\\ndetail in the CXR images. Nonetheless, the performance differential between the\\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.\\nThis points to intriguing future research avenues, such as refining image resolu-\\ntion parameters during pre-training and fine-tuning phases and investigating whether\\nhigher-resolution pre-training could elevate model performance. It also accentuates the\\nnecessity of tailoring deep learning model design to specific tasks, considering factors\\nlike image resolution and feature granularity.\\nOverall, implementing the Barlow Twins-CXR method on the VinDr dataset\\nresulted in substantial gains despite its data limitations and the inherent challenges\\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\\nimage weights for better applicability in CXR image analysis and beyond in medical\\nimaging.\\n4.2 Linear Evaluation Protocol\\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\\ntraditional ImageNet pre-training on the linear classification performance within the\\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone\\nof the linear classifier and updating only the final linear layer’s weights. This approach\\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\\nthese experiments are presented in Figure 6 and Table 2.\\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\\n11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10}),\n",
       " Document(page_content='Ground Truth Bbox ImageNet Weights Our method\\n20e27597c972c6e7fdb4d1e7638e227e\\n03431b577d1ccf075e930c4c4913c079\\nfd810298e165ef0b9a88bb25fda7a34bGround Truth Bbox ImageNet Weights Our method\\n9eba0d101f410f9cdfae46cb094ae2a6\\n87a8df2f22475c7200ebe891d0f25b88\\nad86f42123384e2441cce36347aa7d1aa) b)Fig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\\nbelow the heatmaps refer to the image numbers in the dataset.\\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\\nmethods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\\nImage-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)\\n1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\\nimage analysis, particularly when annotated data is scarce.\\n12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11}),\n",
       " Document(page_content='Fig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.\\n4.3 End-to-End Finetuning\\nIn our end-to-end experiments, where we permitted updates to all model layers, the\\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\\nImageNet pre-trained equivalent across all training set sizes. The results of these\\nexperiments are presented in Figure 7 and Table 3.\\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various\\ndataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.\\n13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12}),\n",
       " Document(page_content='Table 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\\npre-training methods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627) 0.7756 (0.7745, 0.7768) 0.8107 (0.8098, 0.8116)\\nImage-Net 0.6163 (0.6110, 0.6216) 0.7168 (0.7093, 0.7243) 0.7866 (0.7843, 0.7889)\\n1Scores are presented with 95% confidence intervals.\\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\nAUC than the ImageNet counterpart.\\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\\nover the frozen backbone setup was less marked, suggesting that the wealth of features\\nlearned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.\\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning\\nacross all model layers may elevate the risk of overfitting, particularly when data is\\nscarce. The narrowing performance differential between the two pre-training strategies\\nwith increasing data volume indicates that the distinction between domain-specific\\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\\ntial with larger datasets. This trend implies that the influence of the pre-training\\nstrategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.\\n5 Discussion\\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\\nunannotated CXR images for learning valuable representations and enhances trans-\\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\\nThis leads to quicker training and improved performance on tasks like abnormality\\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input\\nresolutions, outshining models pre-trained on ImageNet.\\nOne of the primary limitations of our study is the scarcity of CXR datasets with\\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\\nmay limit the generalizability of our findings. Additionally, the computational cost\\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\\nimages with an image size of 224*224 pixels, the training process required two days\\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\\nability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.\\n6 Future Work\\nOur future endeavours include developing a demo interactive system for deployment\\nand testing in emergency rooms. It will allow practical evaluation of the model’s\\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\\n14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13}),\n",
       " Document(page_content='Additionally, we plan to explore more advanced self-supervised learning methods,\\nobject detection frameworks, and backbone networks to refine our approach further.\\nThe continuous evolution of these technologies promises to address some of the current\\nlimitations and expand the applicability and accuracy of our model in medical image\\nanalysis.\\n7 Conclusions\\nThe results of this study provide strong support for the application of self-supervised\\nlearning in the field of abnormality detection, especially valuable in environments\\nwhere radiologists face high workloads but the corresponding data labelling resources\\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\\nCXR image, attributable to differences in imaging equipment, patient demographics,\\nand other locale-specific factors [34][35]. Such variations often impede the cross-\\nregional applicability of a model, thus limiting its generalizability. By employing the\\nBarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\\nnetworks to local datasets tailored to the unique characteristics of their regional data.\\nOur findings might also have significant implications for clinical practice, suggest-\\ning that this strategy could be a game-changer in aiding radiologists to interpret\\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\\ntially increasing patients’ throughput and improving the overall quality of care.\\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\\nholds particular promise in areas where standardization of medical imaging presents\\nchallenges.\\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.\\n8 Abbreviations\\nAP: anterior-posterior\\nAUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping\\nCIUSSS: Centre int´ egr´ e universitaire de sant´ e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer\\nFPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic\\nROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate\\n15', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14}),\n",
       " Document(page_content='WBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n9 Declarations\\n9.1 Ethics approval and consent to participate\\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\\nrations of Helsinki). The studies reported in this manuscript used reputable public\\ndatasets and did not require any additional data involving human participants, human\\ndata, or human tissue.\\n9.2 Consent for publication\\nNot applicable\\n9.3 Availability of data and materials\\nThe datasets generated and/or analysed during the current study are available in the\\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\\n9.4 Competing interests\\nThe authors declare that they have no competing interests\\n9.5 Funding\\nNo external funding was associated with this research study.\\n9.6 Authors’ contributions\\nHS designed the research methodology, analyzed data, was responsible for experiments\\nand results visualization, and participated in manuscript drafting and revision. LM\\nassisted in developing the research methodology and contributed to the drafting and\\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\\nin statistical analysis. DL contributed to the study design, offered statistical analysis\\nexpertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.\\nAll authors read and approved the final manuscript.\\n9.7 Acknowledgements\\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆ ıle-de-\\nmontr´ eal for the computational resources and support provided, which were essential\\nfor the research conducted as part of the graduate internship program. We are espe-\\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\\nTheir expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.\\n16', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15}),\n",
       " Document(page_content='References\\n[1] Satia, I., Bashagha, S., Bibi, A., et al. : Assessing the accuracy and certainty in\\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502\\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al. : The role of chest imaging\\nin patient management during the covid-19 pandemic: a multinational consensus\\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978\\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\\nTrend in radiologist workload compared to number of admissions in the emer-\\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337\\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\\ndate-of-access (2022)\\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al. : Effect of a comprehensive\\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:\\na retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054\\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al. : Transfer learning for medical\\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7\\n[8] Zbontar, J., Jing, L., Misra, I., et al. : Barlow twins: Self-supervised learning\\nvia redundancy reduction. In: Proceedings of the International Conference on\\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230\\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al. : Vindr-cxr: An open dataset of chest\\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w\\n17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16}),\n",
       " Document(page_content='[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90\\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225\\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\\n[14] Ga´ al, G., Maga, B., Luk´ acs, A.: Attention u-net based adversarial architectures\\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304\\n[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045\\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al. : Greedy layer-wise training of deep\\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476\\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al. : Context encoders: Feature\\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278\\n[18] Chen, T., Kornblith, S., Norouzi, M., et al. : A simple framework for contrastive\\nlearning of visual representations. In: Proceedings of the International Conference\\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.\\n3525087\\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728\\n[20] Azizi, S., Mustafa, B., Ryan, F., et al. : Big self-supervised models advance medical\\nimage classification. In: Proceedings of the IEEE/CVF International Conference\\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346\\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17}),\n",
       " Document(page_content='representation and transferability of chest x-ray models. In: Medical Imaging with\\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\\ntransfer learning work for medical images: Feature reuse & other factors. In:\\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901\\n[24] Wang, X., Peng, Y., Lu, L., et al. : Chestx-ray8: Hospital-scale chest x-ray\\ndatabase and benchmarks on weakly-supervised classification and localization\\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\\nfrom different object detection models. Image and Vision Computing 107, 104117\\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\\n[26] He, K., Fan, H., Wu, Y., et al. : Momentum contrast for unsupervised visual\\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975\\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169\\n[28] Lin, T. Y., Doll´ ar, P., Girshick, R., et al. : Feature pyramid networks for object\\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106\\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155\\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\\nmizing mutual information across views. In: Proceedings of the 33rd International\\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\\n19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18}),\n",
       " Document(page_content='[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277\\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)\\n[33] Szyma´ nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\\nlabel data. Proceedings of the First International Workshop on Learning with\\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756\\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979\\n[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader2 = PyPDFLoader(\"../documents/barlowtwins-CXR.pdf\")\n",
    "documents2 = pdf_loader2.load()\n",
    "documents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning\\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸ cois Samson3,\\nDianbo Liu2,4\\n1*D´ epartement d’informatique et de recherche op´ erationnelle, Universit´ e\\nde Montr´ eal, 2920 chemin de la Tour, Montr´ eal, H3T 1J4, QC, Canada.\\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´ eal, H2S\\n3H1, QC, Canada.\\n3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ ˆIle-de-Montr´ eal, 400 Blvd. De Maisonneuve Ouest,\\nMontr´ eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National\\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.\\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;\\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\\nAbstract\\nBackground: Chest X-ray imaging based abnormality localization, essential in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0, 'start_index': 0}),\n",
       " Document(page_content='diagnosing various diseases, faces significant clinical challenges due to complex\\ninterpretations and the growing workload of radiologists. Recent advances in deep\\nlearning, especially self-supervised learning, offer promising solutions to enhance\\nimage analysis efficiency, accuracy and reliability.\\nThis study aims to improve autonomic abnormality localization performance of\\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\\nsupervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0, 'start_index': 976}),\n",
       " Document(page_content='backbone pre-trained on ImageNet. This was followed by supervised fine-tuning\\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.\\nResults: Our experiments showed a significant improvement in model perfor-\\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\\nracy of chest X-ray image base abnormality localization, outperforming tradi-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 0}),\n",
       " Document(page_content='tional transfer learning methods. Its ability to adapt to various imaging conditions\\nand regional variations demonstrates the potential of self-supervised learning in\\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\\nparticularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.\\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.\\n1 Introduction\\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\\ntriaging patients and prioritizing the care order due to its convenience and flexibility.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 923}),\n",
       " Document(page_content='Effective mitigation addresses the lack of availability of computed tomography and\\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\\nplex interpretation often requires a highly qualified radiologist to make an accurate\\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\\ning and remote areas, qualified radiologists are insufficient to cope with the increased\\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\\npopulation [4]. This situation necessitates urgently developing and introducing auto-\\nmated technologies like AI-based image analysis tools to aid radiologists in quicker', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 1853}),\n",
       " Document(page_content='and more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.\\nIn recent years, deep learning models have rapidly advanced in various medi-\\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\\nysis because it can identify and precisely locate the types of anomalies in the images,\\n2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 2802}),\n",
       " Document(page_content='providing doctors with more specific and valuable information. However, training\\nthese models requires a large amount of annotated data. These annotations must be\\nperformed by experienced radiologists for CXR images, as well as for most medi-\\ncal images, making such annotated data not only costly, but also rare, with only a\\nvery limited number of public datasets including bounding box information. Although\\ntransfer learning is widely regarded as an effective method to solve the problem\\nof scarce labelling data, its application in medical image analysis still faces limita-\\ntions. This is mainly due to the significant difference in feature distribution between\\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\\ndatasets. This disparity suggests that directly applying these pre-trained weights to\\nmedical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 0}),\n",
       " Document(page_content='To fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\\nemploying a dual-phase training process to enhance CXR image analysis. The first\\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\\napplied to fine-tune the model. Our experiments show that such a training strategy\\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\\nmodel performance. We observed a 3% increase in model accuracy on the mean\\nAverage Precision benchmark, surpassing the results achieved by directly performing\\nconventional transfer learning from ImageNet pre-trained weights.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 963}),\n",
       " Document(page_content='This study extends the application of self-supervised learning to chest X-ray abnor-\\nmality localization. It demonstrates the potential of self-supervised learning in medical\\nimaging analysis, especially in the absence of annotated data. By effectively improving\\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\\nresents a significant advancement in the field of CXR abnormality localization, paving\\nthe way for more efficient and accurate diagnostic methods in the future.\\n2 Related Work\\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\\nmodels trained with vast amounts of labelled data are capable of identifying features', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 1937}),\n",
       " Document(page_content='of various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\\nimages [13]. This achievement is attributed to the advanced image recognition and\\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\\narchitecture which incorporates attention mechanisms, as proposed by Guszt´ av Ga´ al\\net al. [14], has made significant strides in accurately segmenting lung structures, thus\\naiding in detailed analysis and diagnosis of diseases.\\n3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 2881}),\n",
       " Document(page_content='Self-supervised learning has recently gained popularity in the field of medical imag-\\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\\nuseful feature representations by training deep networks on unsupervised data [16].\\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\\ntionships in data by designing innovative pretext tasks, such as image reconstruction\\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\\net al. used large-scale images for self-supervised learning to improve accuracy and con-\\nvergence speed significantly in downstream tasks, achieving better performance than\\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 0}),\n",
       " Document(page_content='based on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].\\nIn terms of transfer learning, applying models trained in one domain to another has\\nled to notable success in medical image analysis. Research indicates that well-processed\\ntransfer results from ImageNet can improve model performance in the medical imag-\\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\\nthe significant difference in feature distribution between medical and natural images,\\nfeatures learned from natural images may not always be broadly applicable to med-\\nical images [23]. Various cross-domain adaptive transfer learning methods have been\\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\\ning and sequential domain adaptation techniques. By tuning model parameters, these\\nmethods can be better adapted to the characteristics of medical images, improving', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 952}),\n",
       " Document(page_content='the performance and accuracy of models in medical image analysis [22].\\n3 Methods\\n3.1 Dataset Selection\\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\\n14 diseases with image-level annotations, including disease location annotations in\\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.\\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\\ning sets. The training set, consisting of 15,000 images, was annotated independently', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 1895}),\n",
       " Document(page_content='by three radiologists per image. In contrast, the test set, comprising 3,000 images,\\nunderwent a more rigorous annotation process. Initially, each image was independently\\nannotated by three radiologists. This is followed by a secondary review phase where\\nthese initial annotations are reviewed by two other more experienced radiologists, they\\ncommunicated with each other to resolve any disagreements and reach a consensus on\\n4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 2853}),\n",
       " Document(page_content='Countdisease/abnormalityNo Finding\\nInfiltration\\nEffusion\\nAtelectasis\\nNodule\\nMass\\nPneumothorax\\nConsolidation\\nPleural_Thickening\\nCardiomegaly\\nEmphysema\\nEdema\\nFibrosis\\nPneumonia\\nHernia\\n0 10000 20000 30000 40000 50000 60000 70000Fig. 1 Image-level label distribution of the NIH-CXR dataset.\\nthe final labelling. This meticulous process for the test set created a potential dispar-\\nity in data distribution compared to the training set. To eliminate any bias it might\\nintroduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.\\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\\ncalculating the weighted average of each set of duplicate bounding boxes to create a\\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4, 'start_index': 0}),\n",
       " Document(page_content='tion redundancy and improving target area representation in the dataset. Figure 2\\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\\nable dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.\\n3.2 Dual-Phase Training Process\\nOur training encompasses two primary phases: self-supervised pre-training and sub-\\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\\napproach refined the ImageNet pre-trained model by updating its backbone weights.\\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4, 'start_index': 924}),\n",
       " Document(page_content='aims to further improve the model’s task-specific performance, explicitly enhancing\\nits capabilities in localized diseases in CXR images.\\n5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4, 'start_index': 1887}),\n",
       " Document(page_content='a)\\nb)Fig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\\npreprocessing.\\n3.2.1 Self-Supervised Pre-training\\nFor the first stage of training, we employed the original version of the Barlow Twins\\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\\nconventional contrastive learning, introducing a self-supervised learning framework\\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\\na straightforward yet potent principle: it learns distinctive features by reducing the\\nrepresentational differences between two differently distorted images from the same\\nsource as processed by the network. This strategy is instrumental in enabling the model\\nto identify unique and rich features in each image while concurrently minimizing the\\noverlap in features. The process involves generating two distinct variants of an image\\nthrough data augmentation, followed by their simultaneous processing via two deep\\n6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 5, 'start_index': 0}),\n",
       " Document(page_content='Input\\nimageC2 C3 C4 C5\\nP2 P3 P4 P5Resnet50 Backbone\\nconvrpn_cls\\nrpn_reg\\nRoI poolingRegion Proposal Network\\nFC FC FCFeature map\\nFeature VectorFeature mapClass\\nBoxFeature\\nPyramid\\nNetworkX\\nY AY B\\nImagesDistorted\\nimages\\nZ A\\nZ BResnet 50\\nResnet 50Net Embeddings\\nEmporocal\\ncross-corr .Target\\ncross-corr .\\nI\\nLBT\\nfeature dimensionBarlow twins\\nFaster  R-CNNFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6, 'start_index': 0}),\n",
       " Document(page_content='multi-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\\nfor object detection tasks.\\nneural networks that share identical weights. The objective is to align the network’s\\nweights to enhance the similarity in the high-level representations of these image pairs\\nyet ensure that the individual features remain distinct and independent.\\nThe Barlow Twins method might be particularly useful for medical imaging\\nbecause it extracts features by minimizing the redundancy between representations of\\nperturbed images. In CXR imaging, subtle differences might indicate important health\\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\\nimportant features. In contrast to other contrastive learning algorithms like MoCo', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6, 'start_index': 912}),\n",
       " Document(page_content='[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\\nTwins works at the feature dimension. It aims to assign an independent meaning to\\neach feature dimension. This could lead to a richer feature representation, potentially\\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\\n7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6, 'start_index': 1896}),\n",
       " Document(page_content='pathological states). Moreover, compared to self-supervised learning methods requir-\\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\\noffers a more straightforward training framework, which is particularly important in\\nsituations with limited computational resources.\\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\\nfrom the Torchvision library, this step brings no additional cost. We used images from\\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.\\n3.2.2 Fine-tuning Phase\\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 0}),\n",
       " Document(page_content='Feature Pyramid Network (FPN) [28] as our object detector and trained it on the\\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\\nR-CNN detector employs these regions to detect and classify targets. This architecture\\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\\nPyramid Network (FPN), an architecture frequently employed in object detection,\\nparticularly enhances performance with multi-scale targets. It integrates high-level\\nsemantic information from deeper layers with detailed information from shallower lay-\\ners, producing feature maps of varied scales that effectively detect differently sized\\ntargets.\\nWe employed the MMdetection [29] machine learning toolbox as the platform for', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 935}),\n",
       " Document(page_content='Faster R-CNN, utilizing a number of classical image augmentation techniques and\\nmaintaining consistent hyperparameters across all experiments. Two different input\\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\\non the model’s performance with the pre-trained models. In addition, for comparison,\\nwe also conducted experiments using ImageNet pre-trained weights directly.\\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\\ncomprehensively evaluate the self-supervised learning model’s performance in medical\\nimaging. This method examines the model’s feature transfer capability - its ability to\\nadapt learned representations to new tasks. We first resplit the test set of the NIH\\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\\nand the remaining 20% as an evaluation test set for assessing model performance.\\nWe adopted two distinct strategies during the evaluation: freezing the backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 1859}),\n",
       " Document(page_content='weights or fine-tuning the weights. In the freezing backbone strategy, we kept the\\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\\nthe entire network, encompassing both the self-supervised trained feature extraction\\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\\n8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 2859}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier, allowing us to assess the\\nmodel’s performance across different scales of training data.\\nWhen evaluating the representation transfer ability of a self-supervised learning\\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\\ntest sets is consistent. We used the Iterative stratification for the multi-label data\\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\\nand test sets were roughly similar. This helped prevent biases due to uneven label\\ndistribution, making our evaluation results more reliable and convincing.\\n3.3 Results Analysis Process\\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\\nof our object detection models. mAP is a widely recognized and effective metric in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 0}),\n",
       " Document(page_content='object detection, calculated by averaging precision scores across various object detec-\\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\\nscores for each class. The proportion of correct predictions relative to all predictions\\nfor a specific class across different detection confidence thresholds determines the pre-\\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\\nfor a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.\\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\\nprecision and recall, making it an especially appropriate performance indicator for\\nthis field. The AUC metric represents the area under the Receiver Operating Char-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 933}),\n",
       " Document(page_content='acteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\\nfor imbalanced datasets as it does not rely directly on classification thresholds.\\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\\nlutional layer and observes the impact on the output class scores. This process reveals\\nthe most influential regions for the model’s decision-making. The resulting heat maps\\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 1890}),\n",
       " Document(page_content='4 Results\\n4.1 Transfer Learning on VinDr Abnormality Localization\\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\\n9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 2885}),\n",
       " Document(page_content='using two different input resolutions. Consistent hyperparameter settings were main-\\ntained across all experiments, ensuring that the performance changes were attributable\\nonly to the merits of the pretraining method itself. We visualized the performance of\\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\\non the validation set in Figure 4, and tabulated the corresponding mAP performance\\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\\nperformance baseline without pre-training benefits.\\na) b)\\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\\nbetween the lowest and highest value.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9, 'start_index': 0}),\n",
       " Document(page_content='Table 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.\\nBackBone weight Input size mAP50 (val set) mAP50 (test set)\\nbaseline nopretrained 224 0.1388 (0.1352,0.1424) 0.1342 (0.1306,0.1378)\\nImageNet pretrained 0.2245 (0.2204,0.2286) 0.2210 (0.2194,0.2226)\\nBarlow twins 0.2555 (0.2485,0.2626) 0.2448 (0.2414,0.2482)\\nBarlow twins from ImageNet 0.2625 (0.2568,0.2682) 0.2502 (0.2476,0.2528)\\nImageNet pretrained 640 0.2973 (0.2913,0.3033) 0.280 (0.2757,0.2848)\\nBarlow twins from ImageNet 0.3102 (0.3080,0.3125) 0.289 (0.2826,0.2954)\\n1Scores are presented with 95% confidence intervals.\\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\\npre-training in feature representation across disparate image domains.\\n10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9, 'start_index': 980}),\n",
       " Document(page_content='More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 0}),\n",
       " Document(page_content='with BarlowTwins-CXR, potentially offering more targeted information for clinical\\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\\nderived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.\\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\\nlow Twin-CXR weighted models saw performance improvements due to the increased\\ndetail in the CXR images. Nonetheless, the performance differential between the\\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.\\nThis points to intriguing future research avenues, such as refining image resolu-\\ntion parameters during pre-training and fine-tuning phases and investigating whether\\nhigher-resolution pre-training could elevate model performance. It also accentuates the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 973}),\n",
       " Document(page_content='necessity of tailoring deep learning model design to specific tasks, considering factors\\nlike image resolution and feature granularity.\\nOverall, implementing the Barlow Twins-CXR method on the VinDr dataset\\nresulted in substantial gains despite its data limitations and the inherent challenges\\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\\nimage weights for better applicability in CXR image analysis and beyond in medical\\nimaging.\\n4.2 Linear Evaluation Protocol\\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\\ntraditional ImageNet pre-training on the linear classification performance within the\\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 1954}),\n",
       " Document(page_content='of the linear classifier and updating only the final linear layer’s weights. This approach\\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\\nthese experiments are presented in Figure 6 and Table 2.\\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\\n11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 2950}),\n",
       " Document(page_content='Ground Truth Bbox ImageNet Weights Our method\\n20e27597c972c6e7fdb4d1e7638e227e\\n03431b577d1ccf075e930c4c4913c079\\nfd810298e165ef0b9a88bb25fda7a34bGround Truth Bbox ImageNet Weights Our method\\n9eba0d101f410f9cdfae46cb094ae2a6\\n87a8df2f22475c7200ebe891d0f25b88\\nad86f42123384e2441cce36347aa7d1aa) b)Fig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\\nbelow the heatmaps refer to the image numbers in the dataset.\\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\\nmethods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\\nImage-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11, 'start_index': 0}),\n",
       " Document(page_content='1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\\nimage analysis, particularly when annotated data is scarce.\\n12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11, 'start_index': 974}),\n",
       " Document(page_content='Fig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.\\n4.3 End-to-End Finetuning\\nIn our end-to-end experiments, where we permitted updates to all model layers, the\\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\\nImageNet pre-trained equivalent across all training set sizes. The results of these\\nexperiments are presented in Figure 7 and Table 3.\\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12, 'start_index': 0}),\n",
       " Document(page_content='dataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.\\n13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12, 'start_index': 910}),\n",
       " Document(page_content='Table 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\\npre-training methods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627) 0.7756 (0.7745, 0.7768) 0.8107 (0.8098, 0.8116)\\nImage-Net 0.6163 (0.6110, 0.6216) 0.7168 (0.7093, 0.7243) 0.7866 (0.7843, 0.7889)\\n1Scores are presented with 95% confidence intervals.\\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\nAUC than the ImageNet counterpart.\\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\\nover the frozen backbone setup was less marked, suggesting that the wealth of features\\nlearned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.\\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13, 'start_index': 0}),\n",
       " Document(page_content='across all model layers may elevate the risk of overfitting, particularly when data is\\nscarce. The narrowing performance differential between the two pre-training strategies\\nwith increasing data volume indicates that the distinction between domain-specific\\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\\ntial with larger datasets. This trend implies that the influence of the pre-training\\nstrategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.\\n5 Discussion\\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\\nunannotated CXR images for learning valuable representations and enhances trans-\\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\\nThis leads to quicker training and improved performance on tasks like abnormality\\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13, 'start_index': 939}),\n",
       " Document(page_content='resolutions, outshining models pre-trained on ImageNet.\\nOne of the primary limitations of our study is the scarcity of CXR datasets with\\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\\nmay limit the generalizability of our findings. Additionally, the computational cost\\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\\nimages with an image size of 224*224 pixels, the training process required two days\\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\\nability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.\\n6 Future Work\\nOur future endeavours include developing a demo interactive system for deployment\\nand testing in emergency rooms. It will allow practical evaluation of the model’s\\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\\n14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13, 'start_index': 1889}),\n",
       " Document(page_content='Additionally, we plan to explore more advanced self-supervised learning methods,\\nobject detection frameworks, and backbone networks to refine our approach further.\\nThe continuous evolution of these technologies promises to address some of the current\\nlimitations and expand the applicability and accuracy of our model in medical image\\nanalysis.\\n7 Conclusions\\nThe results of this study provide strong support for the application of self-supervised\\nlearning in the field of abnormality detection, especially valuable in environments\\nwhere radiologists face high workloads but the corresponding data labelling resources\\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\\nCXR image, attributable to differences in imaging equipment, patient demographics,\\nand other locale-specific factors [34][35]. Such variations often impede the cross-\\nregional applicability of a model, thus limiting its generalizability. By employing the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14, 'start_index': 0}),\n",
       " Document(page_content='BarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\\nnetworks to local datasets tailored to the unique characteristics of their regional data.\\nOur findings might also have significant implications for clinical practice, suggest-\\ning that this strategy could be a game-changer in aiding radiologists to interpret\\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\\ntially increasing patients’ throughput and improving the overall quality of care.\\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\\nholds particular promise in areas where standardization of medical imaging presents\\nchallenges.\\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.\\n8 Abbreviations\\nAP: anterior-posterior', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14, 'start_index': 965}),\n",
       " Document(page_content='AUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping\\nCIUSSS: Centre int´ egr´ e universitaire de sant´ e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer\\nFPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic\\nROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate\\n15', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14, 'start_index': 1962}),\n",
       " Document(page_content='WBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n9 Declarations\\n9.1 Ethics approval and consent to participate\\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\\nrations of Helsinki). The studies reported in this manuscript used reputable public\\ndatasets and did not require any additional data involving human participants, human\\ndata, or human tissue.\\n9.2 Consent for publication\\nNot applicable\\n9.3 Availability of data and materials\\nThe datasets generated and/or analysed during the current study are available in the\\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\\n9.4 Competing interests\\nThe authors declare that they have no competing interests\\n9.5 Funding\\nNo external funding was associated with this research study.\\n9.6 Authors’ contributions\\nHS designed the research methodology, analyzed data, was responsible for experiments\\nand results visualization, and participated in manuscript drafting and revision. LM', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15, 'start_index': 0}),\n",
       " Document(page_content='assisted in developing the research methodology and contributed to the drafting and\\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\\nin statistical analysis. DL contributed to the study design, offered statistical analysis\\nexpertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.\\nAll authors read and approved the final manuscript.\\n9.7 Acknowledgements\\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆ ıle-de-\\nmontr´ eal for the computational resources and support provided, which were essential\\nfor the research conducted as part of the graduate internship program. We are espe-\\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\\nTheir expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.\\n16', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15, 'start_index': 971}),\n",
       " Document(page_content='References\\n[1] Satia, I., Bashagha, S., Bibi, A., et al. : Assessing the accuracy and certainty in\\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502\\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al. : The role of chest imaging\\nin patient management during the covid-19 pandemic: a multinational consensus\\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978\\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\\nTrend in radiologist workload compared to number of admissions in the emer-\\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337\\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\\ndate-of-access (2022)\\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al. : Effect of a comprehensive\\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16, 'start_index': 0}),\n",
       " Document(page_content='a retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054\\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al. : Transfer learning for medical\\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7\\n[8] Zbontar, J., Jing, L., Misra, I., et al. : Barlow twins: Self-supervised learning\\nvia redundancy reduction. In: Proceedings of the International Conference on\\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230\\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16, 'start_index': 958}),\n",
       " Document(page_content='Computer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al. : Vindr-cxr: An open dataset of chest\\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w\\n17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16, 'start_index': 1934}),\n",
       " Document(page_content='[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90\\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225\\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\\n[14] Ga´ al, G., Maga, B., Luk´ acs, A.: Attention u-net based adversarial architectures\\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17, 'start_index': 0}),\n",
       " Document(page_content='[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045\\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al. : Greedy layer-wise training of deep\\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476\\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al. : Context encoders: Feature\\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278\\n[18] Chen, T., Kornblith, S., Norouzi, M., et al. : A simple framework for contrastive\\nlearning of visual representations. In: Proceedings of the International Conference\\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17, 'start_index': 973}),\n",
       " Document(page_content='3525087\\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728\\n[20] Azizi, S., Mustafa, B., Ryan, F., et al. : Big self-supervised models advance medical\\nimage classification. In: Proceedings of the IEEE/CVF International Conference\\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346\\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17, 'start_index': 1970}),\n",
       " Document(page_content='representation and transferability of chest x-ray models. In: Medical Imaging with\\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\\ntransfer learning work for medical images: Feature reuse & other factors. In:\\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901\\n[24] Wang, X., Peng, Y., Lu, L., et al. : Chestx-ray8: Hospital-scale chest x-ray\\ndatabase and benchmarks on weakly-supervised classification and localization\\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18, 'start_index': 0}),\n",
       " Document(page_content='Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\\nfrom different object detection models. Image and Vision Computing 107, 104117\\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\\n[26] He, K., Fan, H., Wu, Y., et al. : Momentum contrast for unsupervised visual\\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975\\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169\\n[28] Lin, T. Y., Doll´ ar, P., Girshick, R., et al. : Feature pyramid networks for object\\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18, 'start_index': 978}),\n",
       " Document(page_content='Pattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106\\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155\\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\\nmizing mutual information across views. In: Proceedings of the 33rd International\\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\\n19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18, 'start_index': 1950}),\n",
       " Document(page_content='[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277\\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)\\n[33] Szyma´ nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\\nlabel data. Proceedings of the First International Workshop on Learning with\\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756\\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19, 'start_index': 0}),\n",
       " Document(page_content='[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19, 'start_index': 965})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, add_start_index=True)\n",
    "all_splits = text_splitter.split_documents(documents2)\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"../documents/barlowtwins-CXR.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower()  # Convert to lowercase\n",
    "  #text = re.sub(r'[^a-z0-9\\s-]', '', text)  # Remove non-alphanumeric characters (except space and dash)\n",
    "  text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single spaces\n",
    "  return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "  for doc in documents:\n",
    "    # Apply basic cleaning\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    # Optionally remove stop words\n",
    "    # cleaned_text = remove_stop_words(cleaned_text)  # Uncomment if you want stop word removal\n",
    "    doc.page_content = cleaned_text\n",
    "  return documents\n",
    "documents_copy = copy.deepcopy(documents)\n",
    "preprocessed_documents = preprocess_documents(documents_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning\\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸ cois Samson3,\\nDianbo Liu2,4\\n1*D´ epartement d’informatique et de recherche op´ erationnelle, Universit´ e\\nde Montr´ eal, 2920 chemin de la Tour, Montr´ eal, H3T 1J4, QC, Canada.\\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´ eal, H2S\\n3H1, QC, Canada.\\n3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ ˆIle-de-Montr´ eal, 400 Blvd. De Maisonneuve Ouest,\\nMontr´ eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National\\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.\\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;\\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\\nAbstract\\nBackground: Chest X-ray imaging based abnormality localization, essential in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0, 'start_index': 0}),\n",
       " Document(page_content='diagnosing various diseases, faces significant clinical challenges due to complex\\ninterpretations and the growing workload of radiologists. Recent advances in deep\\nlearning, especially self-supervised learning, offer promising solutions to enhance\\nimage analysis efficiency, accuracy and reliability.\\nThis study aims to improve autonomic abnormality localization performance of\\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\\nsupervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0, 'start_index': 976}),\n",
       " Document(page_content='backbone pre-trained on ImageNet. This was followed by supervised fine-tuning\\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.\\nResults: Our experiments showed a significant improvement in model perfor-\\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\\nracy of chest X-ray image base abnormality localization, outperforming tradi-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 0}),\n",
       " Document(page_content='tional transfer learning methods. Its ability to adapt to various imaging conditions\\nand regional variations demonstrates the potential of self-supervised learning in\\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\\nparticularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.\\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.\\n1 Introduction\\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\\ntriaging patients and prioritizing the care order due to its convenience and flexibility.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 923}),\n",
       " Document(page_content='Effective mitigation addresses the lack of availability of computed tomography and\\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\\nplex interpretation often requires a highly qualified radiologist to make an accurate\\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\\ning and remote areas, qualified radiologists are insufficient to cope with the increased\\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\\npopulation [4]. This situation necessitates urgently developing and introducing auto-\\nmated technologies like AI-based image analysis tools to aid radiologists in quicker', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 1853}),\n",
       " Document(page_content='and more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.\\nIn recent years, deep learning models have rapidly advanced in various medi-\\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\\nysis because it can identify and precisely locate the types of anomalies in the images,\\n2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1, 'start_index': 2802}),\n",
       " Document(page_content='providing doctors with more specific and valuable information. However, training\\nthese models requires a large amount of annotated data. These annotations must be\\nperformed by experienced radiologists for CXR images, as well as for most medi-\\ncal images, making such annotated data not only costly, but also rare, with only a\\nvery limited number of public datasets including bounding box information. Although\\ntransfer learning is widely regarded as an effective method to solve the problem\\nof scarce labelling data, its application in medical image analysis still faces limita-\\ntions. This is mainly due to the significant difference in feature distribution between\\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\\ndatasets. This disparity suggests that directly applying these pre-trained weights to\\nmedical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 0}),\n",
       " Document(page_content='To fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\\nemploying a dual-phase training process to enhance CXR image analysis. The first\\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\\napplied to fine-tune the model. Our experiments show that such a training strategy\\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\\nmodel performance. We observed a 3% increase in model accuracy on the mean\\nAverage Precision benchmark, surpassing the results achieved by directly performing\\nconventional transfer learning from ImageNet pre-trained weights.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 963}),\n",
       " Document(page_content='This study extends the application of self-supervised learning to chest X-ray abnor-\\nmality localization. It demonstrates the potential of self-supervised learning in medical\\nimaging analysis, especially in the absence of annotated data. By effectively improving\\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\\nresents a significant advancement in the field of CXR abnormality localization, paving\\nthe way for more efficient and accurate diagnostic methods in the future.\\n2 Related Work\\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\\nmodels trained with vast amounts of labelled data are capable of identifying features', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 1937}),\n",
       " Document(page_content='of various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\\nimages [13]. This achievement is attributed to the advanced image recognition and\\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\\narchitecture which incorporates attention mechanisms, as proposed by Guszt´ av Ga´ al\\net al. [14], has made significant strides in accurately segmenting lung structures, thus\\naiding in detailed analysis and diagnosis of diseases.\\n3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2, 'start_index': 2881}),\n",
       " Document(page_content='Self-supervised learning has recently gained popularity in the field of medical imag-\\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\\nuseful feature representations by training deep networks on unsupervised data [16].\\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\\ntionships in data by designing innovative pretext tasks, such as image reconstruction\\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\\net al. used large-scale images for self-supervised learning to improve accuracy and con-\\nvergence speed significantly in downstream tasks, achieving better performance than\\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 0}),\n",
       " Document(page_content='based on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].\\nIn terms of transfer learning, applying models trained in one domain to another has\\nled to notable success in medical image analysis. Research indicates that well-processed\\ntransfer results from ImageNet can improve model performance in the medical imag-\\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\\nthe significant difference in feature distribution between medical and natural images,\\nfeatures learned from natural images may not always be broadly applicable to med-\\nical images [23]. Various cross-domain adaptive transfer learning methods have been\\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\\ning and sequential domain adaptation techniques. By tuning model parameters, these\\nmethods can be better adapted to the characteristics of medical images, improving', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 952}),\n",
       " Document(page_content='the performance and accuracy of models in medical image analysis [22].\\n3 Methods\\n3.1 Dataset Selection\\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\\n14 diseases with image-level annotations, including disease location annotations in\\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.\\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\\ning sets. The training set, consisting of 15,000 images, was annotated independently', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 1895}),\n",
       " Document(page_content='by three radiologists per image. In contrast, the test set, comprising 3,000 images,\\nunderwent a more rigorous annotation process. Initially, each image was independently\\nannotated by three radiologists. This is followed by a secondary review phase where\\nthese initial annotations are reviewed by two other more experienced radiologists, they\\ncommunicated with each other to resolve any disagreements and reach a consensus on\\n4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3, 'start_index': 2853}),\n",
       " Document(page_content='Countdisease/abnormalityNo Finding\\nInfiltration\\nEffusion\\nAtelectasis\\nNodule\\nMass\\nPneumothorax\\nConsolidation\\nPleural_Thickening\\nCardiomegaly\\nEmphysema\\nEdema\\nFibrosis\\nPneumonia\\nHernia\\n0 10000 20000 30000 40000 50000 60000 70000Fig. 1 Image-level label distribution of the NIH-CXR dataset.\\nthe final labelling. This meticulous process for the test set created a potential dispar-\\nity in data distribution compared to the training set. To eliminate any bias it might\\nintroduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.\\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\\ncalculating the weighted average of each set of duplicate bounding boxes to create a\\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4, 'start_index': 0}),\n",
       " Document(page_content='tion redundancy and improving target area representation in the dataset. Figure 2\\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\\nable dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.\\n3.2 Dual-Phase Training Process\\nOur training encompasses two primary phases: self-supervised pre-training and sub-\\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\\napproach refined the ImageNet pre-trained model by updating its backbone weights.\\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4, 'start_index': 924}),\n",
       " Document(page_content='aims to further improve the model’s task-specific performance, explicitly enhancing\\nits capabilities in localized diseases in CXR images.\\n5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4, 'start_index': 1887}),\n",
       " Document(page_content='a)\\nb)Fig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\\npreprocessing.\\n3.2.1 Self-Supervised Pre-training\\nFor the first stage of training, we employed the original version of the Barlow Twins\\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\\nconventional contrastive learning, introducing a self-supervised learning framework\\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\\na straightforward yet potent principle: it learns distinctive features by reducing the\\nrepresentational differences between two differently distorted images from the same\\nsource as processed by the network. This strategy is instrumental in enabling the model\\nto identify unique and rich features in each image while concurrently minimizing the\\noverlap in features. The process involves generating two distinct variants of an image\\nthrough data augmentation, followed by their simultaneous processing via two deep\\n6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 5, 'start_index': 0}),\n",
       " Document(page_content='Input\\nimageC2 C3 C4 C5\\nP2 P3 P4 P5Resnet50 Backbone\\nconvrpn_cls\\nrpn_reg\\nRoI poolingRegion Proposal Network\\nFC FC FCFeature map\\nFeature VectorFeature mapClass\\nBoxFeature\\nPyramid\\nNetworkX\\nY AY B\\nImagesDistorted\\nimages\\nZ A\\nZ BResnet 50\\nResnet 50Net Embeddings\\nEmporocal\\ncross-corr .Target\\ncross-corr .\\nI\\nLBT\\nfeature dimensionBarlow twins\\nFaster  R-CNNFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6, 'start_index': 0}),\n",
       " Document(page_content='multi-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\\nfor object detection tasks.\\nneural networks that share identical weights. The objective is to align the network’s\\nweights to enhance the similarity in the high-level representations of these image pairs\\nyet ensure that the individual features remain distinct and independent.\\nThe Barlow Twins method might be particularly useful for medical imaging\\nbecause it extracts features by minimizing the redundancy between representations of\\nperturbed images. In CXR imaging, subtle differences might indicate important health\\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\\nimportant features. In contrast to other contrastive learning algorithms like MoCo', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6, 'start_index': 912}),\n",
       " Document(page_content='[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\\nTwins works at the feature dimension. It aims to assign an independent meaning to\\neach feature dimension. This could lead to a richer feature representation, potentially\\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\\n7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6, 'start_index': 1896}),\n",
       " Document(page_content='pathological states). Moreover, compared to self-supervised learning methods requir-\\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\\noffers a more straightforward training framework, which is particularly important in\\nsituations with limited computational resources.\\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\\nfrom the Torchvision library, this step brings no additional cost. We used images from\\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.\\n3.2.2 Fine-tuning Phase\\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 0}),\n",
       " Document(page_content='Feature Pyramid Network (FPN) [28] as our object detector and trained it on the\\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\\nR-CNN detector employs these regions to detect and classify targets. This architecture\\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\\nPyramid Network (FPN), an architecture frequently employed in object detection,\\nparticularly enhances performance with multi-scale targets. It integrates high-level\\nsemantic information from deeper layers with detailed information from shallower lay-\\ners, producing feature maps of varied scales that effectively detect differently sized\\ntargets.\\nWe employed the MMdetection [29] machine learning toolbox as the platform for', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 935}),\n",
       " Document(page_content='Faster R-CNN, utilizing a number of classical image augmentation techniques and\\nmaintaining consistent hyperparameters across all experiments. Two different input\\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\\non the model’s performance with the pre-trained models. In addition, for comparison,\\nwe also conducted experiments using ImageNet pre-trained weights directly.\\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\\ncomprehensively evaluate the self-supervised learning model’s performance in medical\\nimaging. This method examines the model’s feature transfer capability - its ability to\\nadapt learned representations to new tasks. We first resplit the test set of the NIH\\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\\nand the remaining 20% as an evaluation test set for assessing model performance.\\nWe adopted two distinct strategies during the evaluation: freezing the backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 1859}),\n",
       " Document(page_content='weights or fine-tuning the weights. In the freezing backbone strategy, we kept the\\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\\nthe entire network, encompassing both the self-supervised trained feature extraction\\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\\n8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7, 'start_index': 2859}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier, allowing us to assess the\\nmodel’s performance across different scales of training data.\\nWhen evaluating the representation transfer ability of a self-supervised learning\\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\\ntest sets is consistent. We used the Iterative stratification for the multi-label data\\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\\nand test sets were roughly similar. This helped prevent biases due to uneven label\\ndistribution, making our evaluation results more reliable and convincing.\\n3.3 Results Analysis Process\\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\\nof our object detection models. mAP is a widely recognized and effective metric in', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 0}),\n",
       " Document(page_content='object detection, calculated by averaging precision scores across various object detec-\\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\\nscores for each class. The proportion of correct predictions relative to all predictions\\nfor a specific class across different detection confidence thresholds determines the pre-\\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\\nfor a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.\\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\\nprecision and recall, making it an especially appropriate performance indicator for\\nthis field. The AUC metric represents the area under the Receiver Operating Char-', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 933}),\n",
       " Document(page_content='acteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\\nfor imbalanced datasets as it does not rely directly on classification thresholds.\\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\\nlutional layer and observes the impact on the output class scores. This process reveals\\nthe most influential regions for the model’s decision-making. The resulting heat maps\\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 1890}),\n",
       " Document(page_content='4 Results\\n4.1 Transfer Learning on VinDr Abnormality Localization\\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\\n9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8, 'start_index': 2885}),\n",
       " Document(page_content='using two different input resolutions. Consistent hyperparameter settings were main-\\ntained across all experiments, ensuring that the performance changes were attributable\\nonly to the merits of the pretraining method itself. We visualized the performance of\\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\\non the validation set in Figure 4, and tabulated the corresponding mAP performance\\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\\nperformance baseline without pre-training benefits.\\na) b)\\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\\nbetween the lowest and highest value.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9, 'start_index': 0}),\n",
       " Document(page_content='Table 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.\\nBackBone weight Input size mAP50 (val set) mAP50 (test set)\\nbaseline nopretrained 224 0.1388 (0.1352,0.1424) 0.1342 (0.1306,0.1378)\\nImageNet pretrained 0.2245 (0.2204,0.2286) 0.2210 (0.2194,0.2226)\\nBarlow twins 0.2555 (0.2485,0.2626) 0.2448 (0.2414,0.2482)\\nBarlow twins from ImageNet 0.2625 (0.2568,0.2682) 0.2502 (0.2476,0.2528)\\nImageNet pretrained 640 0.2973 (0.2913,0.3033) 0.280 (0.2757,0.2848)\\nBarlow twins from ImageNet 0.3102 (0.3080,0.3125) 0.289 (0.2826,0.2954)\\n1Scores are presented with 95% confidence intervals.\\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\\npre-training in feature representation across disparate image domains.\\n10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9, 'start_index': 980}),\n",
       " Document(page_content='More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 0}),\n",
       " Document(page_content='with BarlowTwins-CXR, potentially offering more targeted information for clinical\\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\\nderived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.\\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\\nlow Twin-CXR weighted models saw performance improvements due to the increased\\ndetail in the CXR images. Nonetheless, the performance differential between the\\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.\\nThis points to intriguing future research avenues, such as refining image resolu-\\ntion parameters during pre-training and fine-tuning phases and investigating whether\\nhigher-resolution pre-training could elevate model performance. It also accentuates the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 973}),\n",
       " Document(page_content='necessity of tailoring deep learning model design to specific tasks, considering factors\\nlike image resolution and feature granularity.\\nOverall, implementing the Barlow Twins-CXR method on the VinDr dataset\\nresulted in substantial gains despite its data limitations and the inherent challenges\\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\\nimage weights for better applicability in CXR image analysis and beyond in medical\\nimaging.\\n4.2 Linear Evaluation Protocol\\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\\ntraditional ImageNet pre-training on the linear classification performance within the\\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 1954}),\n",
       " Document(page_content='of the linear classifier and updating only the final linear layer’s weights. This approach\\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\\nthese experiments are presented in Figure 6 and Table 2.\\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\\n11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10, 'start_index': 2950}),\n",
       " Document(page_content='Ground Truth Bbox ImageNet Weights Our method\\n20e27597c972c6e7fdb4d1e7638e227e\\n03431b577d1ccf075e930c4c4913c079\\nfd810298e165ef0b9a88bb25fda7a34bGround Truth Bbox ImageNet Weights Our method\\n9eba0d101f410f9cdfae46cb094ae2a6\\n87a8df2f22475c7200ebe891d0f25b88\\nad86f42123384e2441cce36347aa7d1aa) b)Fig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\\nbelow the heatmaps refer to the image numbers in the dataset.\\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\\nmethods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\\nImage-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11, 'start_index': 0}),\n",
       " Document(page_content='1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\\nimage analysis, particularly when annotated data is scarce.\\n12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11, 'start_index': 974}),\n",
       " Document(page_content='Fig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.\\n4.3 End-to-End Finetuning\\nIn our end-to-end experiments, where we permitted updates to all model layers, the\\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\\nImageNet pre-trained equivalent across all training set sizes. The results of these\\nexperiments are presented in Figure 7 and Table 3.\\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12, 'start_index': 0}),\n",
       " Document(page_content='dataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.\\n13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12, 'start_index': 910}),\n",
       " Document(page_content='Table 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\\npre-training methods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627) 0.7756 (0.7745, 0.7768) 0.8107 (0.8098, 0.8116)\\nImage-Net 0.6163 (0.6110, 0.6216) 0.7168 (0.7093, 0.7243) 0.7866 (0.7843, 0.7889)\\n1Scores are presented with 95% confidence intervals.\\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\nAUC than the ImageNet counterpart.\\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\\nover the frozen backbone setup was less marked, suggesting that the wealth of features\\nlearned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.\\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13, 'start_index': 0}),\n",
       " Document(page_content='across all model layers may elevate the risk of overfitting, particularly when data is\\nscarce. The narrowing performance differential between the two pre-training strategies\\nwith increasing data volume indicates that the distinction between domain-specific\\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\\ntial with larger datasets. This trend implies that the influence of the pre-training\\nstrategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.\\n5 Discussion\\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\\nunannotated CXR images for learning valuable representations and enhances trans-\\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\\nThis leads to quicker training and improved performance on tasks like abnormality\\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13, 'start_index': 939}),\n",
       " Document(page_content='resolutions, outshining models pre-trained on ImageNet.\\nOne of the primary limitations of our study is the scarcity of CXR datasets with\\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\\nmay limit the generalizability of our findings. Additionally, the computational cost\\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\\nimages with an image size of 224*224 pixels, the training process required two days\\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\\nability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.\\n6 Future Work\\nOur future endeavours include developing a demo interactive system for deployment\\nand testing in emergency rooms. It will allow practical evaluation of the model’s\\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\\n14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13, 'start_index': 1889}),\n",
       " Document(page_content='Additionally, we plan to explore more advanced self-supervised learning methods,\\nobject detection frameworks, and backbone networks to refine our approach further.\\nThe continuous evolution of these technologies promises to address some of the current\\nlimitations and expand the applicability and accuracy of our model in medical image\\nanalysis.\\n7 Conclusions\\nThe results of this study provide strong support for the application of self-supervised\\nlearning in the field of abnormality detection, especially valuable in environments\\nwhere radiologists face high workloads but the corresponding data labelling resources\\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\\nCXR image, attributable to differences in imaging equipment, patient demographics,\\nand other locale-specific factors [34][35]. Such variations often impede the cross-\\nregional applicability of a model, thus limiting its generalizability. By employing the', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14, 'start_index': 0}),\n",
       " Document(page_content='BarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\\nnetworks to local datasets tailored to the unique characteristics of their regional data.\\nOur findings might also have significant implications for clinical practice, suggest-\\ning that this strategy could be a game-changer in aiding radiologists to interpret\\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\\ntially increasing patients’ throughput and improving the overall quality of care.\\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\\nholds particular promise in areas where standardization of medical imaging presents\\nchallenges.\\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.\\n8 Abbreviations\\nAP: anterior-posterior', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14, 'start_index': 965}),\n",
       " Document(page_content='AUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping\\nCIUSSS: Centre int´ egr´ e universitaire de sant´ e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer\\nFPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic\\nROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate\\n15', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14, 'start_index': 1962}),\n",
       " Document(page_content='WBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n9 Declarations\\n9.1 Ethics approval and consent to participate\\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\\nrations of Helsinki). The studies reported in this manuscript used reputable public\\ndatasets and did not require any additional data involving human participants, human\\ndata, or human tissue.\\n9.2 Consent for publication\\nNot applicable\\n9.3 Availability of data and materials\\nThe datasets generated and/or analysed during the current study are available in the\\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\\n9.4 Competing interests\\nThe authors declare that they have no competing interests\\n9.5 Funding\\nNo external funding was associated with this research study.\\n9.6 Authors’ contributions\\nHS designed the research methodology, analyzed data, was responsible for experiments\\nand results visualization, and participated in manuscript drafting and revision. LM', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15, 'start_index': 0}),\n",
       " Document(page_content='assisted in developing the research methodology and contributed to the drafting and\\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\\nin statistical analysis. DL contributed to the study design, offered statistical analysis\\nexpertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.\\nAll authors read and approved the final manuscript.\\n9.7 Acknowledgements\\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆ ıle-de-\\nmontr´ eal for the computational resources and support provided, which were essential\\nfor the research conducted as part of the graduate internship program. We are espe-\\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\\nTheir expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.\\n16', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15, 'start_index': 971}),\n",
       " Document(page_content='References\\n[1] Satia, I., Bashagha, S., Bibi, A., et al. : Assessing the accuracy and certainty in\\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502\\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al. : The role of chest imaging\\nin patient management during the covid-19 pandemic: a multinational consensus\\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978\\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\\nTrend in radiologist workload compared to number of admissions in the emer-\\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337\\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\\ndate-of-access (2022)\\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al. : Effect of a comprehensive\\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16, 'start_index': 0}),\n",
       " Document(page_content='a retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054\\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al. : Transfer learning for medical\\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7\\n[8] Zbontar, J., Jing, L., Misra, I., et al. : Barlow twins: Self-supervised learning\\nvia redundancy reduction. In: Proceedings of the International Conference on\\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230\\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16, 'start_index': 958}),\n",
       " Document(page_content='Computer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al. : Vindr-cxr: An open dataset of chest\\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w\\n17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16, 'start_index': 1934}),\n",
       " Document(page_content='[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90\\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225\\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\\n[14] Ga´ al, G., Maga, B., Luk´ acs, A.: Attention u-net based adversarial architectures\\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17, 'start_index': 0}),\n",
       " Document(page_content='[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045\\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al. : Greedy layer-wise training of deep\\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476\\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al. : Context encoders: Feature\\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278\\n[18] Chen, T., Kornblith, S., Norouzi, M., et al. : A simple framework for contrastive\\nlearning of visual representations. In: Proceedings of the International Conference\\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17, 'start_index': 973}),\n",
       " Document(page_content='3525087\\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728\\n[20] Azizi, S., Mustafa, B., Ryan, F., et al. : Big self-supervised models advance medical\\nimage classification. In: Proceedings of the IEEE/CVF International Conference\\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346\\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17, 'start_index': 1970}),\n",
       " Document(page_content='representation and transferability of chest x-ray models. In: Medical Imaging with\\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\\ntransfer learning work for medical images: Feature reuse & other factors. In:\\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901\\n[24] Wang, X., Peng, Y., Lu, L., et al. : Chestx-ray8: Hospital-scale chest x-ray\\ndatabase and benchmarks on weakly-supervised classification and localization\\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18, 'start_index': 0}),\n",
       " Document(page_content='Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\\nfrom different object detection models. Image and Vision Computing 107, 104117\\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\\n[26] He, K., Fan, H., Wu, Y., et al. : Momentum contrast for unsupervised visual\\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975\\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169\\n[28] Lin, T. Y., Doll´ ar, P., Girshick, R., et al. : Feature pyramid networks for object\\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18, 'start_index': 978}),\n",
       " Document(page_content='Pattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106\\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155\\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\\nmizing mutual information across views. In: Proceedings of the 33rd International\\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\\n19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18, 'start_index': 1950}),\n",
       " Document(page_content='[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277\\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)\\n[33] Szyma´ nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\\nlabel data. Proceedings of the First International Workshop on Learning with\\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756\\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19, 'start_index': 0}),\n",
       " Document(page_content='[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19, 'start_index': 965})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Level2\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, add_start_index=True)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retriever(doc):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "    all_splits = text_splitter.split_documents(doc)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma.from_documents(all_splits, embeddings)\n",
    "    retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    return retriever\n",
    "retriever_pre = make_retriever(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_pre.invoke(\"What is the main topic of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Retrieval-Augmented Generation for Large Language Models: A Survey\\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\\nDai1,Jiawei Sun1,Qianyu Guo4,Meng Wang3and Haofen Wang1,3∗\\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n3College of Design and Innovation, Tongji University\\n4School of Computer Science, Fudan University\\nAbstract\\nLarge Language Models (LLMs) demonstrate\\nsignificant capabilities but face challenges such\\nas hallucination, outdated knowledge, and non-\\ntransparent, untraceable reasoning processes.\\nRetrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating\\nknowledge from external databases. This enhances\\nthe accuracy and credibility of the models, particu-\\nlarly for knowledge-intensive tasks, and allows for\\ncontinuous knowledge updates and integration of\\ndomain-specific information. RAG synergistically\\nmerges LLMs’ intrinsic knowledge with the vast,\\ndynamic repositories of external databases. This\\ncomprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms,\\nencompassing the Naive RAG, the Advanced RAG,\\nand the Modular RAG. It meticulously scrutinizes\\nthe tripartite foundation of RAG frameworks,\\nwhich includes the retrieval , the generation and\\nthe augmentation techniques. The paper highlights\\nthe state-of-the-art technologies embedded in\\neach of these critical components, providing a\\nprofound understanding of the advancements in\\nRAG systems. Furthermore, this paper introduces\\nthe metrics and benchmarks for assessing RAG\\nmodels, along with the most up-to-date evaluation\\nframework. In conclusion, the paper delineates\\nprospective avenues for research, including the\\nidentification of challenges, the expansion of\\nmulti-modalities, and the progression of the RAG\\ninfrastructure and its ecosystem.1.\\n1 Introduction\\nLarge language models (LLMs) such as the GPT se-\\nries[Brown et al. , 2020, OpenAI, 2023 ]and the LLama se-\\nries [Touvron et al. , 2023 ], along with other models like\\nGemini [Google, 2023 ], have achieved remarkable suc-\\ncess in natural language processing, demonstrating supe-\\n∗Corresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Surveyrior performance on various benchmarks including Super-\\nGLUE [Wang et al. , 2019 ], MMLU [Hendrycks et al. , 2020 ],\\nand BIG-bench [Srivastava et al. , 2022 ]. Despite these\\nadvancements, LLMs exhibit notable limitations, par-\\nticularly in handling domain-specific or highly special-\\nized queries [Kandpal et al. , 2023 ]. A common issue is\\nthe generation of incorrect information, or ”hallucina-\\ntions” [Zhang et al. , 2023b ], especially when queries extend\\nbeyond the model’s training data or necessitate up-to-date in-\\nformation. These shortcomings underscore the impractical-\\nity of deploying LLMs as black-box solutions in real-world\\nproduction environments without additional safeguards. One\\npromising approach to mitigate these limitations is Retrieval-\\nAugmented Generation (RAG), which integrates external\\ndata retrieval into the generative process, thereby enhancing\\nthe model’s ability to provide accurate and relevant responses.\\nRAG, introduced by Lewis et al. [Lewis et al. , 2020 ]in\\nmid-2020, stands as a paradigm within the realm of LLMs,\\nenhancing generative tasks. Specifically, RAG involves an\\ninitial retrieval step where the LLMs query an external data\\nsource to obtain relevant information before proceeding to an-\\nswer questions or generate text. This process not only informs\\nthe subsequent generation phase but also ensures that the re-\\nsponses are grounded in retrieved evidence, thereby signif-\\nicantly enhancing the accuracy and relevance of the output.\\nThe dynamic retrieval of information from knowledge bases\\nduring the inference phase allows RAG to address issues such\\nas the generation of factually incorrect content, commonly\\nreferred to as “hallucinations.” The integration of RAG into\\nLLMs has seen rapid adoption and has become a pivotal tech-\\nnology in refining the capabilities of chatbots and rendering\\nLLMs more viable for practical applications.\\nThe evolutionary trajectory of RAG unfolds across four\\ndistinctive phases, as illustrated in Figure 1. In its in-\\nception in 2017, aligned with the emergence of the Trans-\\nformer architecture, the primary thrust was on assimilating\\nadditional knowledge through Pre-Training Models (PTM)\\nto augment language models. This epoch witnessed RAG’s\\nfoundational efforts predominantly directed at optimizing\\npre-training methodologies.\\nFollowing this initial phase, a period of relative dormancy\\nensued before the advent of chatGPT, during which there was\\nminimal advancement in related research for RAG. The sub-\\nsequent arrival of chatGPT marked a pivotal moment in thearXiv:2312.10997v4  [cs.CL]  5 Jan 2024', metadata={'source': '../documents/2312.10997.pdf', 'page': 0}),\n",
       " Document(page_content='Figure 1: Technology tree of RAG research development featuring representative works\\ntrajectory, propelling LLMs into the forefront. The com-\\nmunity’s focal point shifted towards harnessing the capabil-\\nities of LLMs to attain heightened controllability and ad-\\ndress evolving requirements. Consequently, the lion’s share\\nof RAG endeavors concentrated on inference, with a minor-\\nity dedicated to fine-tuning processes. As LLM capabili-\\nties continued to advance, especially with the introduction of\\nGPT-4, the landscape of RAG technology underwent a sig-\\nnificant transformation. The emphasis evolved into a hybrid\\napproach, combining the strengths of RAG and fine-tuning,\\nalongside a dedicated minority continuing the focus on opti-\\nmizing pre-training methodologies.\\nDespite the rapid growth of RAG research, there has been\\na lack of systematic consolidation and abstraction in the field,\\nwhich poses challenges in understanding the comprehensive\\nlandscape of RAG advancements. This survey aims to out-\\nline the entire RAG process and encompass the current and\\nfuture directions of RAG research, by providing a thorough\\nexamination of retrieval augmentation in LLMs.\\nTherefore, this paper aims to comprehensively summarize\\nand organize the technical principles, developmental history,\\ncontent, and, in particular, the relevant methods and applica-\\ntions after the emergence of LLMs, as well as the evaluation\\nmethods and application scenarios of RAG. It seeks to pro-vide a comprehensive overview and analysis of existing RAG\\ntechnologies and offer conclusions and prospects for future\\ndevelopment methods. This survey intends to furnish readers\\nand practitioners with a thorough and systematic comprehen-\\nsion of large models and RAG, elucidate the progression and\\nkey technologies of retrieval augmentation, clarify the merits\\nand limitations of various technologies along with their suit-\\nable contexts, and forecast potential future developments.\\nOur contributions are as follows:\\n• We present a thorough and systematic review of the\\nstate-of-the-art RAG, delineating its evolution through\\nparadigms including naive RAG, advanced RAG, and\\nmodular RAG. This review contextualizes the broader\\nscope of RAG research within the landscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generator” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We construct a thorough evaluation framework for RAG,\\noutlining the evaluation objectives and metrics. Our\\ncomparative analysis clarifies the strengths and weak-\\nnesses of RAG compared to fine-tuning from various', metadata={'source': '../documents/2312.10997.pdf', 'page': 1}),\n",
       " Document(page_content='perspectives. Additionally, we anticipate future direc-\\ntions for RAG, emphasizing potential enhancements to\\ntackle current challenges, expansions into multi-modal\\nsettings, and the development of its ecosystem.\\nThe paper unfolds as follows: Section 2 and 3 define RAG\\nand detail its developmental process. Section 4 through 6 ex-\\nplore core components—Retrieval, “Generation” and “Aug-\\nmentation”—highlighting diverse embedded technologies.\\nSection 7 focuses on RAG’s evaluation system. Section 8\\ncompare RAG with other LLM optimization methods and\\nsuggest potential directions for its evolution. The paper con-\\ncludes in Section 9.\\n2 Definition\\nThe definition of RAG can be summarized from its workflow.\\nFigure 2 depicts a typical RAG application workflow. In this\\nscenario, a user inquires ChatGPT about a recent high-profile\\nevent (i.e., the abrupt dismissal and reinstatement of Ope-\\nnAI’s CEO) which generated considerable public discourse.\\nChatGPT as the most renowned and widely utilized LLM,\\nconstrained by its pretraining data, lacks knowledge of re-\\ncent events. RAG addresses this gap by retrieving up-to-date\\ndocument excerpts from external knowledge bases. In this in-\\nstance, it procures a selection of news articles pertinent to the\\ninquiry. These articles, alongside the initial question, are then\\namalgamated into an enriched prompt that enables ChatGPT\\nto synthesize an informed response. This example illustrates\\nthe RAG process, demonstrating its capability to enhance the\\nmodel’s responses with real-time information retrieval.\\nTechnologically, RAG has been enriched through various\\ninnovative approaches addressing pivotal questions such as\\n“what to retrieve” “when to retrieve” and “how to use the\\nretrieved information”. For “what to retrieve” research has\\nprogressed from simple token [Khandelwal et al. , 2019 ]and\\nentity retrieval [Nishikawa et al. , 2022 ]to more complex\\nstructures like chunks [Ram et al. , 2023 ]and knowledge\\ngraph [Kang et al. , 2023 ], with studies focusing on the\\ngranularity of retrieval and the level of data structur-\\ning. Coarse granularity brings more information but\\nwith lower precision. Retrieving structured text provides\\nmore information while sacrificing efficiency. The ques-\\ntion of “when to retrieve” has led to strategies ranging\\nfrom single [Wang et al. , 2023e, Shi et al. , 2023 ]to adap-\\ntive [Jiang et al. , 2023b, Huang et al. , 2023 ]and multiple\\nretrieval [Izacard et al. , 2022 ]methods. High frequency of\\nretrieval brings more information and lower efficiency. As\\nfor ”how to use” the retrieved data, integration techniques\\nhave been developed across various levels of the model\\narchitecture, including the input [Khattab et al. , 2022 ],\\nintermediate [Borgeaud et al. , 2022 ], and output lay-\\ners[Liang et al. , 2023 ]. Although the “intermediate” and\\n“output layers” are more effective, there are problems with\\nthe need for training and low efficiency.\\nRAG is a paradigm that enhances LLMs by integrating ex-\\nternal knowledge bases. It employs a synergistic approach,\\ncombining information retrieval mechanisms and In-Context\\nLearning (ICL) to bolster the LLM’s performance. In this\\nframework, a query initiated by a user prompts the retrieval ofpertinent information via search algorithms. This information\\nis then woven into the LLM’s prompts, providing additional\\ncontext for the generation process. RAG’s key advantage lies\\nin its obviation of the need for retraining of LLMs for task-\\nspecific applications. Developers can instead append an ex-\\nternal knowledge repository, enriching the input and thereby\\nrefining the model’s output precision. RAG has become one\\nof the most popular architectures in LLMs’ systems, due to\\nits high practicality and low barrier to entry, with many con-\\nversational products being built almost entirely on RAG.\\nThe RAG workflow comprises three key steps. First, the\\ncorpus is partitioned into discrete chunks, upon which vec-\\ntor indices are constructed utilizing an encoder model. Sec-\\nond, RAG identifies and retrieves chunks based on their vec-\\ntor similarity to the query and indexed chunks. Finally, the\\nmodel synthesizes a response conditioned on the contextual\\ninformation gleaned from the retrieved chunks. These steps\\nform the fundamental framework of the RAG process, under-\\npinning its information retrieval and context-aware genera-\\ntion capabilities. Next, we will provide an introduction to the\\nRAG research framework.\\n3 RAG Framework\\nThe RAG research paradigm is continuously evolving, and\\nthis section primarily delineates its progression. We cate-\\ngorize it into three types: Naive RAG, Advanced RAG, and\\nModular RAG. While RAG were cost-effective and surpassed\\nthe performance of the native LLM, they also exhibited sev-\\neral limitations. The development of Advanced RAG and\\nModular RAG was a response to these specific shortcomings\\nin Naive RAG.\\n3.1 Naive RAG\\nThe Naive RAG research paradigm represents the earliest\\nmethodology, which gained prominence shortly after the\\nwidespread adoption of ChatGPT. The Naive RAG follows a\\ntraditional process that includes indexing, retrieval, and gen-\\neration. It is also characterized as a “Retrieve-Read” frame-\\nwork [Maet al. , 2023a ].\\nIndexing\\nThe indexing process is a crucial initial step in data prepara-\\ntion that occurs offline and involves several stages. It begins\\nwith data indexing, where original data is cleansed and ex-\\ntracted, and various file formats such as PDF, HTML, Word,\\nand Markdown are converted into standardized plain text. In\\norder to fit within the context limitations of language models,\\nthis text is then segmented into smaller, more manageable\\nchunks in a process known as chunking. These chunks are\\nsubsequently transformed into vector representations through\\nan embedding model, chosen for its balance between infer-\\nence efficiency and model size. This facilitates similarity\\ncomparisons during the retrieval phase. Finally, an index is\\ncreated to store these text chunks and their vector embed-\\ndings as key-value pairs, which allows for efficient and scal-\\nable search capabilities.\\nRetrieval\\nUpon receipt of a user query, the system employs the same en-\\ncoding model utilized during the indexing phase to transcode', metadata={'source': '../documents/2312.10997.pdf', 'page': 2}),\n",
       " Document(page_content='Figure 2: A representative instance of the RAG process applied to question answering\\nthe input into a vector representation. It then proceeds to\\ncompute the similarity scores between the query vector and\\nthe vectorized chunks within the indexed corpus. The system\\nprioritizes and retrieves the top K chunks that demonstrate\\nthe greatest similarity to the query. These chunks are subse-\\nquently used as the expanded contextual basis for addressing\\nthe user’s request.\\nGeneration\\nThe posed query and selected documents are synthesized into\\na coherent prompt to which a large language model is tasked\\nwith formulating a response. The model’s approach to an-\\nswering may vary depending on task-specific criteria, allow-\\ning it to either draw upon its inherent parametric knowledge\\nor restrict its responses to the information contained within\\nthe provided documents. In cases of ongoing dialogues,\\nany existing conversational history can be integrated into the\\nprompt, enabling the model to engage in multi-turn dialogue\\ninteractions effectively.\\nDrawbacks in Naive RAG\\nNaive RAG faces significant challenges in three key areas:\\n“Retrieval,” “Generation,” and “Augmentation”.\\nRetrieval quality poses diverse challenges, including low\\nprecision, leading to misaligned retrieved chunks and po-\\ntential issues like hallucination or mid-air drop. Low recall\\nalso occurs, resulting in the failure to retrieve all relevant\\nchunks, thereby hindering the LLMs’ ability to craft compre-hensive responses. Outdated information further compounds\\nthe problem, potentially yielding inaccurate retrieval results.\\nResponse generation quality presents hallucination chal-\\nlenge, where the model generates answers not grounded in\\nthe provided context, as well as issues of irrelevant context\\nand potential toxicity or bias in the model’s output.\\nThe augmentation process presents its own challenges in\\neffectively integrating context from retrieved passages with\\nthe current generation task, potentially leading to disjointed\\nor incoherent output. Redundancy and repetition are also\\nconcerns, especially when multiple retrieved passages con-\\ntain similar information, resulting in repetitive content in the\\ngenerated response.\\nDiscerning the importance and relevance of multiple re-\\ntrieved passages to the generation task is another challenge,\\nrequiring the proper balance of each passage’s value. Addi-\\ntionally, reconciling differences in writing styles and tones to\\nensure consistency in the output is crucial.\\nLastly, there’s a risk of generation models overly depend-\\ning on augmented information, potentially resulting in out-\\nputs that merely reiterate the retrieved content without pro-\\nviding new value or synthesized information.\\n3.2 Advanced RAG\\nAdvanced RAG has been developed with targeted enhance-\\nments to address the shortcomings of Naive RAG. In terms\\nof retrieval quality, Advanced RAG implements pre-retrieval', metadata={'source': '../documents/2312.10997.pdf', 'page': 3}),\n",
       " Document(page_content='and post-retrieval strategies. To address the indexing chal-\\nlenges experienced by Naive RAG, Advanced RAG has re-\\nfined its indexing approach using techniques such as slid-\\ning window, fine-grained segmentation, and metadata. It has\\nalso introduced various methods to optimize the retrieval pro-\\ncess [ILIN, 2023 ].\\nPre-Retrieval Process\\nOptimizing Data Indexing .The goal of optimizing data index-\\ning is to enhance the quality of the content being indexed.\\nThis involves five primary strategies: enhancing data gran-\\nularity, optimizing index structures, adding metadata, align-\\nment optimization, and mixed retrieval.\\nEnhancing data granularity aims to elevate text standard-\\nization, consistency, factual accuracy, and rich context to im-\\nprove the RAG system’s performance. This includes remov-\\ning irrelevant information, dispelling ambiguity in entities\\nand terms, confirming factual accuracy, maintaining context,\\nand updating outdated documents.\\nOptimizing index structures involves adjusting the size of\\nchunks to capture relevant context, querying across multiple\\nindex paths, and incorporating information from the graph\\nstructure to capture relevant context by leveraging relation-\\nships between nodes in a graph data index.\\nAdding metadata information involves integrating refer-\\nenced metadata, such as dates and purposes, into chunks for\\nfiltering purposes, and incorporating metadata like chapters\\nand subsections of references to improve retrieval efficiency.\\nAlignment optimization addresses alignment issues and\\ndisparities between documents by introducing “hypothetical\\nquestions” [Liet al. , 2023d ]into documents to rectify align-\\nment issues and differences.\\nRetrieval\\nDuring the retrieval stage, the primary focus is on identifying\\nthe appropriate context by calculating the similarity between\\nthe query and chunks. The embedding model is central to\\nthis process. In the advanced RAG, there is potential for op-\\ntimization of the embedding models.\\nFine-tuning Embedding . Fine-tuning embedding models\\nsignificantly impact the relevance of retrieved content in RAG\\nsystems. This process involves customizing embedding mod-\\nels to enhance retrieval relevance in domain-specific contexts,\\nespecially for professional domains dealing with evolving or\\nrare terms. The BGE embedding model [BAAI, 2023 ], such\\nas BGE-large-EN developed by BAAI2, is an example of a\\nhigh-performance embedding model that can be fine-tuned\\nto optimize retrieval relevance. Training data for fine-tuning\\ncan be generated using language models like GPT-3.5-turbo\\nto formulate questions grounded on document chunks, which\\nare then used as fine-tuning pairs.\\nDynamic Embedding adapts to the context in which words\\nare used, unlike static embedding, which uses a single vec-\\ntor for each word [Karpukhin et al. , 2020 ]. For example,\\nin transformer models like BERT, the same word can have\\nvaried embeddings depending on surrounding words. Ope-\\nnAI’s embeddings-ada-02 model3, built upon the principles\\n2https://huggingface.co/BAAI/bge-large-en\\n3https://platform.openai.com/docs/guides/embeddingsof LLMs like GPT, is a sophisticated dynamic embedding\\nmodel that captures contextual understanding. However, it\\nmay not exhibit the same sensitivity to context as the latest\\nfull-size language models like GPT-4.\\nPost-Retrieval Process\\nAfter retrieving valuable context from the database, it is es-\\nsential to merge it with the query as an input into LLMs while\\naddressing challenges posed by context window limits. Sim-\\nply presenting all relevant documents to the LLM at once may\\nexceed the context window limit, introduce noise, and hinder\\nthe focus on crucial information. Additional processing of the\\nretrieved content is necessary to address these issues.\\nRe-Ranking . Re-ranking the retrieved information to re-\\nlocate the most relevant content to the edges of the prompt\\nis a key strategy. This concept has been implemented\\nin frameworks such as LlamaIndex4, LangChain5, and\\nHayStack [Blagojevi, 2023 ]. For example, Diversity Ranker6\\nprioritizes reordering based on document diversity, while\\nLostInTheMiddleRanker alternates placing the best docu-\\nment at the beginning and end of the context window. Ad-\\nditionally, approaches like cohereAI rerank [Cohere, 2023 ],\\nbge-rerank7, and LongLLMLingua [Jiang et al. , 2023a ]re-\\ncalculate the semantic similarity between relevant text and the\\nquery, addressing the challenge of interpreting vector-based\\nsimulated searches for semantic similarity.\\nPrompt Compression . Research indicates that noise in re-\\ntrieved documents adversely affects RAG performance. In\\npost-processing, the emphasis lies in compressing irrelevant\\ncontext, highlighting pivotal paragraphs, and reducing the\\noverall context length. Approaches such as Selective Context\\nand LLMLingua [Litman et al. , 2020, Anderson et al. , 2022 ]\\nutilize small language models to calculate prompt mu-\\ntual information or perplexity, estimating element impor-\\ntance. Recomp [Xuet al. , 2023a ]addresses this by train-\\ning compressors at different granularities, while Long\\nContext [Xuet al. , 2023b ]and “Walking in the Memory\\nMaze” [Chen et al. , 2023a ]design summarization techniques\\nto enhance LLM’s key information perception, particularly in\\ndealing with extensive contexts.\\n3.3 Modular RAG\\nThe modular RAG structure diverges from the tradi-\\ntional Naive RAG framework, providing greater versatil-\\nity and flexibility. It integrates various methods to en-\\nhance functional modules, such as incorporating a search\\nmodule for similarity retrieval and applying a fine-tuning\\napproach in the retriever [Linet al. , 2023 ]. Restructured\\nRAG modules [Yuet al. , 2022 ]and iterative methodologies\\nlike[Shao et al. , 2023 ]have been developed to address spe-\\ncific issues. The modular RAG paradigm is increasingly be-\\ncoming the norm in the RAG domain, allowing for either a\\nserialized pipeline or an end-to-end training approach across\\nmultiple modules. The comparison of three RAG paradigms\\n4https://www.llamaindex.ai\\n5https://www.langchain.com/\\n6https://haystack.deepset.ai/blog/\\nenhancing-rag-pipelines-in-haystack\\n7https://huggingface.co/BAAI/bge-reranker-large', metadata={'source': '../documents/2312.10997.pdf', 'page': 4}),\n",
       " Document(page_content='Figure 3: Comparison between the three paradigms of RAG\\nis depicted in Figure 3. However, Modular RAG is not stan-\\ndalone. Advanced RAG is a specialized form of modular\\nRAG, and further, Naive RAG itself is a special case of Ad-\\nvanced RAG. The relationship among the three paradigms is\\none of inheritance and development.\\nNew Modules\\nSearch Module . In contrast to the similarity retrieval in\\nNaive/Advanced RAG, the Search Module is tailored to spe-\\ncific scenarios and incorporates direct searches on additional\\ncorpora. This integration is achieved using code generated\\nby the LLM, query languages such as SQL or Cypher, and\\nother custom tools. The data sources for these searches can\\ninclude search engines, text data, tabular data, and knowledge\\ngraphs [Wang et al. , 2023d ].\\nMemory Module . This module harnesses the memory ca-\\npabilities of the LLM to guide retrieval. The approach in-\\nvolves identifying memories most similar to the current input.\\nSelfmem [Cheng et al. , 2023b ]utilizes a retrieval-enhanced\\ngenerator to create an unbounded memory pool iteratively,\\ncombining the “original question” and “dual question”. By\\nemploying a retrieval-enhanced generative model that uses its\\nown outputs to improve itself, the text becomes more aligned\\nwith the data distribution during the reasoning process. Con-\\nsequently, the model’s own outputs are utilized instead of the\\ntraining data [Wang et al. , 2022a ].\\nFusion . RAG-Fusion [Raudaschl, 2023 ]enhances tradi-\\ntional search systems by addressing their limitations through\\na multi-query approach that expands user queries into mul-tiple, diverse perspectives using an LLM. This approach not\\nonly captures the explicit information users seek but also un-\\ncovers deeper, transformative knowledge. The fusion pro-\\ncess involves parallel vector searches of both original and\\nexpanded queries, intelligent re-ranking to optimize results,\\nand pairing the best outcomes with new queries. This sophis-\\nticated method ensures search results that align closely with\\nboth the explicit and implicit intentions of the user, leading to\\nmore insightful and relevant information discovery.\\nRouting . The RAG system’s retrieval process utilizes di-\\nverse sources, differing in domain, language, and format,\\nwhich can be either alternated or merged based on the sit-\\nuation [Liet al. , 2023b ]. Query routing decides the subse-\\nquent action to a user’s query, with options ranging from\\nsummarization, searching specific databases, or merging dif-\\nferent pathways into a single response. The query router also\\nchooses the appropriate data store for the query, which may\\ninclude various sources like vector stores, graph databases, or\\nrelational databases, or a hierarchy of indices—for instance, a\\nsummary index and a document block vector index for multi-\\ndocument storage. The query router’s decision-making is pre-\\ndefined and executed via LLMs calls, which direct the query\\nto the chosen index.\\nPredict . It addresses the common issues of redundancy\\nand noise in retrieved content. Instead of directly retrieving\\nfrom a data source, this module utilizes the LLM to generate\\nthe necessary context [Yuet al. , 2022 ]. The content produced\\nby the LLM is more likely to contain pertinent information\\ncompared to that obtained through direct retrieval.', metadata={'source': '../documents/2312.10997.pdf', 'page': 5}),\n",
       " Document(page_content='Task Adapter . This module focuses on adapting RAG to a\\nvariety of downstream tasks. UPRISE automates the retrieval\\nof prompts for zero-shot task inputs from a pre-constructed\\ndata pool, thereby enhancing universality across tasks and\\nmodels [Cheng et al. , 2023a ]. Meanwhile, PROMPTAGA-\\nTOR [Daiet al. , 2022 ]utilizes LLM as a few-shot query gen-\\nerator and, based on the generated data, creates task-specific\\nretrievers. By leveraging the generalization capability of\\nLLMs, it enables the development of task-specific end-to-end\\nretrievers with minimal examples.\\nNew Patterns\\nThe organizational structure of Modular RAG is highly adapt-\\nable, allowing for the substitution or rearrangement of mod-\\nules within the RAG process to suit specific problem contexts.\\nNaive RAG and Advanced RAG can both be considered as\\nbeing composed of some fixed modules. As illustrated in the\\nfigure 3, Naive RAG primarily consists of the “Retrieve” and\\n“Read” modules. A typical pattern of Advanced RAG builds\\nupon the foundation of Naive RAG by adding “Rewrite” and\\n“Rerank” modules. However, on the whole, modular RAG\\nenjoys greater diversity and flexibility.\\nCurrent research primarily explores two organizational\\nparadigms. The first involves adding or replacing modules,\\nwhile the second focuses on adjusting the organizational flow\\nbetween modules. This flexibility enables tailoring the RAG\\nprocess to effectively address a wide array of tasks.\\nAdding or Replacing Modules .The strategy of introducing\\nor substituting modules involves maintaining the core struc-\\nture of the Retrieval-Read process while integrating addi-\\ntional modules to enhance specific functionalities. The RRR\\nmodel [Maet al. , 2023a ]introduces the Rewrite-Retrieve-\\nRead process, utilizing the LLM performance as a reinforce-\\nment learning incentive for a rewriting module. This enables\\nthe rewriter to fine-tune retrieval queries, thereby improving\\nthe downstream task performance of the reader.\\nSimilarly, modules can be selectively swapped in method-\\nologies like Generate-Read [Yuet al. , 2022 ], where the\\nLLM’s generation module takes the place of the retrieval\\nmodule. The Recite-Read approach [Sunet al. , 2022 ]trans-\\nforms external retrieval into retrieval from model weights,\\nrequiring the LLM to initially memorize task-specific infor-\\nmation and subsequently produce output capable of handling\\nknowledge-intensive natural language processing tasks.\\nAdjusting the Flow between Modules . zheIn the realm\\nof module flow adjustment, there is a focus on enhancing\\nthe interaction between language models and retrieval mod-\\nels. DSP [Khattab et al. , 2022 ]introduces the Demonstrate-\\nSearch-Predict framework, treating the context learning sys-\\ntem as an explicit program rather than a final task prompt,\\nleading to more effective handling of knowledge-intensive\\ntasks. The ITER-RETGEN [Shao et al. , 2023 ]approach uti-\\nlizes generated content to guide retrieval, iteratively im-\\nplementing “retrieval-enhanced generation” and “generation-\\nenhanced retrieval” within a Retrieve-Read-Retrieve-Read\\nflow. This method demonstrates an innovative way of using\\none module’s output to improve the functionality of another.Optimizing the RAG Pipeline\\nThe optimization of the retrieval process aims to enhance the\\nefficiency and quality of information in RAG systems. Cur-\\nrent research focuses on integrating diverse search technolo-\\ngies, refining retrieval steps, incorporating cognitive back-\\ntracking, implementing versatile query strategies, and lever-\\naging embedding similarity. These efforts collectively strive\\nto achieve a balance between retrieval efficiency and the\\ndepth of contextual information in RAG systems.\\nHybrid Search Exploration . The RAG system optimizes its\\nperformance by intelligently integrating various techniques,\\nincluding keyword-based search, semantic search, and vec-\\ntor search. This approach leverages the unique strengths of\\neach method to accommodate diverse query types and infor-\\nmation needs, ensuring consistent retrieval of highly relevant\\nand context-rich information. The use of hybrid search serves\\nas a robust supplement to retrieval strategies, thereby enhanc-\\ning the overall efficacy of the RAG pipeline.\\nRecursive Retrieval and Query Engine . Recursive retrieval\\ninvolves acquiring smaller chunks during the initial retrieval\\nphase to capture key semantic meanings. Subsequently, larger\\nchunks containing more contextual information are provided\\nto the LLM in later stages of the process. This two-step re-\\ntrieval method helps to strike a balance between efficiency\\nand the delivery of contextually rich responses.\\nStepBack-prompt approach encourages the LLM to move\\naway from specific instances and engage in reasoning around\\nbroader concepts and principles [Zheng et al. , 2023 ]. Experi-\\nmental results demonstrate a significant performance increase\\nin various challenging, inference-based tasks when backward\\nprompts are used, highlighting their natural adaptability to the\\nRAG process. These retrieval-enhancing steps can be applied\\nboth in generating responses to backward prompts and in the\\nfinal question-answering process.\\nSub-Queries . Depending on the scenario, various query\\nstrategies can be employed, such as using query engines\\nprovided by frameworks like LlamaIndex, leveraging tree\\nqueries, utilizing vector queries, or executing simple sequen-\\ntial querying of chunks.\\nHypothetical Document Embeddings . HyDE operates on\\nthe belief that the answers generated might be closer in the\\nembedding space than a direct query. Using the LLM, HyDE\\ncreates a hypothetical document (answer) in response to a\\nquery, embeds this document, and uses the resulting em-\\nbedding to retrieve real documents similar to the hypotheti-\\ncal one. Instead of seeking embedding similarity based on\\nthe query, this approach focuses on the embedding similar-\\nity from one answer to another [Gao et al. , 2022 ]. However,\\nit might not consistently produce desirable outcomes, espe-\\ncially when the language model is unfamiliar with the subject\\nmatter, potentially leading to more instances with errors.\\n4 Retrieval\\nIn the context of RAG, it is crucial to efficiently retrieve rel-\\nevant documents from the data source. However, creating a\\nproficient retriever presents significant challenges. This sec-\\ntionelves into three fundamental questions: 1) How can we\\nachieve accurate semantic representations? 2) What methods', metadata={'source': '../documents/2312.10997.pdf', 'page': 6}),\n",
       " Document(page_content='can align the semantic spaces of queries and documents? 3)\\nHow can the retriever’s output be aligned with the preferences\\nof the Large Language Model?\\n4.1 Enhancing Semantic Representations\\nIn RAG, the semantic space is essential as it involves the mul-\\ntidimensional mapping of queries and documents. Retrieval\\naccuracy in this semantic space significantly impacts RAG\\noutcomes. This section will present two methods for building\\naccurate semantic spaces.\\nChunk optimization\\nWhen managing external documents, the initial step involves\\nbreaking them down into smaller chunks to extract fine-\\ngrained features, which are then embedded to represent their\\nsemantics. However, embedding overly large or excessively\\nsmall text chunks may lead to sub-optimal outcomes. There-\\nfore, identifying the optimal chunk size for documents within\\nthe corpus is crucial to ensuring the accuracy and relevance\\nof the retrieved results.\\nChoosing an appropriate chunking strategy requires care-\\nful consideration of several vital factors, such as the nature\\nof the indexed content, the embedding model and its opti-\\nmal block size, the expected length and complexity of user\\nqueries, and the specific application’s utilization of the re-\\ntrieved results. For instance, the selection of a chunking\\nmodel should be based on the content’s length—whether it\\nis longer or shorter. Additionally, different embedding mod-\\nels demonstrate distinct performance characteristics at vary-\\ning block sizes. For example, sentence-transformer performs\\nbetter with single sentences, while text-embedding-ada-002\\nexcels with blocks containing 256 or 512 tokens.\\nAdditionally, factors like the length and complexity of user\\ninput questions, and the specific needs of the application (e.g.,\\nsemantic search or question answering), have effect on the\\nchoice of a chunking strategy. This choice can be directly in-\\nfluenced by the token limits of the selected LLMs, requiring\\nadjustments to the block size. In reality, getting precise query\\nresults involves flexibly applying different chunking strate-\\ngies. There is no one-size-fits-all ”best” strategy, only the\\nmost appropriate one for a particular context.\\nCurrent research in RAG explores various block optimiza-\\ntion techniques aimed at improving both retrieval efficiency\\nand accuracy. One such approach involves the use of slid-\\ning window technology, enabling layered retrieval by merg-\\ning globally related information across multiple retrieval pro-\\ncesses. Another strategy, known as the “small2big” method,\\nutilizes small text blocks during the initial search phase and\\nsubsequently provides larger related text blocks to the lan-\\nguage model for processing.\\nThe abstract embedding technique prioritizes top K re-\\ntrieval based on document abstracts (or summaries), offering\\na comprehensive understanding of the entire document con-\\ntext. Additionally, the metadata filtering technique leverages\\ndocument metadata to enhance the filtering process. An in-\\nnovative approach, the graph indexing technique, transforms\\nentities and relationships into nodes and connections, sig-\\nnificantly improving relevance, particularly in the context of\\nmulti-hop problems.The combination of these diverse methods has led to no-\\ntable advancements, resulting in enhanced retrieval outcomes\\nand improved performance for RAG.\\nFine-tuning Embedding Models\\nOnce the appropriate size of chunks is determined, the\\nnext crucial step involves embedding these chunks and the\\nquery into the semantic space using an embedding model.\\nThe effectiveness of the embedding is critical as it impacts\\nthe model’s ability to represent the corpus. Recent re-\\nsearch has introduced prominent embedding models such as\\nAngIE, V oyage, BGE,etc [Li and Li, 2023, V oyageAI, 2023,\\nBAAI, 2023 ]. These models have undergone pre-training on\\nextensive corpora. However, their capability to accurately\\ncapture domain-specific information may be limited when ap-\\nplied to specialized domains.\\nMoreover, task-specific fine-tuning of embedding models\\nis essential to ensure that the model comprehends the user\\nquery in terms of content relevance. A model without fine-\\ntuning may not adequately address the requirements of a spe-\\ncific task. Consequently, fine-tuning an embedding model be-\\ncomes crucial for downstream applications. There are two\\nprimary paradigms in embedding fine-tuning methods.\\nDomain Knowledge Fine-tuning . To ensure that an embed-\\nding model accurately captures domain-specific information,\\nit is imperative to utilize domain-specific datasets for fine-\\ntuning. This process diverges from standard language model\\nfine-tuning, chiefly in the nature of the datasets involved.\\nTypically, the dataset for embedding model fine-tuning en-\\ncompasses three principal elements: queries, a corpus, and\\nrelevant documents. The model employs these queries to\\nidentify pertinent documents within the corpus. The effi-\\ncacy of the model is then gauged based on its ability to re-\\ntrieve these relevant documents in response to the queries.\\nThe dataset construction, model fine-tuning, and evalua-\\ntion phases each present distinct challenges. The LlamaIn-\\ndex[Liu, 2023 ]introduces a suite of pivotal classes and func-\\ntions designed to enhance the embedding model fine-tuning\\nworkflow, thereby simplifying these intricate processes. By\\ncurating a corpus infused with domain knowledge and lever-\\naging the methodologies offered, one can adeptly fine-tune an\\nembedding model to align closely with the specific require-\\nments of the target domain.\\nFine-tuning for Downstream Tasks . Fine-tuning embed-\\nding models for downstream tasks is a critical step in en-\\nhancing model performance. In the realm of utilizing RAG\\nfor these tasks, innovative methods have emerged to fine-\\ntune embedding models by harnessing the capabilities of\\nLLMs. For example, PROMPTAGATOR [Daiet al. , 2022 ]\\nutilizes the LLM as a few-shot query generator to cre-\\nate task-specific retrievers, addressing challenges in super-\\nvised fine-tuning, particularly in data-scarce domains. An-\\nother approach, LLM-Embedder [Zhang et al. , 2023a ], ex-\\nploits LLMs to generate reward signals for data across mul-\\ntiple downstream tasks. The retriever is fine-tuned with two\\ntypes of supervised signals: hard labels for the dataset and\\nsoft rewards from the LLMs. This dual-signal approach fos-\\nters a more effective fine-tuning process, tailoring the embed-\\nding model to diverse downstream applications.', metadata={'source': '../documents/2312.10997.pdf', 'page': 7}),\n",
       " Document(page_content='While these methods improve semantic representation\\nby incorporating domain knowledge and task-specific fine-\\ntuning, retrievers may not always exhibit optimal compatibil-\\nity with certain LLMs. To address this, some researchers have\\nexplored direct supervision of the fine-tuning process using\\nfeedback from LLMs. This direct supervision seeks to align\\nthe retriever more closely with the LLM, thereby improving\\nperformance on downstream tasks. A more comprehensive\\ndiscussion on this topic is presented in Section 4.3.\\n4.2 Aligning Queries and Documents\\nIn the context of RAG applications, retrievers may utilize\\na single embedding model for encoding both the query and\\nthe documents, or employ separate models for each. Addi-\\ntionally, the user’s original query may suffer from imprecise\\nphrasing and lack of semantic information. Therefore, it is\\ncrucial to align the semantic space of the user’s query with\\nthose of the documents. This section introduces two funda-\\nmental techniques aimed at achieving this alignment.\\nQuery Rewriting\\nQuery rewriting is a fundamental approach for aligning\\nthe semantics of a query and a document. Methods\\nsuch as Query2Doc and ITER-RETGEN leverage LLMs\\nto create a pseudo-document by combining the origi-\\nnal query with additional guidance [Wang et al. , 2023c,\\nShao et al. , 2023 ]. HyDE constructs query vectors using\\ntextual cues to generate a “hypothetical” document captur-\\ning essential patterns [Gao et al. , 2022 ]. RRR introduces a\\nframework that reverses the traditional retrieval and read-\\ning order, focusing on query rewriting [Maet al. , 2023a ].\\nSTEP-BACKPROMPTING enables LLMs to perform ab-\\nstract reasoning and retrieval based on high-level con-\\ncepts [Zheng et al. , 2023 ]. Additionally, the multi-query re-\\ntrieval method utilizes LLMs to generate and execute multiple\\nsearch queries simultaneously, advantageous for addressing\\ncomplex problems with multiple sub-problems.\\nEmbedding Transformation\\nBeyond broad strategies such as query rewriting, there exist\\nmore granular techniques specifically designed for embed-\\nding transformations. LlamaIndex [Liu, 2023 ]exemplifies\\nthis by introducing an adapter module that can be integrated\\nfollowing the query encoder. This adapter facilitates fine-\\ntuning, thereby optimizing the representation of query em-\\nbeddings to map them into a latent space that is more closely\\naligned with the intended tasks.\\nThe challenge of aligning queries with structured exter-\\nnal documents, particularly when addressing the incongruity\\nbetween structured and unstructured data, is addressed by\\nSANTA [Liet al. , 2023d ]. It enhances the retriever’s sen-\\nsitivity to structured information through two pre-training\\nstrategies: first, by leveraging the intrinsic alignment between\\nstructured and unstructured data to inform contrastive learn-\\ning in a structured-aware pre-training scheme; and second, by\\nimplementing Masked Entity Prediction. The latter utilizes\\nan entity-centric masking strategy that encourages language\\nmodels to predict and fill in the masked entities, thereby fos-\\ntering a deeper understanding of structured data.The issue of aligning queries with structured exter-\\nnal documents, especially when dealing with the dispar-\\nity between structured and unstructured data, is tackled by\\nSANTA [Liet al. , 2023d ]. This approach improves the re-\\ntriever’s ability to recognize structured information through\\ntwo pre-training strategies: firstly, by utilizing the inher-\\nent alignment between structured and unstructured data to\\nguide contrastive learning in a structured-aware pre-training\\nscheme; and secondly, by employing Masked Entity Predic-\\ntion. The latter uses an entity-centric masking strategy to\\nprompt language models to predict and complete the masked\\nentities, thus promoting a more profound comprehension of\\nstructured data.\\n4.3 Aligning Retriever and LLM\\nIn the RAG pipeline, enhancing retrieval hit rate through var-\\nious techniques may not necessarily improve the final out-\\ncome, as the retrieved documents may not align with the spe-\\ncific requirements of the LLMs. Therefore, this section in-\\ntroduces two methods aimed at aligning the retriever outputs\\nwith the preferences of the LLMs.\\nFine-tuning Retrievers\\nSeveral studies utilize feedback signals from LLMs to refine\\nretrieval models. For instance, AAR [Yuet al. , 2023b ]intro-\\nduces supervisory signals for a pre-trained retriever using an\\nencoder-decoder architecture. This is achieved by identifying\\nthe LM’s preferred documents through FiD cross-attention\\nscores. Subsequently, the retriever undergoes fine-tuning\\nwith hard negative sampling and standard cross-entropy loss.\\nUltimately, the refined retriever can be directly applied to en-\\nhance unseen target LMs, resulting in improved performance\\nin the target task. Additionally, it is suggested that LLMs\\nmay have a preference for focusing on readable rather than\\ninformation-rich documents.\\nREPLUG [Shiet al. , 2023 ]utilizes a retriever and an LLM\\nto calculate the probability distributions of the retrieved doc-\\numents and then performs supervised training by computing\\nthe KL divergence. This straightforward and effective train-\\ning method enhances the performance of the retrieval model\\nby using an LM as the supervisory signal, eliminating the\\nneed for specific cross-attention mechanisms.\\nUPRISE [Cheng et al. , 2023a ]also employs frozen LLMs\\nto fine-tune the prompt retriever. Both the LLM and the re-\\ntriever take prompt-input pairs as inputs and utilize the scores\\nprovided by the LLM to supervise the retriever’s training, ef-\\nfectively treating the LLM as a dataset labeler. In addition,\\nAtlas [Izacard et al. , 2022 ]proposes four methods of super-\\nvised fine-tuning embedding models:\\n•Attention Distillation . This approach employs cross-\\nattention scores generated by the LLM during output to\\ndistill the model’s knowledge.\\n•EMDR2 . By using the Expectation-Maximization algo-\\nrithm, this method trains the model with retrieved docu-\\nments as latent variables.\\n•Perplexity Distillation directly trains the model using the\\nperplexity of generated tokens as an indicator.', metadata={'source': '../documents/2312.10997.pdf', 'page': 8}),\n",
       " Document(page_content='•LOOP . This method presents a novel loss function based\\non the impact of document deletion on LLM prediction,\\noffering an efficient training strategy to better adapt the\\nmodel to specific tasks.\\nThese approaches aim to improve the synergy between the\\nretriever and the LLM, leading to enhanced retrieval perfor-\\nmance and more accurate responses to user inquiries.\\nAdapters\\nFine-tuning models may present challenges, such as integrat-\\ning functionality through an API or addressing constraints\\narising from limited local computational resources. Con-\\nsequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.\\nPRCA trains the adapter through a context extraction\\nphase and a reward-driven phase. The retriever’s out-\\nput is then optimized using a token-based autoregres-\\nsive strategy [Yang et al. , 2023b ]. The token filtering ap-\\nproach employs cross-attention scores to efficiently fil-\\nter tokens, selecting only the highest-scoring input to-\\nkens [Berchansky et al. , 2023 ].RECOMP introduces both ex-\\ntractive and generative compressors for summary generation.\\nThese compressors either select relevant sentences or syn-\\nthesize document information, creating summaries tailored to\\nmulti-document queries [Xuet al. , 2023a ].\\nFurthermore, PKG introduces an innovative method for in-\\ntegrating knowledge into white-box models via directive fine-\\ntuning [Luoet al. , 2023 ]. In this approach, the retriever mod-\\nule is directly substituted to generate relevant documents ac-\\ncording to a query. This method assists in addressing the dif-\\nficulties encountered during the fine-tuning process and en-\\nhances model performance.\\n5 Generation\\nA crucial component of RAG is its generator, which is re-\\nsponsible for converting retrieved information into coherent\\nand fluent text. Unlike traditional language models, RAG’s\\ngenerator sets itself apart by improving accuracy and rele-\\nvance via the incorporation of retrieved data. In RAG, the\\ngenerator’s input encompasses not only typical contextual in-\\nformation but also relevant text segments obtained through\\nthe retriever. This comprehensive input enables the generator\\nto gain a deep understanding of the question’s context, result-\\ning in more informative and contextually relevant responses.\\nFurthermore, the generator is guided by the retrieved text to\\nensure coherence between the generated content and the ob-\\ntained information. The diverse input data has led to targeted\\nefforts during the generation phase, all aimed at refining the\\nadaptation of the large model to the input data derived from\\nqueries and documents. In the following subsections, we will\\nexplore the introduction of the generator by delving into as-\\npects of post-retrieval processing and fine-tuning.\\n5.1 Post-retrieval with Frozen LLM\\nIn the realm of untunable LLMs , many studies rely on well-\\nestablished models like GPT-4 [OpenAI, 2023 ]to harness\\ntheir comprehensive internal knowledge for systematically\\nsynthesizing retrieved information from various documents.However, challenges persist with these large models, includ-\\ning limitations on context length and susceptibility to redun-\\ndant information. To tackle these issues, certain research en-\\ndeavors have turned their focus to post-retrieval processing.\\nPost-retrieval processing involves treating, filtering, or op-\\ntimizing the relevant information retrieved by the retriever\\nfrom a large document database. Its main goal is to enhance\\nthe quality of retrieval results, aligning them more closely\\nwith user needs or subsequent tasks. It can be viewed as a\\nreprocessing of the documents obtained during the retrieval\\nphase. Common operations in post-retrieval processing typi-\\ncally include information compression and result reranking.\\nInformation Compression\\nThe retriever excels at retrieving relevant information from a\\nvast knowledge base, but managing the substantial amount of\\ninformation within retrieval documents is a challenge. Ongo-\\ning research aims to extend the context length of large lan-\\nguage models to tackle this issue. However, current large\\nmodels still struggle with context limitations. Therefore,\\nthere are scenarios where condensing information becomes\\nnecessary. Information condensation is significant for reduc-\\ning noise, addressing context length restrictions, and enhanc-\\ning generation effects.\\nPRCA tackled this issue by training an information ex-\\ntractor [Yang et al. , 2023b ]. In the context extraction phase,\\nwhen provided with an input text Sinput , it is capable of\\nproducing an output sequence Cextracted that represents the\\ncondensed context from the input document. The train-\\ning process is designed to minimize the difference between\\nCextracted and the actual context Ctruth .\\nSimilarly, RECOMP adopts a comparable approach by\\ntraining an information condenser using contrastive learn-\\ning[Xuet al. , 2023a ]. Each training data point consists of\\none positive sample and five negative samples, and the en-\\ncoder undergoes training using contrastive loss throughout\\nthis process [Karpukhin et al. , 2020 ].\\nAnother study has taken a different approach by aim-\\ning to reduce the number of documents in order to im-\\nprove the accuracy of the model’s answers. In the study\\nby[Maet al. , 2023b ], they propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and Small\\nLanguage Models (SLMs). In this paradigm, SLMs serve as\\nfilters, while LLMs function as reordering agents. The re-\\nsearch shows that instructing LLMs to rearrange challeng-\\ning samples identified by SLMs leads to significant improve-\\nments in various Information Extraction (IE) tasks.\\nReranking\\nThe re-ranking model is pivotal in optimizing the document\\nset retrieved from the retriever. Language models often face\\nperformance declines when additional context is introduced,\\nand re-ranking effectively addresses this issue. The core con-\\ncept involves rearranging document records to prioritize the\\nmost relevant items at the top, thereby limiting the total num-\\nber of documents. This not only resolves the challenge of\\ncontext window expansion during retrieval but also enhances\\nretrieval efficiency and responsiveness.\\nThe re-ranking model assumes a dual role throughout\\nthe information retrieval process, functioning as both an', metadata={'source': '../documents/2312.10997.pdf', 'page': 9}),\n",
       " Document(page_content='optimizer and a refiner. It provides more effective and\\naccurate input for subsequent language model process-\\ning[Zhuang et al. , 2023 ].\\nContextual compression is incorporated into the reorder-\\ning process to offer more precise retrieval information. This\\nmethod entails reducing the content of individual documents\\nand filtering the entire document, with the ultimate goal of\\npresenting the most relevant information in the search results\\nfor a more focused and accurate display of pertinent content.\\n5.2 Fine-tuning LLM for RAG\\nOptimizing the generator within the RAG model is a critical\\naspect of its architecture. The generator’s role is to take the\\nretrieved information and produce relevant text, forming the\\nfinal output of the model. The optimization of the generator\\naims to ensure that the generated text is both natural and ef-\\nfectively leverages the retrieved documents to better meet the\\nuser’s query needs.\\nIn standard LLMs generation tasks, the input typically\\nconsists of a query. RAG stands out by incorporating not\\nonly a query but also various retrieved documents (struc-\\ntured/unstructured) by the retriever into the input. This ad-\\nditional information can significantly influence the model’s\\nunderstanding, particularly for smaller models. In such cases,\\nfine-tuning the model to adapt to the input of both query and\\nretrieved documents becomes crucial. Before presenting the\\ninput to the fine-tuned model, post-retrieval processing usu-\\nally occurs for the documents retrieved by the retriever. It is\\nessential to note that the fine-tuning method for the genera-\\ntor in RAG aligns with the general fine-tuning approach for\\nLLMs. In the following, we will briefly describe some rep-\\nresentative works involving data (formatted/unformatted) and\\noptimization functions.\\nGeneral Optimization Process\\nAs part of the general optimization process, the training data\\ntypically consists of input-output pairs, aiming to train the\\nmodel to produce the output ygiven the input x. In the work\\nof Self-Mem [Cheng et al. , 2023b ], a traditional training\\nprocess is employed, where given the input x, relevant\\ndocuments zare retrieved (selecting Top-1 in the paper), and\\nafter integrating ( x,z), the model generates the output y.\\nThe paper utilizes two common paradigms for fine-tuning,\\nnamely Joint-Encoder and Dual-Encoder [Arora et al. , 2023,\\nWang et al. , 2022b, Lewis et al. , 2020, Xia et al. , 2019,\\nCaiet al. , 2021, Cheng et al. , 2022 ].\\nIn the Joint-Encoder paradigm, a standard model based on\\nan encoder-decoder is used. Here, the encoder initially en-\\ncodes the input, and the decoder, through attention mecha-\\nnisms, combines the encoded results to generate tokens in\\nan autoregressive manner. On the other hand, in the Dual-\\nEncoder paradigm, the system sets up two independent en-\\ncoders, with each encoder encoding the input (query, con-\\ntext) and the document, respectively. The resulting out-\\nputs undergo bidirectional cross-attention processing by the\\ndecoder in sequence. Both architectures utilize the Trans-\\nformer [Vaswani et al. , 2017 ]as the foundational block and\\noptimize with Negative Log-Likelihood loss.Utilizing Contrastive Learning\\nIn the phase of preparing training data for language mod-\\nels, interaction pairs of input and output are usually created.\\nThis traditional method can lead to ”exposure bias,” where\\nthe model is only trained on individual, correct output ex-\\namples, thus restricting its exposure to a range of possible\\noutputs citesequence. This limitation can hinder the model’s\\nreal-world performance by causing it to overfit to the partic-\\nular examples in the training set, thereby reducing its ability\\nto generalize across various contexts.\\nTo mitigate exposure bias, SURGE [Kang et al. , 2023 ]\\nproposes the use of graph-text contrastive learning. This\\nmethod includes a contrastive learning objective that prompts\\nthe model to produce a range of plausible and coherent re-\\nsponses, expanding beyond the instances encountered in the\\ntraining data. This approach is crucial in reducing overfitting\\nand strengthening the model’s ability to generalize.\\nFor retrieval tasks that engage with structured data, the\\nSANTA framework [Liet al. , 2023d ]implements a tripartite\\ntraining regimen to effectively encapsulate both structural and\\nsemantic nuances. The initial phase focuses on the retriever,\\nwhere contrastive learning is harnessed to refine the query\\nand document embeddings.\\nSubsequently, the generator’s preliminary training stage\\nemploys contrastive learning to align the structured data with\\nits unstructured document descriptions. In a further stage of\\ngenerator training, the model acknowledges the critical role\\nof entity semantics in the representation learning of textual\\ndata for retrieval, as highlighted by [Sciavolino et al. , 2021,\\nZhang et al. , 2019 ]. This process commences with the identi-\\nfication of entities within the structured data, followed by the\\napplication of masks over these entities within the generator’s\\ninput data, thus setting the stage for the model to anticipate\\nand predict these masked elements.\\nThe training regimen progresses with the model learning\\nto reconstruct the masked entities by leveraging contextual\\ninformation. This exercise cultivates the model’s comprehen-\\nsion of the textual data’s structural semantics and facilitates\\nthe alignment of pertinent entities within the structured data.\\nThe overarching optimization goal is to train the language\\nmodel to accurately restore the obscured spans, thereby en-\\nriching its understanding of entity semantics [Yeet al. , 2020 ].\\n6 Augmentation in RAG\\nThis section is structured around three key aspects: the aug-\\nmentation stage, sources of augmentation data, and the aug-\\nmentation process. These facets elucidate the critical tech-\\nnologies pivotal to RAG’s development. A taxonomy of\\nRAG’s core components is presented in Figure 4.\\n6.1 RAG in Augmentation Stages\\nRAG, a knowledge-intensive endeavor, incorporates a vari-\\nety of technical methodologies across the pre-training, fine-\\ntuning, and inference stages of language model training.\\nPre-training Stage\\nDuring the pre-training stage, researchers have investigated\\nmethods to bolster PTMs for open-domain QA through', metadata={'source': '../documents/2312.10997.pdf', 'page': 10}),\n",
       " Document(page_content='Figure 4: Taxonomy of RAG’s core components\\nretrieval-based strategies. The REALM model adopts a struc-\\ntured, interpretable method for knowledge embedding, fram-\\ning pre-training, and fine-tuning as a retrieve-then-predict\\nworkflow within the masked language model (MLM) frame-\\nwork [Arora et al. , 2023 ].\\nRETRO [Borgeaud et al. , 2022 ]leverages retrieval aug-\\nmentation for large-scale pre-training from scratch, achieving\\na reduction in model parameters while surpassing standard\\nGPT models in terms of perplexity. RETRO distinguishes it-\\nself with an additional encoder designed to process features\\nof entities retrieved from an external knowledge base, build-\\ning on the foundational structure of GPT models.\\nAtlas [Izacard et al. , 2022 ]also incorporates a retrieval\\nmechanism into the T5 architecture [Raffel et al. , 2020 ]in\\nboth the pre-training and fine-tuning stages. It uses a pre-\\ntrained T5 to initialize the encoder-decoder language model\\nand a pre-trained Contriever for the dense retriever, improv-\\ning its efficiency for complex language modeling tasks.Furthermore, COG [Lanet al. , 2022 ]introduces a novel\\ntext generation methodology that emulates copying text frag-\\nments from pre-existing collections. Utilizing efficient vector\\nsearch tools, COG computes and indexes contextually mean-\\ningful representations of text fragments, demonstrating supe-\\nrior performance in domains such as question-answering and\\ndomain adaptation when compared to RETRO.\\nThe advent of scaling laws has catalyzed the growth of\\nmodel parameters, propelling autoregressive models into the\\nmainstream. Researchers are expanding the RAG approach to\\npretrained larger models, with RETRO++ exemplifying this\\ntrend by scaling up the model parameters while preserving or\\nenhancing performance [Wang et al. , 2023b ].\\nEmpirical evidence underscores marked improvements in\\ntext generation quality, factual accuracy, reduced toxicity,\\nand downstream task proficiency, especially in knowledge-\\nintensive applications like open-domain QA. These results\\nimply that integrating retrieval mechanisms into the pre-', metadata={'source': '../documents/2312.10997.pdf', 'page': 11}),\n",
       " Document(page_content='training of autoregressive language models constitutes a\\npromising avenue, marrying sophisticated retrieval tech-\\nniques with expansive language models to yield more precise\\nand efficient language generation.\\nThe benefits of augmented pre-training include a robust\\nfoundational model that outperforms standard GPT models\\nin perplexity, text generation quality, and task-specific per-\\nformance, all while utilizing fewer parameters. This method\\nis particularly adept at handling knowledge-intensive tasks\\nand facilitates the development of domain-specific models\\nthrough training on specialized corpora.\\nNonetheless, this approach faces challenges such as the\\nnecessity for extensive pre-training datasets and resources,\\nas well as diminished update frequencies with increasing\\nmodel sizes. Despite these hurdles, the approach offers\\nsignificant advantages in model resilience. Once trained,\\nretrieval-enhanced models can operate independently of ex-\\nternal libraries, enhancing generation speed and operational\\nefficiency. The potential gains identified render this method-\\nology a compelling subject for ongoing investigation and in-\\nnovation in artificial intelligence and machine learning.\\nFine-tuning Stage\\nRAG and Fine-tuning are powerful tools for enhancing\\nLLMs, and combining the two can meet the needs of more\\nspecific scenarios. On one hand, fine-tuning allows for the\\nretrieval of documents with a unique style, achieving bet-\\nter semantic expression and aligning the differences between\\nqueries and documents. This ensures that the output of the\\nretriever is more aptly suited to the scenario at hand. On\\nthe other hand, fine-tuning can fulfill the generation needs of\\nmaking stylized and targeted adjustments. Furthermore, fine-\\ntuning can also be used to align the retriever and generator for\\nimproved model synergy.\\nThe main goal of fine-tuning the retriever is to improve\\nthe quality of semantic representations, achieved by directly\\nfine-tuning the Embedding model using a corpus [Liu, 2023 ].\\nBy aligning the retriever’s capabilities with the prefer-\\nences of the LLMs through feedback signals, both can\\nbe better coordinated [Yuet al. , 2023b, Izacard et al. , 2022,\\nYang et al. , 2023b, Shi et al. , 2023 ]. Fine-tuning the retriever\\nfor specific downstream tasks can lead to improved adapt-\\nability [cite]. The introduction of task-agnostic fine-tuning\\naims to enhance the retriever’s versatility in multi-task sce-\\nnarios [Cheng et al. , 2023a ].\\nFine-tuning generator can result in outputs that are\\nmore stylized and customized. On one hand, it allows\\nfor specialized adaptation to different input data formats.\\nFor example, fine-tuning LLMs to fit the structure of\\nknowledge graphs [Kang et al. , 2023 ], the structure of text\\npairs [Kang et al. , 2023, Cheng et al. , 2023b ], and other spe-\\ncific structures [Liet al. , 2023d ]. On the other hand, by con-\\nstructing directive datasets, one can demand LLMs to gen-\\nerate specific formats content. For instance, in adaptive or\\niterative retrieval scenarios, LLMs are fine-tuned to generate\\ncontent that will help determine the timing for the next step\\nof action [Jiang et al. , 2023b, Asai et al. , 2023 ].\\nBy synergistically fine-tuning both the retriever and the\\ngenerator, we can enhance the model’s generalization capa-bilities and avoid overfitting that may arise from training them\\nseparately. However, joint fine-tuning also leads to increased\\nresource consumption. RA-DIT [Linet al. , 2023 ]presents\\na lightweight, dual-instruction tuning framework that can\\neffectively add retrieval capabilities to any LLMs. The\\nretrieval-enhanced directive fine-tuning updates the LLM,\\nguiding it to make more efficient use of the information re-\\ntrieved and to disregard distracting content.\\nDespite its advantages, fine-tuning has limitations, includ-\\ning the need for specialized datasets for RAG fine-tuning\\nand the requirement for significant computational resources.\\nHowever, this stage allows for customizing models to specific\\nneeds and data formats, potentially reducing resource usage\\ncompared to the pre-training phase while still being able to\\nfine-tune the model’s output style.\\nIn summary, the fine-tuning stage is essential for the adap-\\ntation of RAG models to specific tasks, enabling the refine-\\nment of both retrievers and generators. This stage enhances\\nthe model’s versatility and adaptability to various tasks, de-\\nspite the challenges presented by resource and dataset re-\\nquirements. The strategic fine-tuning of RAG models is\\ntherefore a critical component in the development of efficient\\nand effective retrieval-augmented systems.\\nInference Stage\\nThe inference stage in RAG models is crucial, as it in-\\nvolves extensive integration with LLMs. Traditional RAG\\napproaches, also known as Naive RAG, involve incorporating\\nretrieval content at this stage to guide the generation process.\\nTo overcome the limitations of Naive RAG, advanced tech-\\nniques introduce more contextually rich information dur-\\ning inference. The DSP framework [Khattab et al. , 2022 ]\\nutilizes a sophisticated exchange of natural language text\\nbetween fronzen LMs and retrieval models (RMs), en-\\nriching the context and thereby improving generation out-\\ncomes. The PKG [Luoet al. , 2023 ]method equips LLMs\\nwith a knowledge-guided module that allows for the retrieval\\nof pertinent information without modifying the LMs’ pa-\\nrameters, enabling more complex task execution. CREA-\\nICL [Liet al. , 2023b ]employs a synchronous retrieval of\\ncross-lingual knowledge to enhance context, while RE-\\nCITE [Sunet al. , 2022 ]generates context by sampling para-\\ngraphs directly from LLMs.\\nFurther refinement of the RAG process during infer-\\nence is seen in approaches that cater to tasks necessi-\\ntating multi-step reasoning. ITRG [Feng et al. , 2023 ]it-\\neratively retrieves information to identify the correct rea-\\nsoning paths, thereby improving task adaptability. ITER-\\nRETGEN [Shao et al. , 2023 ]follows an iterative strat-\\negy, merging retrieval and generation in a cyclical pro-\\ncess that alternates between “retrieval-enhanced generation”\\nand “generation-enhanced retrieval”. For non-knowledge-\\nintensive (NKI) tasks, PGRA [Guo et al. , 2023 ]proposes a\\ntwo-stage framework, starting with a task-agnostic retriever\\nfollowed by a prompt-guided reranker to select and priori-\\ntize evidence. In contrast, IRCOT [Trivedi et al. , 2022 ]com-\\nbines RAG with Chain of Thought (CoT) methodologies, al-\\nternating CoT-guided retrievals with retrieval-informed CoT\\nprocesses, significantly boosting GPT-3’s performance across', metadata={'source': '../documents/2312.10997.pdf', 'page': 12}),\n",
       " Document(page_content='various question-answering tasks.\\nIn essence, these inference-stage enhancements provide\\nlightweight, cost-effective alternatives that leverage the ca-\\npabilities of pre-trained models without necessitating further\\ntraining. The principal advantage is maintaining static LLM\\nparameters while supplying contextually relevant information\\nto meet specific task demands. Nevertheless, this approach is\\nnot without limitations, as it requires meticulous data pro-\\ncessing and optimization, and is bound by the foundational\\nmodel’s intrinsic capabilities. To address diverse task require-\\nments effectively, this method is often paired with procedural\\noptimization techniques such as step-wise reasoning, iterative\\nretrieval, and adaptive retrieval strategies.\\n6.2 Augmentation Source\\nThe effectiveness of RAG models is heavily impacted by the\\nselection of data sources for augmentation. Different levels of\\nknowledge and dimensions require distinct processing tech-\\nniques. They are categorized as unstructured data, structured\\ndata, and content generated by LLMs. The technology tree\\nof representative RAG research with different augmentation\\naspects is depicted in Figure 5. The leaves, colored in three\\ndifferent shades, represent enhancements using various types\\nof data: unstructured data, structured data, and content gener-\\nated by LLMs. The diagram clearly shows that initially, aug-\\nmentation was mainly achieved through unstructured data,\\nsuch as pure text. This approach later expanded to include\\nthe use of structured data (e.g. knowledge graph) for further\\nimprovement. More recently, there has been a growing trend\\nin research that utilizes content generated by the LLMs them-\\nselves for retrieval and augmentation purposes.\\nAugmented with Unstructured Data\\nUnstructured text, is gathered from corpora, such as prompt\\ndata for fine-tuning large models [Cheng et al. , 2023a ]and\\ncross-lingual data [Liet al. , 2023b ]. Retrieval units vary from\\ntokens (e.g., kNN-LM [Khandelwal et al. , 2019 ]) to phrases\\n(e.g., NPM, COG [Leeet al. , 2020, Lan et al. , 2022 ]) and\\ndocument paragraphs, with finer granularities offering pre-\\ncision at the cost of increased retrieval complexity.\\nFLARE [Jiang et al. , 2023b ]introduces an active re-\\ntrieval approach, triggered by the LM’s generation of low-\\nprobability words. It creates a temporary sentence for doc-\\nument retrieval, then regenerates the sentence with the re-\\ntrieved context to predict subsequent sentences. RETRO uses\\nthe previous chunk to retrieve the nearest neighbor at the\\nchunk level, combined with the previous chunk’s context, it\\nguides the generation of the next chunk. To preserve causal-\\nity, the generation of the next block Cionly utilizes the near-\\nest neighbor of the previous block N(Ci−1)and not N(Ci).\\nAugmented with Structured Data\\nStructured data, such as knowledge graphs (KGs), pro-\\nvide high-quality context and mitigate model hallucina-\\ntions. RET-LLMs [Modarressi et al. , 2023 ]constructs a\\nknowledge graph memory from past dialogues for future ref-\\nerence. SUGRE [Kang et al. , 2023 ]employs Graph Neu-\\nral Networks (GNNs) to encode relevant KG subgraphs,\\nensuring consistency between retrieved facts and gener-\\nated text through multi-modal contrastive learning. Knowl-edGPT [Wang et al. , 2023d ]generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness and contextuality.\\nLLMs-Generated Content in RAG\\nAddressing the limitations of external auxiliary information\\nin RAG, some research has focused on exploiting LLMs’ in-\\nternal knowledge. SKR [Wang et al. , 2023e ]classifies ques-\\ntions as known or unknown, applying retrieval enhancement\\nselectively. GenRead [Yuet al. , 2022 ]replaces the retriever\\nwith an LLM generator, finding that LLM-generated con-\\ntexts often contain more accurate answers due to better align-\\nment with the pre-training objectives of causal language mod-\\neling. Selfmem [Cheng et al. , 2023b ]iteratively creates an\\nunbounded memory pool with a retrieval-enhanced genera-\\ntor, using a memory selector to choose outputs that serve as\\ndual problems to the original question, thus self-enhancing\\nthe generative model.\\nThese methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model per-\\nformance and task effectiveness.\\n6.3 Augmentation Process\\nIn the domain of RAG, the standard practice often involves\\na singular retrieval step followed by generation, which can\\nlead to inefficiencies. A notable issue, termed the “lost\\nin the middle” phenomenon, arises when a single retrieval\\nyields redundant content that may dilute or contradict es-\\nsential information, thereby degrading the generation qual-\\nity[Liuet al. , 2023a ]. Furthermore, such singular retrieval is\\ntypically insufficient for complex problems demanding multi-\\nstep reasoning, as it provides a limited scope of informa-\\ntion[Yoran et al. , 2023 ].\\nAs illustrated in Figure 5, to circumvent these challenges,\\ncontemporary research has proposed methods for refining the\\nretrieval process: iterative retrieval, recursive retrieval and\\nadaptive retrieval. Iterative retrieval allows the model to en-\\ngage in multiple retrieval cycles, enhancing the depth and\\nrelevance of the information obtained. Recursive retrieval\\nprocess where the results of one retrieval operation are used\\nas the input for the subsequent retrieval. It helps to delve\\ndeeper into relevant information, particularly when dealing\\nwith complex or multi-step queries. Recursive retrieval is of-\\nten used in scenarios where a gradual approach is needed to\\nconverge on a final answer, such as in academic research, le-\\ngal case analysis, or certain types of data mining tasks. Adap-\\ntive retrieval, on the other hand, offers a dynamic adjustment\\nmechanism, tailoring the retrieval process to the specific de-\\nmands of varying tasks and contexts.\\nIterative Retrieval\\nIterative retrieval in RAG models is a process where doc-\\numents are repeatedly collected based on the initial query\\nand the text generated thus far, providing a more compre-\\nhensive knowledge base for LLMs [Borgeaud et al. , 2022,\\nArora et al. , 2023 ]. This approach has been shown to en-\\nhance the robustness of subsequent answer generation by of-\\nfering additional contextual references through multiple re-\\ntrieval iterations. However, it may suffer from semantic dis-\\ncontinuity and the accumulation of irrelevant information, as', metadata={'source': '../documents/2312.10997.pdf', 'page': 13}),\n",
       " Document(page_content='Figure 5: Technology tree of representative RAG research with different augmentation aspects\\nit typically relies on a sequence of n tokens to demarcate the\\nboundaries between generated text and retrieved documents.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized. Recursive re-\\ntrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summa-\\nrizing sections of a document or lengthy PDF before per-\\nforming a retrieval based on this summary. Subsequently, a\\nsecondary retrieval within the document refines the search,\\nembodying the recursive nature of the process. In contrast,\\nmulti-hop retrieval is designed to delve deeper into graph-\\nstructured data sources, extracting interconnected informa-\\ntion[Liet al. , 2023c ].\\nAdditionally, some methodologies integrate the steps of re-\\ntrieval and generation. ITER-RETGEN [Shao et al. , 2023 ]\\nemploys a synergistic approach that leverages “retrieval-\\nenhanced generation” alongside “generation-enhanced re-\\ntrieval” for tasks that necessitate the reproduction of specific\\ninformation. The model harnesses the content required to ad-\\ndress the input task as a contextual basis for retrieving per-\\ntinent knowledge, which in turn facilitates the generation of\\nimproved responses in subsequent iterations.\\nRecursive Retrieval\\nRecursive Retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.The process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [Trivedi et al. , 2022 ]uses chain-of-\\nthought to guide the retrieval process and refines the CoT\\nwith the obtained retrieval results. ToC [Kim et al. , 2023 ]\\ncreates a clarification tree that systematically optimizes the\\nambiguous parts in the Query. It can be particularly useful in\\ncomplex search scenarios where the user’s needs are not en-\\ntirely clear from the outset or where the information sought\\nis highly specialized or nuanced. The recursive nature of the\\nprocess allows for continuous learning and adaptation to the\\nuser’s requirements, often resulting in improved satisfaction\\nwith the search outcomes.\\nAdaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare and Self-\\nRAG [Jiang et al. , 2023b, Asai et al. , 2023 ], refine the RAG\\nframework by enabling LLMs to actively determine the op-\\ntimal moments and content for retrieval, thus enhancing the\\nefficiency and relevance of the information sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as\\nseen in model agents like AutoGPT, Toolformer, and\\nGraph-Toolformer [Yang et al. , 2023c, Schick et al. , 2023,', metadata={'source': '../documents/2312.10997.pdf', 'page': 14}),\n",
       " Document(page_content='Zhang, 2023 ]. Graph-Toolformer, for instance, divides its re-\\ntrieval process into distinct steps where LLMs proactively use\\nretrievers, apply Self-Ask techniques, and employ few-shot\\nprompts to initiate search queries. This proactive stance al-\\nlows LLMs to decide when to search for necessary informa-\\ntion, akin to how an agent utilizes tools.\\nWebGPT [Nakano et al. , 2021 ]integrates a reinforcement\\nlearning framework to train the GPT-3 model in au-\\ntonomously using a search engine during text generation.\\nIt navigates this process using special tokens that facili-\\ntate actions such as search engine queries, browsing results,\\nand citing references, thereby expanding GPT-3’s capabilities\\nthrough the use of external search engines.\\nFlare automates timing retrieval by monitoring the confi-\\ndence of the generation process, as indicated by the probabil-\\nity of generated terms [Jiang et al. , 2023b ]. When the prob-\\nability falls below a certain threshold would activates the re-\\ntrieval system to collect relevant information, thus optimizing\\nthe retrieval cycle.\\nSelf-RAG [Asai et al. , 2023 ]introduces “reflection to-\\nkens” that allow the model to introspect its outputs. These\\ntokens come in two varieties: “retrieve” and “critic”. The\\nmodel autonomously decides when to activate retrieval, or\\nalternatively, a predefined threshold may trigger the pro-\\ncess. During retrieval, the generator conducts a fragment-\\nlevel beam search across multiple paragraphs to derive the\\nmost coherent sequence. Critic scores are used to update the\\nsubdivision scores, with the flexibility to adjust these weights\\nduring inference, tailoring the model’s behavior. Self-RAG’s\\ndesign obviates the need for additional classifiers or reliance\\non Natural Language Inference (NLI) models, thus stream-\\nlining the decision-making process for when to engage re-\\ntrieval mechanisms and improving the model’s autonomous\\njudgment capabilities in generating accurate responses.\\nLLM optimization has received significant attention due to\\nits increasing prevalence. Techniques such as prompt engi-\\nneering, Fine-Tuning (FT), and RAG each have distinct char-\\nacteristics, visually represented in Figure 6. While prompt\\nengineering leverages a model’s inherent capabilities, opti-\\nmizing LLMs often requires the application of both RAG and\\nFT methods. The choice between RAG and FT should be\\nbased on the specific requirements of the scenario and the in-\\nherent properties of each approach. A detailed comparison of\\nRAG and FT is presented in Table 1.\\n6.4 RAG vs Fine-Tuning\\nRAG is like giving a model a textbook for tailored informa-\\ntion retrieval, perfect for specific queries. On the other hand,\\nFT is like a student internalizing knowledge over time, bet-\\nter for replicating specific structures, styles, or formats. FT\\ncan improve model performance and efficiency by reinforc-\\ning base model knowledge, adjusting outputs, and teaching\\ncomplex instructions. However, it is not as good for integrat-\\ning new knowledge or rapidly iterating new use cases.\\nThe two methods, RAG and FT, are not mutually exclusive\\nand can be complementary, augmenting a model’s capabil-\\nities at different levels. In some cases, their combined use\\nmay yield optimal performance. The optimization processinvolving RAG and FT can necessitate multiple iterations to\\nachieve satisfactory results.\\n7 RAG Evaluation\\nThe rapid advancement and growing adoption of RAG in the\\nfield of Natural Language Processing (NLP) have propelled\\nthe evaluation of RAG models to the forefront of research in\\nthe LLMs community. The primary objective of this evalua-\\ntion is to comprehend and optimize the performance of RAG\\nmodels across diverse application scenarios.\\nHistorically, RAG models assessments have centered\\non their execution in specific downstream tasks. These\\nevaluations employ established metrics suitable to the tasks\\nat hand. For instance, question answering evaluations\\nmight rely on EM and F1 scores [Wang et al. , 2023a,\\nShiet al. , 2023, Feng et al. , 2023, Ma et al. , 2023a ], whereas\\nfact-checking tasks often hinge on accuracy as the pri-\\nmary metric [Lewis et al. , 2020, Izacard et al. , 2022,\\nShao et al. , 2023 ]. Tools like RALLE, designed for the auto-\\nmatic evaluation of RAG applications, similarly base their as-\\nsessments on these task-specific metrics [Hoshi et al. , 2023 ].\\nDespite this, there is a notable paucity of research dedicated\\nto evaluating the distinct characteristics of RAG models, with\\nonly a handful of related studies.\\nThe following section shifts the focus from task-specific\\nevaluation methods and metrics to provide a synthesis of the\\nexisting literature based on their unique attributes. This ex-\\nploration covers the objectives of RAG evaluation, the aspects\\nalong which these models are assessed, and the benchmarks\\nand tools available for such evaluations. The aim is to offer a\\ncomprehensive overview of RAG model evaluation, outlining\\nthe methodologies that specifically address the unique aspects\\nof these advanced generative systems.\\n7.1 Evaluation Targets\\nThe assessment of RAG models mainly revolves around two\\nkey components: the retrieval and generation modules. This\\ndivision ensures a thorough evaluation of both the quality of\\ncontext provided and the quality of content produced.\\nRetrieval Quality\\nEvaluating the retrieval quality is crucial for determining the\\neffectiveness of the context sourced by the retriever com-\\nponent. Standard metrics from the domains of search en-\\ngines, recommendation systems, and information retrieval\\nsystems are employed to measure the performance of the\\nRAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [Liu, 2023,\\nNguyen, 2023 ].\\nGeneration Quality\\nThe assessment of generation quality centers on the gener-\\nator’s capacity to synthesize coherent and relevant answers\\nfrom the retrieved context. This evaluation can be catego-\\nrized based on the content’s objectives: unlabeled and la-\\nbeled content. For unlabeled content, the evaluation encom-\\npasses the faithfulness, relevance, and non-harmfulness of the\\ngenerated answers. In contrast, for labeled content, the fo-\\ncus is on the accuracy of the information produced by the', metadata={'source': '../documents/2312.10997.pdf', 'page': 15}),\n",
       " Document(page_content='Table 1: Comparison between RAG and Fine-Tuning\\nFeature Comparison RAG Fine-Tuning\\nKnowledge UpdatesDirectly updating the retrieval knowledge\\nbase ensures that the information remains\\ncurrent without the need for frequent retrain-\\ning, making it well-suited for dynamic data\\nenvironments.Stores static data, requiring retraining for\\nknowledge and data updates.\\nExternal KnowledgeProficient in leveraging external resources,\\nparticularly suitable for accessing documents\\nor other structured/unstructured databases.Can be utilized to align the externally ac-\\nquired knowledge from pretraining with large\\nlanguage models, but may be less practical\\nfor frequently changing data sources.\\nData ProcessingInvolves minimal data processing and han-\\ndling.Depends on the creation of high-quality\\ndatasets, and limited datasets may not result\\nin significant performance improvements.\\nModel CustomizationFocuses on information retrieval and inte-\\ngrating external knowledge but may not fully\\ncustomize model behavior or writing style.Allows adjustments of LLM behavior, writ-\\ning style, or specific domain knowledge\\nbased on specific tones or terms.\\nInterpretabilityResponses can be traced back to specific data\\nsources, providing higher interpretability and\\ntraceability.Similar to a black box, it is not always clear\\nwhy the model reacts a certain way, resulting\\nin relatively lower interpretability.\\nComputational ResourcesDepends on computational resources to sup-\\nport retrieval strategies and technologies re-\\nlated to databases. Additionally, it requires\\nthe maintenance of external data source inte-\\ngration and updates.The preparation and curation of high-quality\\ntraining datasets, defining fine-tuning objec-\\ntives, and providing corresponding computa-\\ntional resources are necessary.\\nLatency RequirementsInvolves data retrieval, which may lead to\\nhigher latency.LLM after fine-tuning can respond without\\nretrieval, resulting in lower latency.\\nReducing HallucinationsInherently less prone to hallucinations as\\neach answer is grounded in retrieved evi-\\ndence.Can help reduce hallucinations by training\\nthe model based on specific domain data but\\nmay still exhibit hallucinations when faced\\nwith unfamiliar input.\\nEthical and Privacy IssuesEthical and privacy concerns arise from the\\nstorage and retrieval of text from external\\ndatabases.Ethical and privacy concerns may arise due\\nto sensitive content in the training data.\\nmodel [Liu, 2023 ]. Additionally, both retrieval and genera-\\ntion quality assessments can be conducted through manual\\nor automatic evaluation methods [Liu, 2023, Lan et al. , 2022,\\nLeng et al. , 2023 ].\\n7.2 Evaluation Aspects\\nContemporary evaluation practices of RAG models empha-\\nsize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\nQuality Scores\\nQuality scores include context relevance, answer faith-\\nfulness, and answer relevance. These quality scoresevaluate the efficiency of the RAG model from differ-\\nent perspectives in the process of information retrieval\\nand generation [Eset al. , 2023, Saad-Falcon et al. , 2023,\\nJarvis and Allard, 2023 ]. The quality scores—context rele-\\nvance, answer faithfulness, and answer relevance—assess the\\nRAG model’s efficiency from various angles throughout the\\ninformation retrieval and generation process [Eset al. , 2023,\\nSaad-Falcon et al. , 2023, Jarvis and Allard, 2023 ].\\nContext Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\nAnswer Faithfulness ensures that the generated answers re-\\nmain true to the retrieved context, maintaining consistency', metadata={'source': '../documents/2312.10997.pdf', 'page': 16}),\n",
       " Document(page_content='Figure 6: RAG compared with other model optimization methods\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively address-\\ning the core inquiry.\\nRequired Abilities\\nRAG evaluation also encompasses four abilities indicative of\\nits adaptability and efficiency: noise robustness, negative re-\\njection, information integration, and counterfactual robust-\\nness [Chen et al. , 2023b, Liu et al. , 2023b ]. These abilities\\nare critical for the model’s performance under various chal-\\nlenges and complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in re-\\nfraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency\\nin synthesizing information from multiple documents to ad-\\ndress complex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating thequality of generation.\\nThe specific metrics for each evaluation aspect are summa-\\nrized in Table 2. It is essential to recognize that these metrics,\\nderived from related work, are traditional measures and do\\nnot yet represent a mature or standardized approach for quan-\\ntifying RAG evaluation aspects. Custom metrics tailored to\\nthe nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\n7.3 Evaluation Benchmarks and Tools\\nThis section delineates the evaluation framework for RAG\\nmodels, comprising benchmark tests and automated eval-\\nuation tools. These instruments furnish quantitative met-\\nrics that not only gauge RAG model performance but also\\nenhance comprehension of the model’s capabilities across\\nvarious evaluation aspects. Prominent benchmarks such as\\nRGB and RECALL [Chen et al. , 2023b, Liu et al. , 2023b ]\\nfocus on appraising the essential abilities of RAG mod-\\nels. Concurrently, state-of-the-art automated tools like RA-\\nGAS [Eset al. , 2023 ], ARES [Saad-Falcon et al. , 2023 ], and\\nTruLens8employ LLMs to adjudicate the quality scores.\\nThese tools and benchmarks collectively form a robust frame-\\nwork for the systematic evaluation of RAG models, as sum-\\nmarized in Table 3.\\n8https://www.trulens.org/trulens eval/core concepts ragtriad/', metadata={'source': '../documents/2312.10997.pdf', 'page': 17}),\n",
       " Document(page_content='Table 2: Summary of metrics applicable for evaluation aspects of RAG\\nContext\\nRelevanceFaithfulnessAnswer\\nRelevanceNoise\\nRobustnessNegative\\nRejectionInformation\\nIntegrationCounterfactual\\nRobustness\\nAccuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nEM ✓\\nRecall ✓\\nPrecision ✓ ✓\\nR-Rate ✓\\nCosine Similarity ✓\\nHit Rate ✓\\nMRR ✓\\nNDCG ✓\\nTable 3: Summary of evaluation frameworks\\nEvaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics\\nRGB† Retrieval Quality\\nGeneration QualityNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual RobustnessAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL†Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\\nRAGAS‡ Retrieval Quality\\nGeneration QualityContext Relevance\\nFaithfulness\\nAnswer Relevance*\\n*\\nCosine Similarity\\nARES‡ Retrieval Quality\\nGeneration QualityContext Relevance\\nFaithfulness\\nAnswer RelevanceAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡ Retrieval Quality\\nGeneration QualityContext Relevance\\nFaithfulness\\nAnswer Relevance*\\n*\\n*\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\n8 Future Prospects\\nThis section explores three future prospects for RAG: future\\nchallenges, modality expansion, and the RAG ecosystem.\\n8.1 Future Challenges of RAG\\nDespite the considerable progress in RAG technology, several\\nchallenges persist that warrant in-depth research:\\nContext Length . RAG’s efficacy is limited by the context\\nwindow size of Large Language Models (LLMs). Balancing\\nthe trade-off between a window that is too short, risking insuf-\\nficient information, and one that is too long, risking informa-\\ntion dilution, is crucial. With ongoing efforts to expand LLM\\ncontext windows to virtually unlimited sizes, the adaptation\\nof RAG to these changes presents a significant research ques-\\ntion[Xuet al. , 2023c, Packer et al. , 2023, Xiao et al. , 2023 ].\\nRobustness . The presence of noise or contradictory infor-\\nmation during retrieval can detrimentally affect RAG’s out-put quality. This situation is figuratively referred to as “Mis-\\ninformation can be worse than no information at all”. Im-\\nproving RAG’s resistance to such adversarial or counterfac-\\ntual inputs is gaining research momentum and has become a\\nkey performance metric [Yuet al. , 2023a, Glass et al. , 2021,\\nBaek et al. , 2023 ].\\nHybrid Approaches (RAG+FT) . Combining RAG with\\nfine-tuning is emerging as a leading strategy. Determining the\\noptimal integration of RAG and fine-tuning whether sequen-\\ntial, alternating, or through end-to-end joint training—and\\nhow to harness both parameterized and non-parameterized\\nadvantages are areas ripe for exploration [Linet al. , 2023 ].\\nExpanding LLM Roles . Beyond generating final answers,\\nLLMs are leveraged for retrieval and evaluation within RAG\\nframeworks. Identifying ways to further unlock LLMs poten-\\ntial in RAG systems is a growing research direction.\\nScaling Laws . While scaling laws [Kaplan et al. , 2020 ]are\\nestablished for LLMs, their applicability to RAG remains un-', metadata={'source': '../documents/2312.10997.pdf', 'page': 18}),\n",
       " Document(page_content='certain. Initial studies [Wang et al. , 2023b ]have begun to ad-\\ndress this, yet the parameter count in RAG models still lags\\nbehind that of LLMs. The possibility of an Inverse Scaling\\nLaw9, where smaller models outperform larger ones, is par-\\nticularly intriguing and merits further investigation.\\nProduction-Ready RAG . RAG’s practicality and alignment\\nwith engineering requirements have facilitated its adoption.\\nHowever, enhancing retrieval efficiency, improving document\\nrecall in large knowledge bases, and ensuring data secu-\\nrity—such as preventing inadvertent disclosure of document\\nsources or metadata by LLMs—are critical engineering chal-\\nlenges that remain to be addressed [Alon et al. , 2022 ].\\nModality Extension of RAG\\nRAG has transcended its initial text-based question-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage . RA-CM3 [Yasunaga et al. , 2022 ]stands as a pio-\\nneering multimodal model of both retrieving and generating\\ntext and images. BLIP-2 [Liet al. , 2023a ]leverages frozen\\nimage encoders alongside LLMs for efficient visual language\\npre-training, enabling zero-shot image-to-text conversions.\\nThe “Visualize Before You Write” method [Zhuet al. , 2022 ]\\nemploys image generation to steer the LM’s text generation,\\nshowing promise in open-ended text generation tasks.\\nAudio and Video . The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [Zhao et al. , 2022 ]. UEOP marks\\na significant advancement in end-to-end automatic speech\\nrecognition by incorporating external, offline strategies for\\nvoice-to-text conversion [Chan et al. , 2023 ]. Additionally,\\nKNN-based attention fusion leverages audio embeddings and\\nsemantically related text embeddings to refine ASR, thereby\\naccelerating domain adaptation. Vid2Seq augments language\\nmodels with specialized temporal markers, facilitating the\\nprediction of event boundaries and textual descriptions within\\na unified output sequence [Yang et al. , 2023a ].\\nCode . RBPS [Nashid et al. , 2023 ]excels in small-scale\\nlearning tasks by retrieving code examples that align with de-\\nvelopers’ objectives through encoding and frequency analy-\\nsis. This approach has demonstrated efficacy in tasks such as\\ntest assertion generation and program repair. For structured\\nknowledge, the CoK method [Liet al. , 2023c ]first extracts\\nfacts pertinent to the input query from a knowledge graph,\\nthen integrates these facts as hints within the input, enhancing\\nperformance in knowledge graph question-answering tasks.\\n8.2 Ecosystem of RAG\\nDownstream Tasks and Evaluation\\nRAG has shown considerable promise in enriching language\\nmodels with the capacity to handle intricate queries and pro-\\nduce detailed responses by leveraging extensive knowledge\\nbases. Empirical evidence suggests that RAG excels in a\\nvariety of downstream tasks, including open-ended question\\nanswering and fact verification. The integration of RAG not\\nonly bolsters the precision and relevance of responses but also\\ntheir diversity and depth.\\n9https://github.com/inverse-scaling/prizeThe scalability and versatility of RAG across multiple do-\\nmains warrant further investigation, particularly in special-\\nized fields such as medicine, law, and education. In these ar-\\neas, RAG could potentially reduce training costs and enhance\\nperformance compared to traditional fine-tuning approaches\\nin professional domain knowledge question answering.\\nConcurrently, refining the evaluation framework for RAG\\nis essential to maximize its efficacy and utility across different\\ntasks. This entails the development of nuanced metrics and\\nassessment tools that can gauge aspects such as contextual\\nrelevance, creativity of content, and non-maleficence.\\nFurthermore, improving the interpretability of RAG-driven\\nmodels continues to be a key goal. Doing so would allow\\nusers to understand the reasoning behind the responses gener-\\nated by the model, thereby promoting trust and transparency\\nin the use of RAG applications.\\nTechnical Stack\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.\\nEmerging technical stacks, while not as feature-rich as\\nLangChain and LLamaIndex, distinguish themselves with\\nspecialized offerings. For instance, Flowise AI10prioritizes a\\nlow-code approach, enabling users to deploy AI applications,\\nincluding RAG, through a user-friendly drag-and-drop inter-\\nface. Other technologies like HayStack, Meltano11, and Co-\\nhere Coral12are also gaining attention for their unique con-\\ntributions to the field.\\nIn addition to AI-focused providers, traditional software\\nand cloud service providers are expanding their offerings to\\ninclude RAG-centric services. Verba13from Weaviate is de-\\nsigned for personal assistant applications, while Amazon’s\\nKendra14provides an intelligent enterprise search service, al-\\nlowing users to navigate through various content repositories\\nusing built-in connectors. During the evolution of the RAG\\ntechnology landscape, there has been a clear divergence to-\\nwards different specializations, such as: 1) Customization.\\nTailoring RAG to meet a specific requirements. 2) Simpli-\\nfication. Making RAG easier to use, thereby reducing the ini-\\ntial learning curve. 3) Specialization. Refining RAG to serve\\nproduction environments more effectively.\\nThe mutual growth of RAG models and their technical\\nstack is evident; technological advancements consistently es-\\ntablish new standards for the existing infrastructure. In turn,\\nenhancements to the technical stack drive the evolution of\\nRAG capabilities. The RAG toolkit is converging into a foun-\\ndational technical stack, laying the groundwork for advanced\\nenterprise applications. However, the concept of a fully in-\\ntegrated, comprehensive platform remains on the horizon,\\npending further innovation and development.\\n10https://flowiseai.com\\n11https://meltano.com\\n12https://cohere.com/coral\\n13https://github.com/weaviate/Verba\\n14https://aws.amazon.com/cn/kendra/', metadata={'source': '../documents/2312.10997.pdf', 'page': 19}),\n",
       " Document(page_content='Figure 7: Summary of RAG ecosystem\\n9 Conclusion\\nThe summary of this paper, as depicted in Figure 7, high-\\nlights RAG’s significant advancement in enhancing the ca-\\npabilities of LLMs through the integration of parameter-\\nized knowledge from language models with extensive non-\\nparameterized data from external knowledge bases. Our sur-\\nvey illustrates the evolution of RAG technologies and their\\nimpact on knowledge-intensive tasks. Our analysis delin-\\neates three developmental paradigms within the RAG frame-\\nwork: Naive, Advanced, and Modular RAG, each marking\\na progressive enhancement over its predecessors. The Ad-\\nvanced RAG paradigm extends beyond the Naive approach\\nby incorporating sophisticated architectural elements, includ-\\ning query rewriting, chunk reranking, and prompt summariza-\\ntion. These innovations have led to a more nuanced and mod-\\nular architecture that enhances both the performance and the\\ninterpretability of LLMs. RAG’s technical integration with\\nother AI methodologies, such as fine-tuning and reinforce-\\nment learning, has further expanded its capabilities. In con-\\ntent retrieval, a hybrid methodology that leverages both struc-\\ntured and unstructured data sources is emerging as a trend,\\nproviding a more enriched retrieval process. Cutting-edge re-\\nsearch within the RAG framework is exploring novel con-\\ncepts such as self-retrieval from LLMs and the dynamic tim-\\ning of information retrieval.\\nDespite the strides made in RAG technology, research op-\\nportunities abound in improving its robustness and its abil-\\nity to manage extended contexts. RAG’s application scope is\\nalso widening into multimodal domains, adapting its princi-ples to interpret and process diverse data forms such as im-\\nages, videos, and code. This expansion underscores RAG’s\\nsignificant practical implications for AI deployment, attract-\\ning interest from both academic and industrial sectors. The\\ngrowing ecosystem of RAG is underscored by an increase in\\nRAG-centric AI applications and the ongoing development\\nof supportive tools. However, as RAG’s application land-\\nscape expands, there is an imperative need to refine evaluation\\nmethodologies to keep pace with its evolution. Ensuring that\\nperformance assessments remain accurate and representative\\nis crucial for capturing the full extent of RAG’s contributions\\nto the AI research and development community.\\nReferences\\n[Alon et al. , 2022 ]Uri Alon, Frank Xu, Junxian He, Sudipta\\nSengupta, Dan Roth, and Graham Neubig. Neuro-\\nsymbolic language modeling with automaton-augmented\\nretrieval. In International Conference on Machine Learn-\\ning, pages 468–485. PMLR, 2022.\\n[Anderson et al. , 2022 ]Nathan Anderson, Caleb Wilson,\\nand Stephen D. Richardson. Lingua: Addressing scenar-\\nios for live interpretation and automatic dubbing. In Jan-\\nice Campbell, Stephen Larocca, Jay Marciano, Konstantin\\nSavenkov, and Alex Yanishevsky, editors, Proceedings of\\nthe 15th Biennial Conference of the Association for Ma-\\nchine Translation in the Americas (Volume 2: Users and\\nProviders Track and Government Track) , pages 202–209,', metadata={'source': '../documents/2312.10997.pdf', 'page': 20}),\n",
       " Document(page_content='Orlando, USA, September 2022. Association for Machine\\nTranslation in the Americas.\\n[Arora et al. , 2023 ]Daman Arora, Anush Kini, Sayak Ray\\nChowdhury, Nagarajan Natarajan, Gaurav Sinha, and\\nAmit Sharma. Gar-meets-rag paradigm for zero-shot infor-\\nmation retrieval. arXiv preprint arXiv:2310.20158 , 2023.\\n[Asai et al. , 2023 ]Akari Asai, Zeqiu Wu, Yizhong Wang,\\nAvirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning\\nto retrieve, generate, and critique through self-reflection.\\narXiv preprint arXiv:2310.11511 , 2023.\\n[BAAI, 2023 ]BAAI. Flagembedding. https://github.com/\\nFlagOpen/FlagEmbedding, 2023.\\n[Baek et al. , 2023 ]Jinheon Baek, Soyeong Jeong, Minki\\nKang, Jong C Park, and Sung Ju Hwang. Knowledge-\\naugmented language model verification. arXiv preprint\\narXiv:2310.12836 , 2023.\\n[Berchansky et al. , 2023 ]Moshe Berchansky, Peter Izsak,\\nAvi Caciularu, Ido Dagan, and Moshe Wasserblat. Opti-\\nmizing retrieval-augmented reader models via token elim-\\nination. arXiv preprint arXiv:2310.13682 , 2023.\\n[Blagojevi, 2023 ]Vladimir Blagojevi. Enhancing rag\\npipelines in haystack: Introducing diversityranker and\\nlostinthemiddleranker. https://towardsdatascience.com/\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5,\\n2023.\\n[Borgeaud et al. , 2022 ]Sebastian Borgeaud, Arthur Men-\\nsch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\\nMillican, George Bm Van Den Driessche, Jean-Baptiste\\nLespiau, Bogdan Damoc, Aidan Clark, et al. Improving\\nlanguage models by retrieving from trillions of tokens.\\nInInternational conference on machine learning , pages\\n2206–2240. PMLR, 2022.\\n[Brown et al. , 2020 ]Tom Brown, Benjamin Mann, Nick Ry-\\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\\nAmanda Askell, et al. Language models are few-shot\\nlearners. Advances in neural information processing sys-\\ntems, 33:1877–1901, 2020.\\n[Caiet al. , 2021 ]Deng Cai, Yan Wang, Huayang Li, Wai\\nLam, and Lemao Liu. Neural machine translation\\nwith monolingual translation memory. arXiv preprint\\narXiv:2105.11269 , 2021.\\n[Chan et al. , 2023 ]David M Chan, Shalini Ghosh, Ariya\\nRastrow, and Bj ¨orn Hoffmeister. Using external off-\\npolicy speech-to-text mappings in contextual end-to-\\nend automated speech recognition. arXiv preprint\\narXiv:2301.02736 , 2023.\\n[Chen et al. , 2023a ]Howard Chen, Ramakanth Pasunuru,\\nJason Weston, and Asli Celikyilmaz. Walking down the\\nmemory maze: Beyond context limit through interactive\\nreading. arXiv preprint arXiv:2310.05029 , 2023.\\n[Chen et al. , 2023b ]Jiawei Chen, Hongyu Lin, Xianpei\\nHan, and Le Sun. Benchmarking large language mod-\\nels in retrieval-augmented generation. arXiv preprint\\narXiv:2309.01431 , 2023.[Cheng et al. , 2022 ]Xin Cheng, Shen Gao, Lemao Liu,\\nDongyan Zhao, and Rui Yan. Neural machine transla-\\ntion with contrastive translation memories. arXiv preprint\\narXiv:2212.03140 , 2022.\\n[Cheng et al. , 2023a ]Daixuan Cheng, Shaohan Huang,\\nJunyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao\\nSun, Furu Wei, Denvy Deng, and Qi Zhang. Uprise: Uni-\\nversal prompt retrieval for improving zero-shot evaluation.\\narXiv preprint arXiv:2303.08518 , 2023.\\n[Cheng et al. , 2023b ]Xin Cheng, Di Luo, Xiuying Chen,\\nLemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself\\nup: Retrieval-augmented text generation with self mem-\\nory. arXiv preprint arXiv:2305.02437 , 2023.\\n[Cohere, 2023 ]Cohere. Say goodbye to irrelevant search\\nresults: Cohere rerank is here. https://txt.cohere.com/\\nrerank/, 2023.\\n[Daiet al. , 2022 ]Zhuyun Dai, Vincent Y Zhao, Ji Ma,\\nYi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin\\nGuu, Keith B Hall, and Ming-Wei Chang. Promptagator:\\nFew-shot dense retrieval from 8 examples. arXiv preprint\\narXiv:2209.11755 , 2022.\\n[Eset al. , 2023 ]Shahul Es, Jithin James, Luis Espinosa-\\nAnke, and Steven Schockaert. Ragas: Automated eval-\\nuation of retrieval augmented generation. arXiv preprint\\narXiv:2309.15217 , 2023.\\n[Feng et al. , 2023 ]Zhangyin Feng, Xiaocheng Feng, Dezhi\\nZhao, Maojin Yang, and Bing Qin. Retrieval-generation\\nsynergy augmented large language models. arXiv preprint\\narXiv:2310.05149 , 2023.\\n[Gao et al. , 2022 ]Luyu Gao, Xueguang Ma, Jimmy Lin, and\\nJamie Callan. Precise zero-shot dense retrieval without\\nrelevance labels. arXiv preprint arXiv:2212.10496 , 2022.\\n[Glass et al. , 2021 ]Michael Glass, Gaetano Rossiello,\\nMd Faisal Mahbub Chowdhury, and Alfio Gliozzo.\\nRobust retrieval augmented generation for zero-shot slot\\nfilling. arXiv preprint arXiv:2108.13934 , 2021.\\n[Google, 2023 ]Google. Gemini: A family of highly capable\\nmultimodal models. https://goo.gle/GeminiPaper, 2023.\\n[Guo et al. , 2023 ]Zhicheng Guo, Sijie Cheng, Yile Wang,\\nPeng Li, and Yang Liu. Prompt-guided retrieval augmen-\\ntation for non-knowledge-intensive tasks. arXiv preprint\\narXiv:2305.17653 , 2023.\\n[Hendrycks et al. , 2020 ]Dan Hendrycks, Collin Burns,\\nSteven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\\nand Jacob Steinhardt. Measuring massive multitask lan-\\nguage understanding. arXiv preprint arXiv:2009.03300 ,\\n2020.\\n[Hoshi et al. , 2023 ]Yasuto Hoshi, Daisuke Miyashita,\\nYouyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu\\nTorii, and Jun Deguchi. Ralle: A framework for devel-\\noping and evaluating retrieval-augmented large language\\nmodels. arXiv preprint arXiv:2308.10633 , 2023.\\n[Huang et al. , 2023 ]Jie Huang, Wei Ping, Peng Xu, Mo-\\nhammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan', metadata={'source': '../documents/2312.10997.pdf', 'page': 21}),\n",
       " Document(page_content='Catanzaro. Raven: In-context learning with retrieval aug-\\nmented encoder-decoder language models. arXiv preprint\\narXiv:2308.07922 , 2023.\\n[ILIN, 2023 ]IV AN ILIN. Advanced rag techniques:\\nan illustrated overview. https://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n[Izacard et al. , 2022 ]Gautier Izacard, Patrick Lewis, Maria\\nLomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,\\nand Edouard Grave. Few-shot learning with re-\\ntrieval augmented language models. arXiv preprint\\narXiv:2208.03299 , 2022.\\n[Jarvis and Allard, 2023 ]Colin Jarvis and John Al-\\nlard. A survey of techniques for maximizing\\nllm performance. https://community.openai.com/\\nt/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2,\\n2023.\\n[Jiang et al. , 2023a ]Huiqiang Jiang, Qianhui Wu, Chin-Yew\\nLin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing\\nprompts for accelerated inference of large language mod-\\nels.arXiv preprint arXiv:2310.05736 , 2023.\\n[Jiang et al. , 2023b ]Zhengbao Jiang, Frank F Xu, Luyu\\nGao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming\\nYang, Jamie Callan, and Graham Neubig. Active retrieval\\naugmented generation. arXiv preprint arXiv:2305.06983 ,\\n2023.\\n[Kandpal et al. , 2023 ]Nikhil Kandpal, Haikang Deng,\\nAdam Roberts, Eric Wallace, and Colin Raffel. Large\\nlanguage models struggle to learn long-tail knowledge.\\nInInternational Conference on Machine Learning , pages\\n15696–15707. PMLR, 2023.\\n[Kang et al. , 2023 ]Minki Kang, Jin Myung Kwak, Jinheon\\nBaek, and Sung Ju Hwang. Knowledge graph-augmented\\nlanguage models for knowledge-grounded dialogue gener-\\nation. arXiv preprint arXiv:2305.18846 , 2023.\\n[Kaplan et al. , 2020 ]Jared Kaplan, Sam McCandlish, Tom\\nHenighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\\nScaling laws for neural language models. arXiv preprint\\narXiv:2001.08361 , 2020.\\n[Karpukhin et al. , 2020 ]Vladimir Karpukhin, Barlas O ˘guz,\\nSewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval\\nfor open-domain question answering. arXiv preprint\\narXiv:2004.04906 , 2020.\\n[Khandelwal et al. , 2019 ]Urvashi Khandelwal, Omer Levy,\\nDan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gen-\\neralization through memorization: Nearest neighbor lan-\\nguage models. arXiv preprint arXiv:1911.00172 , 2019.\\n[Khattab et al. , 2022 ]Omar Khattab, Keshav Santhanam,\\nXiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\\nand Matei Zaharia. Demonstrate-search-predict: Compos-\\ning retrieval and language models for knowledge-intensive\\nnlp. arXiv preprint arXiv:2212.14024 , 2022.[Kim et al. , 2023 ]Gangwoo Kim, Sungdong Kim, Byeong-\\nguk Jeon, Joonsuk Park, and Jaewoo Kang. Tree\\nof clarifications: Answering ambiguous questions with\\nretrieval-augmented large language models. arXiv preprint\\narXiv:2310.14696 , 2023.\\n[Lanet al. , 2022 ]Tian Lan, Deng Cai, Yan Wang, Heyan\\nHuang, and Xian-Ling Mao. Copy is all you need. In\\nThe Eleventh International Conference on Learning Rep-\\nresentations , 2022.\\n[Leeet al. , 2020 ]Jinhyuk Lee, Mujeen Sung, Jaewoo Kang,\\nand Danqi Chen. Learning dense representations of\\nphrases at scale. arXiv preprint arXiv:2012.12624 , 2020.\\n[Leng et al. , 2023 ]Quinn Leng, Kasey Uhlenhuth, and\\nAlkis Polyzotis. Best practices for llm evaluation\\nof rag applications. https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[Lewis et al. , 2020 ]Patrick Lewis, Ethan Perez, Aleksan-\\ndra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\\nGoyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt ¨aschel, et al. Retrieval-augmented generation for\\nknowledge-intensive nlp tasks. Advances in Neural Infor-\\nmation Processing Systems , 33:9459–9474, 2020.\\n[Li and Li, 2023 ]Xianming Li and Jing Li. Angle-optimized\\ntext embeddings. arXiv preprint arXiv:2309.12871 , 2023.\\n[Liet al. , 2023a ]Junnan Li, Dongxu Li, Silvio Savarese, and\\nSteven Hoi. Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large language\\nmodels. arXiv preprint arXiv:2301.12597 , 2023.\\n[Liet al. , 2023b ]Xiaoqian Li, Ercong Nie, and Sheng\\nLiang. From classification to generation: Insights into\\ncrosslingual retrieval augmented icl. arXiv preprint\\narXiv:2311.06595 , 2023.\\n[Liet al. , 2023c ]Xingxuan Li, Ruochen Zhao, Yew Ken\\nChia, Bosheng Ding, Lidong Bing, Shafiq Joty, and Sou-\\njanya Poria. Chain of knowledge: A framework for\\ngrounding large language models with structured knowl-\\nedge bases. arXiv preprint arXiv:2305.13269 , 2023.\\n[Liet al. , 2023d ]Xinze Li, Zhenghao Liu, Chenyan Xiong,\\nShi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware\\nlanguage model pretraining improves dense retrieval on\\nstructured data. arXiv preprint arXiv:2305.19912 , 2023.\\n[Liang et al. , 2023 ]Han Liang, Wenqian Zhang, Wenxuan\\nLi, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based\\nmulti-human motion generation under complex interac-\\ntions. arXiv preprint arXiv:2304.05684 , 2023.\\n[Linet al. , 2023 ]Xi Victoria Lin, Xilun Chen, Mingda\\nChen, Weijia Shi, Maria Lomeli, Rich James, Pedro Ro-\\ndriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al.\\nRa-dit: Retrieval-augmented dual instruction tuning. arXiv\\npreprint arXiv:2310.01352 , 2023.\\n[Litman et al. , 2020 ]Ron Litman, Oron Anschel, Shahar\\nTsiper, Roee Litman, Shai Mazor, and R Manmatha. Scat-\\nter: selective context attentional scene text recognizer. In\\nproceedings of the IEEE/CVF conference on computer vi-\\nsion and pattern recognition , pages 11962–11972, 2020.', metadata={'source': '../documents/2312.10997.pdf', 'page': 22}),\n",
       " Document(page_content='[Liuet al. , 2023a ]Nelson F Liu, Kevin Lin, John Hewitt,\\nAshwin Paranjape, Michele Bevilacqua, Fabio Petroni,\\nand Percy Liang. Lost in the middle: How language mod-\\nels use long contexts. arXiv preprint arXiv:2307.03172 ,\\n2023.\\n[Liuet al. , 2023b ]Yi Liu, Lianzhe Huang, Shicheng Li,\\nSishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and\\nXu Sun. Recall: A benchmark for llms robustness\\nagainst external counterfactual knowledge. arXiv preprint\\narXiv:2311.08147 , 2023.\\n[Liu, 2023 ]Jerry Liu. Building production-ready rag\\napplications. https://www.ai.engineer/summit/schedule/\\nbuilding-production-ready-rag-applications, 2023.\\n[Luoet al. , 2023 ]Ziyang Luo, Can Xu, Pu Zhao, Xiubo\\nGeng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin\\nJiang. Augmented large language models with paramet-\\nric knowledge guiding. arXiv preprint arXiv:2305.04757 ,\\n2023.\\n[Maet al. , 2023a ]Xinbei Ma, Yeyun Gong, Pengcheng\\nHe, Hai Zhao, and Nan Duan. Query rewriting for\\nretrieval-augmented large language models. arXiv preprint\\narXiv:2305.14283 , 2023.\\n[Maet al. , 2023b ]Yubo Ma, Yixin Cao, YongChing Hong,\\nand Aixin Sun. Large language model is not a good few-\\nshot information extractor, but a good reranker for hard\\nsamples! ArXiv , abs/2303.08559, 2023.\\n[Modarressi et al. , 2023 ]Ali Modarressi, Ayyoob Imani,\\nMohsen Fayyaz, and Hinrich Sch ¨utze. Ret-llm: Towards\\na general read-write memory for large language models.\\narXiv preprint arXiv:2305.14322 , 2023.\\n[Nakano et al. , 2021 ]Reiichiro Nakano, Jacob Hilton,\\nSuchir Balaji, Jeff Wu, Long Ouyang, Christina Kim,\\nChristopher Hesse, Shantanu Jain, Vineet Kosaraju,\\nWilliam Saunders, et al. Webgpt: Browser-assisted\\nquestion-answering with human feedback. arXiv preprint\\narXiv:2112.09332 , 2021.\\n[Nashid et al. , 2023 ]Noor Nashid, Mifta Sintaha, and Ali\\nMesbah. Retrieval-based prompt selection for code-related\\nfew-shot learning. In 2023 IEEE/ACM 45th International\\nConference on Software Engineering (ICSE) , pages 2450–\\n2462, 2023.\\n[Nguyen, 2023 ]Isabelle Nguyen. Evaluating rag part i: How\\nto evaluate document retrieval. https://www.deepset.ai/\\nblog/rag-evaluation-retrieval, 2023.\\n[Nishikawa et al. , 2022 ]Sosuke Nishikawa, Ryokan Ri,\\nIkuya Yamada, Yoshimasa Tsuruoka, and Isao Echizen.\\nEase: Entity-aware contrastive learning of sentence em-\\nbedding. arXiv preprint arXiv:2205.04260 , 2022.\\n[OpenAI, 2023 ]OpenAI. Gpt-4 technical report. https://cdn.\\nopenai.com/papers/gpt-4.pdf, 2023.\\n[Packer et al. , 2023 ]Charles Packer, Vivian Fang, Shishir G\\nPatil, Kevin Lin, Sarah Wooders, and Joseph E Gonza-\\nlez. Memgpt: Towards llms as operating systems. arXiv\\npreprint arXiv:2310.08560 , 2023.[Raffel et al. , 2020 ]Colin Raffel, Noam Shazeer, Adam\\nRoberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits\\nof transfer learning with a unified text-to-text transformer.\\nThe Journal of Machine Learning Research , 21(1):5485–\\n5551, 2020.\\n[Ram et al. , 2023 ]Ori Ram, Yoav Levine, Itay Dalmedigos,\\nDor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\\nYoav Shoham. In-context retrieval-augmented language\\nmodels. arXiv preprint arXiv:2302.00083 , 2023.\\n[Raudaschl, 2023 ]Adrian H. Raudaschl. Forget rag, the\\nfuture is rag-fusion. https://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[Saad-Falcon et al. , 2023 ]Jon Saad-Falcon, Omar Khattab,\\nChristopher Potts, and Matei Zaharia. Ares: An automated\\nevaluation framework for retrieval-augmented generation\\nsystems. arXiv preprint arXiv:2311.09476 , 2023.\\n[Schick et al. , 2023 ]Timo Schick, Jane Dwivedi-Yu,\\nRoberto Dess `ı, Roberta Raileanu, Maria Lomeli, Luke\\nZettlemoyer, Nicola Cancedda, and Thomas Scialom.\\nToolformer: Language models can teach themselves to\\nuse tools. arXiv preprint arXiv:2302.04761 , 2023.\\n[Sciavolino et al. , 2021 ]Christopher Sciavolino, Zexuan\\nZhong, Jinhyuk Lee, and Danqi Chen. Simple entity-\\ncentric questions challenge dense retrievers. arXiv\\npreprint arXiv:2109.08535 , 2021.\\n[Shao et al. , 2023 ]Zhihong Shao, Yeyun Gong, Yelong\\nShen, Minlie Huang, Nan Duan, and Weizhu Chen. En-\\nhancing retrieval-augmented large language models with\\niterative retrieval-generation synergy. arXiv preprint\\narXiv:2305.15294 , 2023.\\n[Shiet al. , 2023 ]Weijia Shi, Sewon Min, Michihiro Ya-\\nsunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\\nZettlemoyer, and Wen-tau Yih. Replug: Retrieval-\\naugmented black-box language models. arXiv preprint\\narXiv:2301.12652 , 2023.\\n[Srivastava et al. , 2022 ]Aarohi Srivastava, Abhinav Ras-\\ntogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya\\nGupta, Adri `a Garriga-Alonso, et al. Beyond the imitation\\ngame: Quantifying and extrapolating the capabilities of\\nlanguage models. arXiv preprint arXiv:2206.04615 , 2022.\\n[Sunet al. , 2022 ]Zhiqing Sun, Xuezhi Wang, Yi Tay, Yim-\\ning Yang, and Denny Zhou. Recitation-augmented lan-\\nguage models. arXiv preprint arXiv:2210.01296 , 2022.\\n[Touvron et al. , 2023 ]Hugo Touvron, Louis Martin, Kevin\\nStone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\\nShruti Bhosale, et al. Llama 2: Open foundation and\\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 ,\\n2023.\\n[Trivedi et al. , 2022 ]Harsh Trivedi, Niranjan Balasubrama-\\nnian, Tushar Khot, and Ashish Sabharwal. Inter-\\nleaving retrieval with chain-of-thought reasoning for\\nknowledge-intensive multi-step questions. arXiv preprint\\narXiv:2212.10509 , 2022.', metadata={'source': '../documents/2312.10997.pdf', 'page': 23}),\n",
       " Document(page_content='[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki\\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. Advances in neural information processing systems ,\\n30, 2017.\\n[V oyageAI, 2023 ]V oyageAI. V oyage’s embedding models.\\nhttps://docs.voyageai.com/embeddings/, 2023.\\n[Wang et al. , 2019 ]Alex Wang, Yada Pruksachatkun, Nikita\\nNangia, Amanpreet Singh, Julian Michael, Felix Hill,\\nOmer Levy, and Samuel Bowman. Superglue: A stick-\\nier benchmark for general-purpose language understand-\\ning systems. Advances in neural information processing\\nsystems , 32, 2019.\\n[Wang et al. , 2022a ]Shuohang Wang, Yichong Xu, Yuwei\\nFang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu,\\nand Michael Zeng. Training data is more valuable than you\\nthink: A simple and effective method by retrieving from\\ntraining data. arXiv preprint arXiv:2203.08773 , 2022.\\n[Wang et al. , 2022b ]Shuohang Wang, Yichong Xu, Yuwei\\nFang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu,\\nand Michael Zeng. Training data is more valuable than\\nyou think: A simple and effective method by retriev-\\ning from training data. In Smaranda Muresan, Preslav\\nNakov, and Aline Villavicencio, editors, Proceedings of\\nthe 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers) , pages 3170–\\n3179, Dublin, Ireland, May 2022. Association for Compu-\\ntational Linguistics.\\n[Wang et al. , 2023a ]Boxin Wang, Wei Ping, Lawrence\\nMcAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan\\nCatanzaro. Instructretro: Instruction tuning post retrieval-\\naugmented pretraining. arXiv preprint arXiv:2310.07713 ,\\n2023.\\n[Wang et al. , 2023b ]Boxin Wang, Wei Ping, Peng Xu,\\nLawrence McAfee, Zihan Liu, Mohammad Shoeybi,\\nYi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao,\\net al. Shall we pretrain autoregressive language models\\nwith retrieval? a comprehensive study. arXiv preprint\\narXiv:2304.06762 , 2023.\\n[Wang et al. , 2023c ]Liang Wang, Nan Yang, and Furu Wei.\\nQuery2doc: Query expansion with large language models.\\narXiv preprint arXiv:2303.07678 , 2023.\\n[Wang et al. , 2023d ]Xintao Wang, Qianwen Yang, Yongting\\nQiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua\\nXiao, and Wei Wang. Knowledgpt: Enhancing large lan-\\nguage models with retrieval and storage access on knowl-\\nedge bases. arXiv preprint arXiv:2308.11761 , 2023.\\n[Wang et al. , 2023e ]Yile Wang, Peng Li, Maosong Sun,\\nand Yang Liu. Self-knowledge guided retrieval aug-\\nmentation for large language models. arXiv preprint\\narXiv:2310.05002 , 2023.\\n[Xiaet al. , 2019 ]Mengzhou Xia, Guoping Huang, Lemao\\nLiu, and Shuming Shi. Graph based translation mem-\\nory for neural machine translation. In Proceedings of\\nthe AAAI conference on artificial intelligence , volume 33,\\npages 7297–7304, 2019.[Xiao et al. , 2023 ]Guangxuan Xiao, Yuandong Tian, Beidi\\nChen, Song Han, and Mike Lewis. Efficient stream-\\ning language models with attention sinks. arXiv preprint\\narXiv:2309.17453 , 2023.\\n[Xuet al. , 2023a ]Fangyuan Xu, Weijia Shi, and Eunsol\\nChoi. Recomp: Improving retrieval-augmented lms with\\ncompression and selective augmentation. arXiv preprint\\narXiv:2310.04408 , 2023.\\n[Xuet al. , 2023b ]Peng Xu, Wei Ping, Xianchao Wu,\\nLawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Sub-\\nramanian, Evelina Bakhturina, Mohammad Shoeybi, and\\nBryan Catanzaro. Retrieval meets long context large lan-\\nguage models. arXiv preprint arXiv:2310.03025 , 2023.\\n[Xuet al. , 2023c ]Peng Xu, Wei Ping, Xianchao Wu,\\nLawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Sub-\\nramanian, Evelina Bakhturina, Mohammad Shoeybi, and\\nBryan Catanzaro. Retrieval meets long context large lan-\\nguage models. arXiv preprint arXiv:2310.03025 , 2023.\\n[Yang et al. , 2023a ]Antoine Yang, Arsha Nagrani,\\nPaul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset,\\nIvan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq:\\nLarge-scale pretraining of a visual language model for\\ndense video captioning. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 10714–10726, 2023.\\n[Yang et al. , 2023b ]Haoyan Yang, Zhitao Li, Yong Zhang,\\nJianzong Wang, Ning Cheng, Ming Li, and Jing Xiao.\\nPrca: Fitting black-box large language models for retrieval\\nquestion answering via pluggable reward-driven contex-\\ntual adapter. arXiv preprint arXiv:2310.18347 , 2023.\\n[Yang et al. , 2023c ]Hui Yang, Sifu Yue, and Yunzhong He.\\nAuto-gpt for online decision making: Benchmarks and ad-\\nditional opinions. arXiv preprint arXiv:2306.02224 , 2023.\\n[Yasunaga et al. , 2022 ]Michihiro Yasunaga, Armen Agha-\\njanyan, Weijia Shi, Rich James, Jure Leskovec, Percy\\nLiang, Mike Lewis, Luke Zettlemoyer, and Wen-tau\\nYih. Retrieval-augmented multimodal language modeling.\\narXiv preprint arXiv:2211.12561 , 2022.\\n[Yeet al. , 2020 ]Deming Ye, Yankai Lin, Jiaju Du, Zheng-\\nhao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. Coref-\\nerential reasoning learning for language representation.\\narXiv preprint arXiv:2004.06870 , 2020.\\n[Yoran et al. , 2023 ]Ori Yoran, Tomer Wolfson, Ori Ram,\\nand Jonathan Berant. Making retrieval-augmented lan-\\nguage models robust to irrelevant context. arXiv preprint\\narXiv:2310.01558 , 2023.\\n[Yuet al. , 2022 ]Wenhao Yu, Dan Iter, Shuohang Wang, Yi-\\nchong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,\\nMichael Zeng, and Meng Jiang. Generate rather than re-\\ntrieve: Large language models are strong context genera-\\ntors. arXiv preprint arXiv:2209.10063 , 2022.\\n[Yuet al. , 2023a ]Wenhao Yu, Hongming Zhang, Xiaoman\\nPan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-\\nof-note: Enhancing robustness in retrieval-augmented lan-\\nguage models. arXiv preprint arXiv:2311.09210 , 2023.', metadata={'source': '../documents/2312.10997.pdf', 'page': 24}),\n",
       " Document(page_content='[Yuet al. , 2023b ]Zichun Yu, Chenyan Xiong, Shi Yu, and\\nZhiyuan Liu. Augmentation-adapted retriever improves\\ngeneralization of language models as generic plug-in.\\narXiv preprint arXiv:2305.17331 , 2023.\\n[Zhang et al. , 2019 ]Zhengyan Zhang, Xu Han, Zhiyuan Liu,\\nXin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced\\nlanguage representation with informative entities. arXiv\\npreprint arXiv:1905.07129 , 2019.\\n[Zhang et al. , 2023a ]Peitian Zhang, Shitao Xiao, Zheng\\nLiu, Zhicheng Dou, and Jian-Yun Nie. Retrieve any-\\nthing to augment large language models. arXiv preprint\\narXiv:2310.07554 , 2023.\\n[Zhang et al. , 2023b ]Yue Zhang, Yafu Li, Leyang Cui, Deng\\nCai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao,\\nYu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:\\nA survey on hallucination in large language models. arXiv\\npreprint arXiv:2309.01219 , 2023.\\n[Zhang, 2023 ]Jiawei Zhang. Graph-toolformer: To em-\\npower llms with graph reasoning ability via prompt aug-\\nmented by chatgpt. arXiv preprint arXiv:2304.11116 ,\\n2023.\\n[Zhao et al. , 2022 ]Jinming Zhao, Gholamreza Haffar, and\\nEhsan Shareghi. Generating synthetic speech from\\nspokenvocab for speech translation. arXiv preprint\\narXiv:2210.08174 , 2022.\\n[Zheng et al. , 2023 ]Huaixiu Steven Zheng, Swaroop\\nMishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi,\\nQuoc V Le, and Denny Zhou. Take a step back: Evoking\\nreasoning via abstraction in large language models. arXiv\\npreprint arXiv:2310.06117 , 2023.\\n[Zhuet al. , 2022 ]Wanrong Zhu, An Yan, Yujie Lu, Wenda\\nXu, Xin Eric Wang, Miguel Eckstein, and William Yang\\nWang. Visualize before you write: Imagination-\\nguided open-ended text generation. arXiv preprint\\narXiv:2210.03765 , 2022.\\n[Zhuang et al. , 2023 ]Shengyao Zhuang, Bing Liu, Bevan\\nKoopman, and Guido Zuccon. Open-source large\\nlanguage models are strong zero-shot query likeli-\\nhood models for document ranking. arXiv preprint\\narXiv:2310.13243 , 2023.', metadata={'source': '../documents/2312.10997.pdf', 'page': 25})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retriever(doc):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "    all_splits = text_splitter.split_documents(doc)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = Chroma.from_documents(all_splits, embeddings)\n",
    "    retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    return retriever\n",
    "retriever_pre = make_retriever(preprocessed_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='dispelling ambiguity in entities and terms confirming factual accuracy maintaining context and updating outdated documents optimizing index structures involves adjusting the size of chunks to capture relevant context querying across multiple index paths and incorporating information from the graph structure to capture relevant context by leveraging relation- ships between nodes in a graph data index adding metadata information involves integrating refer- enced metadata such as dates and purposes into chunks for filtering purposes and incorporating metadata like chapters and subsections of references to improve retrieval efficiency alignment optimization addresses alignment issues and disparities between documents by introducing hypothetical questions liet al 2023d into documents to rectify align- ment issues and differences retrieval during the retrieval stage the primary focus is on identifying the appropriate context by calculating the similarity between the query and chunks the', metadata={'page': 4, 'source': '../documents/2312.10997.pdf', 'start_index': 805}),\n",
       " Document(page_content='event ie the abrupt dismissal and reinstatement of ope- nais ceo which generated considerable public discourse chatgpt as the most renowned and widely utilized llm constrained by its pretraining data lacks knowledge of re- cent events rag addresses this gap by retrieving up-to-date document excerpts from external knowledge bases in this in- stance it procures a selection of news articles pertinent to the inquiry these articles alongside the initial question are then amalgamated into an enriched prompt that enables chatgpt to synthesize an informed response this example illustrates the rag process demonstrating its capability to enhance the models responses with real-time information retrieval technologically rag has been enriched through various innovative approaches addressing pivotal questions such as what to retrieve when to retrieve and how to use the retrieved information for what to retrieve research has progressed from simple token khandelwal et al 2019 and entity retrieval', metadata={'page': 2, 'source': '../documents/2312.10997.pdf', 'start_index': 806}),\n",
       " Document(page_content='that utilizes content generated by the llms them- selves for retrieval and augmentation purposes augmented with unstructured data unstructured text is gathered from corpora such as prompt data for fine-tuning large models cheng et al 2023a and cross-lingual data liet al 2023b retrieval units vary from tokens eg knn-lm khandelwal et al 2019 to phrases eg npm cog leeet al 2020 lan et al 2022 and document paragraphs with finer granularities offering pre- cision at the cost of increased retrieval complexity flare jiang et al 2023b introduces an active re- trieval approach triggered by the lms generation of low- probability words it creates a temporary sentence for doc- ument retrieval then regenerates the sentence with the re- trieved context to predict subsequent sentences retro uses the previous chunk to retrieve the nearest neighbor at the chunk level combined with the previous chunks context it guides the generation of the next chunk to preserve causal- ity the generation of the next', metadata={'page': 13, 'source': '../documents/2312.10997.pdf', 'start_index': 1600})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_pre.invoke(\"What is the main topic of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='dispelling ambiguity in entities and terms confirming factual accuracy maintaining context and updating outdated documents optimizing index structures involves adjusting the size of chunks to capture relevant context querying across multiple index paths and incorporating information from the graph structure to capture relevant context by leveraging relation- ships between nodes in a graph data index adding metadata information involves integrating refer- enced metadata such as dates and purposes into chunks for filtering purposes and incorporating metadata like chapters and subsections of references to improve retrieval efficiency alignment optimization addresses alignment issues and disparities between documents by introducing hypothetical questions liet al 2023d into documents to rectify align- ment issues and differences retrieval during the retrieval stage the primary focus is on identifying the appropriate context by calculating the similarity between the query and chunks the', metadata={'page': 4, 'source': '../documents/2312.10997.pdf', 'start_index': 805}),\n",
       " Document(page_content='event ie the abrupt dismissal and reinstatement of ope- nais ceo which generated considerable public discourse chatgpt as the most renowned and widely utilized llm constrained by its pretraining data lacks knowledge of re- cent events rag addresses this gap by retrieving up-to-date document excerpts from external knowledge bases in this in- stance it procures a selection of news articles pertinent to the inquiry these articles alongside the initial question are then amalgamated into an enriched prompt that enables chatgpt to synthesize an informed response this example illustrates the rag process demonstrating its capability to enhance the models responses with real-time information retrieval technologically rag has been enriched through various innovative approaches addressing pivotal questions such as what to retrieve when to retrieve and how to use the retrieved information for what to retrieve research has progressed from simple token khandelwal et al 2019 and entity retrieval', metadata={'page': 2, 'source': '../documents/2312.10997.pdf', 'start_index': 806}),\n",
       " Document(page_content='that utilizes content generated by the llms them- selves for retrieval and augmentation purposes augmented with unstructured data unstructured text is gathered from corpora such as prompt data for fine-tuning large models cheng et al 2023a and cross-lingual data liet al 2023b retrieval units vary from tokens eg knn-lm khandelwal et al 2019 to phrases eg npm cog leeet al 2020 lan et al 2022 and document paragraphs with finer granularities offering pre- cision at the cost of increased retrieval complexity flare jiang et al 2023b introduces an active re- trieval approach triggered by the lms generation of low- probability words it creates a temporary sentence for doc- ument retrieval then regenerates the sentence with the re- trieved context to predict subsequent sentences retro uses the previous chunk to retrieve the nearest neighbor at the chunk level combined with the previous chunks context it guides the generation of the next chunk to preserve causal- ity the generation of the next', metadata={'page': 13, 'source': '../documents/2312.10997.pdf', 'start_index': 1600})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_pre.invoke(\"What is the document about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='alternates placing the best docu- ment at the beginning and end of the context window ad- ditionally approaches like cohereai rerank cohere 2023 bge-rerank7 and longllmlingua jiang et al 2023a re- calculate the semantic similarity between relevant text and the query addressing the challenge of interpreting vector-based simulated searches for semantic similarity prompt compression research indicates that noise in re- trieved documents adversely affects rag performance in post-processing the emphasis lies in compressing irrelevant context highlighting pivotal paragraphs and reducing the overall context length approaches such as selective context and llmlingua litman et al 2020 anderson et al 2022 utilize small language models to calculate prompt mu- tual information or perplexity estimating element impor- tance recomp xuet al 2023a addresses this by train- ing compressors at different granularities while long context xuet al 2023b and walking in the memory maze chen et al 2023a design', metadata={'page': 4, 'source': '../documents/2312.10997.pdf', 'start_index': 4001}),\n",
       " Document(page_content='is heavily impacted by the selection of data sources for augmentation different levels of knowledge and dimensions require distinct processing tech- niques they are categorized as unstructured data structured data and content generated by llms the technology tree of representative rag research with different augmentation aspects is depicted in figure 5 the leaves colored in three different shades represent enhancements using various types of data unstructured data structured data and content gener- ated by llms the diagram clearly shows that initially aug- mentation was mainly achieved through unstructured data such as pure text this approach later expanded to include the use of structured data eg knowledge graph for further improvement more recently there has been a growing trend in research that utilizes content generated by the llms them- selves for retrieval and augmentation purposes augmented with unstructured data unstructured text is gathered from corpora such as prompt data', metadata={'page': 13, 'source': '../documents/2312.10997.pdf', 'start_index': 796}),\n",
       " Document(page_content='dispelling ambiguity in entities and terms confirming factual accuracy maintaining context and updating outdated documents optimizing index structures involves adjusting the size of chunks to capture relevant context querying across multiple index paths and incorporating information from the graph structure to capture relevant context by leveraging relation- ships between nodes in a graph data index adding metadata information involves integrating refer- enced metadata such as dates and purposes into chunks for filtering purposes and incorporating metadata like chapters and subsections of references to improve retrieval efficiency alignment optimization addresses alignment issues and disparities between documents by introducing hypothetical questions liet al 2023d into documents to rectify align- ment issues and differences retrieval during the retrieval stage the primary focus is on identifying the appropriate context by calculating the similarity between the query and chunks the', metadata={'page': 4, 'source': '../documents/2312.10997.pdf', 'start_index': 805})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_pre.invoke(\"What is the central theme of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_pre = make_retriever(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801}),\n",
       " Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801}),\n",
       " Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_pre.invoke(\"What is the main topic of the document?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import fitz\n",
    "import torch\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from langchain import hub\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#from langchain_community.llms import HuggingFacePipeline\n",
    "#from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "#TEST\n",
    "from FakeLLM import FakePromptCopyLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# 假设你有一个PDF文件的路径\n",
    "pdf_path = \"../documents/barlowtwins-CXR.pdf\"\n",
    "\n",
    "# 以二进制读取模式打开PDF文件，并读取其内容\n",
    "with open(pdf_path, 'rb') as pdf_file:\n",
    "    pdf_bytes = pdf_file.read()  # 读取文件内容到一个字节串变量中\n",
    "\n",
    "# 使用读取到的字节串创建一个BytesIO对象\n",
    "file_stream = io.BytesIO(pdf_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "doc = fitz.open(stream=file_stream, filetype=\"pdf\")\n",
    "# 遍历PDF中的每一页\n",
    "for page_num in range(len(doc)):\n",
    "    page = doc[page_num]  # 获取页面对象\n",
    "    text = page.get_text()  # 提取当前页面的文本\n",
    "    documents.append(text)  # 将文本添加到documents列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning\\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸cois Samson3,\\nDianbo Liu2,4\\n1*D´epartement d’informatique et de recherche op´erationnelle, Universit´e\\nde Montr´eal, 2920 chemin de la Tour, Montr´eal, H3T 1J4, QC, Canada.\\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´eal, H2S\\n3H1, QC, Canada.\\n3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ˆIle-de-Montr´eal, 400 Blvd. De Maisonneuve Ouest,\\nMontr´eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National\\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.\\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;\\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\\nAbstract\\nBackground: Chest X-ray imaging based abnormality localization, essential in\\ndiagnosing various diseases, faces significant clinical challenges due to complex\\ninterpretations and the growing workload of radiologists. Recent advances in deep\\nlearning, especially self-supervised learning, offer promising solutions to enhance\\nimage analysis efficiency, accuracy and reliability.\\nThis study aims to improve autonomic abnormality localization performance of\\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\\nsupervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1\\n',\n",
       " 'backbone pre-trained on ImageNet. This was followed by supervised fine-tuning\\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.\\nResults: Our experiments showed a significant improvement in model perfor-\\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\\nracy of chest X-ray image base abnormality localization, outperforming tradi-\\ntional transfer learning methods. Its ability to adapt to various imaging conditions\\nand regional variations demonstrates the potential of self-supervised learning in\\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\\nparticularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.\\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.\\n1 Introduction\\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\\ntriaging patients and prioritizing the care order due to its convenience and flexibility.\\nEffective mitigation addresses the lack of availability of computed tomography and\\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\\nplex interpretation often requires a highly qualified radiologist to make an accurate\\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\\ning and remote areas, qualified radiologists are insufficient to cope with the increased\\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\\npopulation [4]. This situation necessitates urgently developing and introducing auto-\\nmated technologies like AI-based image analysis tools to aid radiologists in quicker\\nand more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.\\nIn recent years, deep learning models have rapidly advanced in various medi-\\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\\nysis because it can identify and precisely locate the types of anomalies in the images,\\n2\\n',\n",
       " 'providing doctors with more specific and valuable information. However, training\\nthese models requires a large amount of annotated data. These annotations must be\\nperformed by experienced radiologists for CXR images, as well as for most medi-\\ncal images, making such annotated data not only costly, but also rare, with only a\\nvery limited number of public datasets including bounding box information. Although\\ntransfer learning is widely regarded as an effective method to solve the problem\\nof scarce labelling data, its application in medical image analysis still faces limita-\\ntions. This is mainly due to the significant difference in feature distribution between\\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\\ndatasets. This disparity suggests that directly applying these pre-trained weights to\\nmedical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].\\nTo fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\\nemploying a dual-phase training process to enhance CXR image analysis. The first\\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\\napplied to fine-tune the model. Our experiments show that such a training strategy\\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\\nmodel performance. We observed a 3% increase in model accuracy on the mean\\nAverage Precision benchmark, surpassing the results achieved by directly performing\\nconventional transfer learning from ImageNet pre-trained weights.\\nThis study extends the application of self-supervised learning to chest X-ray abnor-\\nmality localization. It demonstrates the potential of self-supervised learning in medical\\nimaging analysis, especially in the absence of annotated data. By effectively improving\\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\\nresents a significant advancement in the field of CXR abnormality localization, paving\\nthe way for more efficient and accurate diagnostic methods in the future.\\n2 Related Work\\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\\nmodels trained with vast amounts of labelled data are capable of identifying features\\nof various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\\nimages [13]. This achievement is attributed to the advanced image recognition and\\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\\narchitecture which incorporates attention mechanisms, as proposed by Guszt´av Ga´al\\net al. [14], has made significant strides in accurately segmenting lung structures, thus\\naiding in detailed analysis and diagnosis of diseases.\\n3\\n',\n",
       " 'Self-supervised learning has recently gained popularity in the field of medical imag-\\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\\nuseful feature representations by training deep networks on unsupervised data [16].\\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\\ntionships in data by designing innovative pretext tasks, such as image reconstruction\\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\\net al. used large-scale images for self-supervised learning to improve accuracy and con-\\nvergence speed significantly in downstream tasks, achieving better performance than\\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model\\nbased on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].\\nIn terms of transfer learning, applying models trained in one domain to another has\\nled to notable success in medical image analysis. Research indicates that well-processed\\ntransfer results from ImageNet can improve model performance in the medical imag-\\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\\nthe significant difference in feature distribution between medical and natural images,\\nfeatures learned from natural images may not always be broadly applicable to med-\\nical images [23]. Various cross-domain adaptive transfer learning methods have been\\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\\ning and sequential domain adaptation techniques. By tuning model parameters, these\\nmethods can be better adapted to the characteristics of medical images, improving\\nthe performance and accuracy of models in medical image analysis [22].\\n3 Methods\\n3.1 Dataset Selection\\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\\n14 diseases with image-level annotations, including disease location annotations in\\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.\\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\\ning sets. The training set, consisting of 15,000 images, was annotated independently\\nby three radiologists per image. In contrast, the test set, comprising 3,000 images,\\nunderwent a more rigorous annotation process. Initially, each image was independently\\nannotated by three radiologists. This is followed by a secondary review phase where\\nthese initial annotations are reviewed by two other more experienced radiologists, they\\ncommunicated with each other to resolve any disagreements and reach a consensus on\\n4\\n',\n",
       " 'Count\\ndisease/abnormality\\nNo Finding\\nInfiltration\\nEffusion\\nAtelectasis\\nNodule\\nMass\\nPneumothorax\\nConsolidation\\nPleural_Thickening\\nCardiomegaly\\nEmphysema\\nEdema\\nFibrosis\\nPneumonia\\nHernia\\n0\\n10000\\n20000\\n30000\\n40000\\n50000\\n60000\\n70000\\nFig. 1 Image-level label distribution of the NIH-CXR dataset.\\nthe final labelling. This meticulous process for the test set created a potential dispar-\\nity in data distribution compared to the training set. To eliminate any bias it might\\nintroduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.\\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\\ncalculating the weighted average of each set of duplicate bounding boxes to create a\\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-\\ntion redundancy and improving target area representation in the dataset. Figure 2\\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\\nable dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.\\n3.2 Dual-Phase Training Process\\nOur training encompasses two primary phases: self-supervised pre-training and sub-\\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\\napproach refined the ImageNet pre-trained model by updating its backbone weights.\\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step\\naims to further improve the model’s task-specific performance, explicitly enhancing\\nits capabilities in localized diseases in CXR images.\\n5\\n',\n",
       " 'a)\\nb)\\nFig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\\npreprocessing.\\n3.2.1 Self-Supervised Pre-training\\nFor the first stage of training, we employed the original version of the Barlow Twins\\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\\nconventional contrastive learning, introducing a self-supervised learning framework\\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\\na straightforward yet potent principle: it learns distinctive features by reducing the\\nrepresentational differences between two differently distorted images from the same\\nsource as processed by the network. This strategy is instrumental in enabling the model\\nto identify unique and rich features in each image while concurrently minimizing the\\noverlap in features. The process involves generating two distinct variants of an image\\nthrough data augmentation, followed by their simultaneous processing via two deep\\n6\\n',\n",
       " 'Input\\nimage\\nC2\\nC3\\nC4\\nC5\\nP2\\nP3\\nP4\\nP5\\nResnet50 Backbone\\nconv\\nrpn_cls\\nrpn_reg\\nRoI pooling\\nRegion Proposal Network\\nFC\\nFC\\nFC\\nFeature map\\nFeature Vector\\nFeature map\\nClass\\nBox\\nFeature\\nPyramid\\nNetwork\\nX\\nY A\\nY B\\nImages\\nDistorted\\nimages\\nZ A\\nZ B\\nResnet 50\\nResnet 50\\nNet\\nEmbeddings\\nEmporocal\\ncross-corr.\\nTarget\\ncross-corr.\\nI\\nLBT\\nfeature dimension\\nBarlow twins\\nFaster R-CNN\\nFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with\\nmulti-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\\nfor object detection tasks.\\nneural networks that share identical weights. The objective is to align the network’s\\nweights to enhance the similarity in the high-level representations of these image pairs\\nyet ensure that the individual features remain distinct and independent.\\nThe Barlow Twins method might be particularly useful for medical imaging\\nbecause it extracts features by minimizing the redundancy between representations of\\nperturbed images. In CXR imaging, subtle differences might indicate important health\\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\\nimportant features. In contrast to other contrastive learning algorithms like MoCo\\n[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\\nTwins works at the feature dimension. It aims to assign an independent meaning to\\neach feature dimension. This could lead to a richer feature representation, potentially\\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\\n7\\n',\n",
       " 'pathological states). Moreover, compared to self-supervised learning methods requir-\\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\\noffers a more straightforward training framework, which is particularly important in\\nsituations with limited computational resources.\\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\\nfrom the Torchvision library, this step brings no additional cost. We used images from\\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.\\n3.2.2 Fine-tuning Phase\\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with\\nFeature Pyramid Network (FPN) [28] as our object detector and trained it on the\\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\\nR-CNN detector employs these regions to detect and classify targets. This architecture\\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\\nPyramid Network (FPN), an architecture frequently employed in object detection,\\nparticularly enhances performance with multi-scale targets. It integrates high-level\\nsemantic information from deeper layers with detailed information from shallower lay-\\ners, producing feature maps of varied scales that effectively detect differently sized\\ntargets.\\nWe employed the MMdetection [29] machine learning toolbox as the platform for\\nFaster R-CNN, utilizing a number of classical image augmentation techniques and\\nmaintaining consistent hyperparameters across all experiments. Two different input\\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\\non the model’s performance with the pre-trained models. In addition, for comparison,\\nwe also conducted experiments using ImageNet pre-trained weights directly.\\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\\ncomprehensively evaluate the self-supervised learning model’s performance in medical\\nimaging. This method examines the model’s feature transfer capability - its ability to\\nadapt learned representations to new tasks. We first resplit the test set of the NIH\\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\\nand the remaining 20% as an evaluation test set for assessing model performance.\\nWe adopted two distinct strategies during the evaluation: freezing the backbone\\nweights or fine-tuning the weights. In the freezing backbone strategy, we kept the\\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\\nthe entire network, encompassing both the self-supervised trained feature extraction\\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\\n8\\n',\n",
       " 'evaluation training set data for training the linear classifier, allowing us to assess the\\nmodel’s performance across different scales of training data.\\nWhen evaluating the representation transfer ability of a self-supervised learning\\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\\ntest sets is consistent. We used the Iterative stratification for the multi-label data\\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\\nand test sets were roughly similar. This helped prevent biases due to uneven label\\ndistribution, making our evaluation results more reliable and convincing.\\n3.3 Results Analysis Process\\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\\nof our object detection models. mAP is a widely recognized and effective metric in\\nobject detection, calculated by averaging precision scores across various object detec-\\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\\nscores for each class. The proportion of correct predictions relative to all predictions\\nfor a specific class across different detection confidence thresholds determines the pre-\\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\\nfor a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.\\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\\nprecision and recall, making it an especially appropriate performance indicator for\\nthis field. The AUC metric represents the area under the Receiver Operating Char-\\nacteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\\nfor imbalanced datasets as it does not rely directly on classification thresholds.\\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\\nlutional layer and observes the impact on the output class scores. This process reveals\\nthe most influential regions for the model’s decision-making. The resulting heat maps\\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.\\n4 Results\\n4.1 Transfer Learning on VinDr Abnormality Localization\\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\\n9\\n',\n",
       " 'using two different input resolutions. Consistent hyperparameter settings were main-\\ntained across all experiments, ensuring that the performance changes were attributable\\nonly to the merits of the pretraining method itself. We visualized the performance of\\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\\non the validation set in Figure 4, and tabulated the corresponding mAP performance\\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\\nperformance baseline without pre-training benefits.\\na)\\nb)\\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\\nbetween the lowest and highest value.\\nTable 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.\\nBackBone weight\\nInput size\\nmAP50 (val set)\\nmAP50 (test set)\\nbaseline nopretrained\\n224\\n0.1388 (0.1352,0.1424)\\n0.1342 (0.1306,0.1378)\\nImageNet pretrained\\n0.2245 (0.2204,0.2286)\\n0.2210 (0.2194,0.2226)\\nBarlow twins\\n0.2555 (0.2485,0.2626)\\n0.2448 (0.2414,0.2482)\\nBarlow twins from ImageNet\\n0.2625 (0.2568,0.2682)\\n0.2502 (0.2476,0.2528)\\nImageNet pretrained\\n640\\n0.2973 (0.2913,0.3033)\\n0.280 (0.2757,0.2848)\\nBarlow twins from ImageNet\\n0.3102 (0.3080,0.3125)\\n0.289 (0.2826,0.2954)\\n1Scores are presented with 95% confidence intervals.\\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\\npre-training in feature representation across disparate image domains.\\n10\\n',\n",
       " 'More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features\\nwith BarlowTwins-CXR, potentially offering more targeted information for clinical\\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\\nderived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.\\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\\nlow Twin-CXR weighted models saw performance improvements due to the increased\\ndetail in the CXR images. Nonetheless, the performance differential between the\\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.\\nThis points to intriguing future research avenues, such as refining image resolu-\\ntion parameters during pre-training and fine-tuning phases and investigating whether\\nhigher-resolution pre-training could elevate model performance. It also accentuates the\\nnecessity of tailoring deep learning model design to specific tasks, considering factors\\nlike image resolution and feature granularity.\\nOverall, implementing the Barlow Twins-CXR method on the VinDr dataset\\nresulted in substantial gains despite its data limitations and the inherent challenges\\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\\nimage weights for better applicability in CXR image analysis and beyond in medical\\nimaging.\\n4.2 Linear Evaluation Protocol\\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\\ntraditional ImageNet pre-training on the linear classification performance within the\\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone\\nof the linear classifier and updating only the final linear layer’s weights. This approach\\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\\nthese experiments are presented in Figure 6 and Table 2.\\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\\n11\\n',\n",
       " 'Ground Truth Bbox\\nImageNet Weights\\nOur method\\n20e27597c972c6e7fdb4d1e7638e227e\\n03431b577d1ccf075e930c4c4913c079\\nfd810298e165ef0b9a88bb25fda7a34b\\nGround Truth Bbox\\nImageNet Weights\\nOur method\\n9eba0d101f410f9cdfae46cb094ae2a6\\n87a8df2f22475c7200ebe891d0f25b88\\nad86f42123384e2441cce36347aa7d1a\\na)\\nb)\\nFig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\\nbelow the heatmaps refer to the image numbers in the dataset.\\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\\nmethods at 224 and 640 input resolutions.\\nModel\\n1%\\n10%\\n100%\\nBarlowtwin-CXR\\n0.6586 (0.6556, 0.6616)\\n0.7773 (0.7756, 0.7790)\\n0.8031 (0.8027, 0.8035)\\nImage-Net\\n0.5932 (0.5913, 0.5951)\\n0.6855 (0.6822, 0.6889)\\n0.7098 (0.7089, 0.7107)\\n1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\\nimage analysis, particularly when annotated data is scarce.\\n12\\n',\n",
       " 'Fig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.\\n4.3 End-to-End Finetuning\\nIn our end-to-end experiments, where we permitted updates to all model layers, the\\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\\nImageNet pre-trained equivalent across all training set sizes. The results of these\\nexperiments are presented in Figure 7 and Table 3.\\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various\\ndataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.\\n13\\n',\n",
       " 'Table 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\\npre-training methods at 224 and 640 input resolutions.\\nModel\\n1%\\n10%\\n100%\\nBarlowtwin-CXR\\n0.6585 (0.6544, 0.6627)\\n0.7756 (0.7745, 0.7768)\\n0.8107 (0.8098, 0.8116)\\nImage-Net\\n0.6163 (0.6110, 0.6216)\\n0.7168 (0.7093, 0.7243)\\n0.7866 (0.7843, 0.7889)\\n1Scores are presented with 95% confidence intervals.\\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\nAUC than the ImageNet counterpart.\\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\\nover the frozen backbone setup was less marked, suggesting that the wealth of features\\nlearned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.\\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning\\nacross all model layers may elevate the risk of overfitting, particularly when data is\\nscarce. The narrowing performance differential between the two pre-training strategies\\nwith increasing data volume indicates that the distinction between domain-specific\\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\\ntial with larger datasets. This trend implies that the influence of the pre-training\\nstrategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.\\n5 Discussion\\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\\nunannotated CXR images for learning valuable representations and enhances trans-\\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\\nThis leads to quicker training and improved performance on tasks like abnormality\\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input\\nresolutions, outshining models pre-trained on ImageNet.\\nOne of the primary limitations of our study is the scarcity of CXR datasets with\\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\\nmay limit the generalizability of our findings. Additionally, the computational cost\\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\\nimages with an image size of 224*224 pixels, the training process required two days\\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\\nability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.\\n6 Future Work\\nOur future endeavours include developing a demo interactive system for deployment\\nand testing in emergency rooms. It will allow practical evaluation of the model’s\\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\\n14\\n',\n",
       " 'Additionally, we plan to explore more advanced self-supervised learning methods,\\nobject detection frameworks, and backbone networks to refine our approach further.\\nThe continuous evolution of these technologies promises to address some of the current\\nlimitations and expand the applicability and accuracy of our model in medical image\\nanalysis.\\n7 Conclusions\\nThe results of this study provide strong support for the application of self-supervised\\nlearning in the field of abnormality detection, especially valuable in environments\\nwhere radiologists face high workloads but the corresponding data labelling resources\\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\\nCXR image, attributable to differences in imaging equipment, patient demographics,\\nand other locale-specific factors [34][35]. Such variations often impede the cross-\\nregional applicability of a model, thus limiting its generalizability. By employing the\\nBarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\\nnetworks to local datasets tailored to the unique characteristics of their regional data.\\nOur findings might also have significant implications for clinical practice, suggest-\\ning that this strategy could be a game-changer in aiding radiologists to interpret\\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\\ntially increasing patients’ throughput and improving the overall quality of care.\\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\\nholds particular promise in areas where standardization of medical imaging presents\\nchallenges.\\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.\\n8 Abbreviations\\nAP: anterior-posterior\\nAUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping\\nCIUSSS: Centre int´egr´e universitaire de sant´e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer\\nFPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic\\nROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate\\n15\\n',\n",
       " 'WBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n9 Declarations\\n9.1 Ethics approval and consent to participate\\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\\nrations of Helsinki). The studies reported in this manuscript used reputable public\\ndatasets and did not require any additional data involving human participants, human\\ndata, or human tissue.\\n9.2 Consent for publication\\nNot applicable\\n9.3 Availability of data and materials\\nThe datasets generated and/or analysed during the current study are available in the\\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\\n9.4 Competing interests\\nThe authors declare that they have no competing interests\\n9.5 Funding\\nNo external funding was associated with this research study.\\n9.6 Authors’ contributions\\nHS designed the research methodology, analyzed data, was responsible for experiments\\nand results visualization, and participated in manuscript drafting and revision. LM\\nassisted in developing the research methodology and contributed to the drafting and\\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\\nin statistical analysis. DL contributed to the study design, offered statistical analysis\\nexpertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.\\nAll authors read and approved the final manuscript.\\n9.7 Acknowledgements\\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆıle-de-\\nmontr´eal for the computational resources and support provided, which were essential\\nfor the research conducted as part of the graduate internship program. We are espe-\\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\\nTheir expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.\\n16\\n',\n",
       " 'References\\n[1] Satia, I., Bashagha, S., Bibi, A., et al.: Assessing the accuracy and certainty in\\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502\\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al.: The role of chest imaging\\nin patient management during the covid-19 pandemic: a multinational consensus\\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978\\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\\nTrend in radiologist workload compared to number of admissions in the emer-\\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337\\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage.\\nAccessed:\\ndate-of-access (2022)\\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al.: Effect of a comprehensive\\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:\\na retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054\\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al.: Transfer learning for medical\\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7\\n[8] Zbontar, J., Jing, L., Misra, I., et al.: Barlow twins: Self-supervised learning\\nvia redundancy reduction. In: Proceedings of the International Conference on\\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230\\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al.: Vindr-cxr: An open dataset of chest\\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w\\n17\\n',\n",
       " '[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90\\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225\\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\\n[14] Ga´al, G., Maga, B., Luk´acs, A.: Attention u-net based adversarial architectures\\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304\\n[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045\\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al.: Greedy layer-wise training of deep\\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476\\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al.: Context encoders: Feature\\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278\\n[18] Chen, T., Kornblith, S., Norouzi, M., et al.: A simple framework for contrastive\\nlearning of visual representations. In: Proceedings of the International Conference\\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.\\n3525087\\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728\\n[20] Azizi, S., Mustafa, B., Ryan, F., et al.: Big self-supervised models advance medical\\nimage classification. In: Proceedings of the IEEE/CVF International Conference\\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346\\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n18\\n',\n",
       " 'representation and transferability of chest x-ray models. In: Medical Imaging with\\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\\ntransfer learning work for medical images: Feature reuse & other factors. In:\\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901\\n[24] Wang, X., Peng, Y., Lu, L., et al.: Chestx-ray8: Hospital-scale chest x-ray\\ndatabase and benchmarks on weakly-supervised classification and localization\\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\\nfrom different object detection models. Image and Vision Computing 107, 104117\\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\\n[26] He, K., Fan, H., Wu, Y., et al.: Momentum contrast for unsupervised visual\\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975\\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169\\n[28] Lin, T. Y., Doll´ar, P., Girshick, R., et al.: Feature pyramid networks for object\\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106\\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155\\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\\nmizing mutual information across views. In: Proceedings of the 33rd International\\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\\n19\\n',\n",
       " '[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277\\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)\\n[33] Szyma´nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\\nlabel data. Proceedings of the First International Workshop on Learning with\\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756\\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979\\n[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n20\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"../documents/barlowtwins-CXR.pdf\")\n",
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning\\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸ cois Samson3,\\nDianbo Liu2,4\\n1*D´ epartement d’informatique et de recherche op´ erationnelle, Universit´ e\\nde Montr´ eal, 2920 chemin de la Tour, Montr´ eal, H3T 1J4, QC, Canada.\\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´ eal, H2S\\n3H1, QC, Canada.\\n3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ ˆIle-de-Montr´ eal, 400 Blvd. De Maisonneuve Ouest,\\nMontr´ eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National\\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.\\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;\\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\\nAbstract\\nBackground: Chest X-ray imaging based abnormality localization, essential in\\ndiagnosing various diseases, faces significant clinical challenges due to complex\\ninterpretations and the growing workload of radiologists. Recent advances in deep\\nlearning, especially self-supervised learning, offer promising solutions to enhance\\nimage analysis efficiency, accuracy and reliability.\\nThis study aims to improve autonomic abnormality localization performance of\\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\\nsupervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0}),\n",
       " Document(page_content='backbone pre-trained on ImageNet. This was followed by supervised fine-tuning\\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.\\nResults: Our experiments showed a significant improvement in model perfor-\\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\\nracy of chest X-ray image base abnormality localization, outperforming tradi-\\ntional transfer learning methods. Its ability to adapt to various imaging conditions\\nand regional variations demonstrates the potential of self-supervised learning in\\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\\nparticularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.\\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.\\n1 Introduction\\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\\ntriaging patients and prioritizing the care order due to its convenience and flexibility.\\nEffective mitigation addresses the lack of availability of computed tomography and\\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\\nplex interpretation often requires a highly qualified radiologist to make an accurate\\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\\ning and remote areas, qualified radiologists are insufficient to cope with the increased\\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\\npopulation [4]. This situation necessitates urgently developing and introducing auto-\\nmated technologies like AI-based image analysis tools to aid radiologists in quicker\\nand more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.\\nIn recent years, deep learning models have rapidly advanced in various medi-\\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\\nysis because it can identify and precisely locate the types of anomalies in the images,\\n2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1}),\n",
       " Document(page_content='providing doctors with more specific and valuable information. However, training\\nthese models requires a large amount of annotated data. These annotations must be\\nperformed by experienced radiologists for CXR images, as well as for most medi-\\ncal images, making such annotated data not only costly, but also rare, with only a\\nvery limited number of public datasets including bounding box information. Although\\ntransfer learning is widely regarded as an effective method to solve the problem\\nof scarce labelling data, its application in medical image analysis still faces limita-\\ntions. This is mainly due to the significant difference in feature distribution between\\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\\ndatasets. This disparity suggests that directly applying these pre-trained weights to\\nmedical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].\\nTo fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\\nemploying a dual-phase training process to enhance CXR image analysis. The first\\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\\napplied to fine-tune the model. Our experiments show that such a training strategy\\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\\nmodel performance. We observed a 3% increase in model accuracy on the mean\\nAverage Precision benchmark, surpassing the results achieved by directly performing\\nconventional transfer learning from ImageNet pre-trained weights.\\nThis study extends the application of self-supervised learning to chest X-ray abnor-\\nmality localization. It demonstrates the potential of self-supervised learning in medical\\nimaging analysis, especially in the absence of annotated data. By effectively improving\\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\\nresents a significant advancement in the field of CXR abnormality localization, paving\\nthe way for more efficient and accurate diagnostic methods in the future.\\n2 Related Work\\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\\nmodels trained with vast amounts of labelled data are capable of identifying features\\nof various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\\nimages [13]. This achievement is attributed to the advanced image recognition and\\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\\narchitecture which incorporates attention mechanisms, as proposed by Guszt´ av Ga´ al\\net al. [14], has made significant strides in accurately segmenting lung structures, thus\\naiding in detailed analysis and diagnosis of diseases.\\n3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2}),\n",
       " Document(page_content='Self-supervised learning has recently gained popularity in the field of medical imag-\\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\\nuseful feature representations by training deep networks on unsupervised data [16].\\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\\ntionships in data by designing innovative pretext tasks, such as image reconstruction\\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\\net al. used large-scale images for self-supervised learning to improve accuracy and con-\\nvergence speed significantly in downstream tasks, achieving better performance than\\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model\\nbased on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].\\nIn terms of transfer learning, applying models trained in one domain to another has\\nled to notable success in medical image analysis. Research indicates that well-processed\\ntransfer results from ImageNet can improve model performance in the medical imag-\\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\\nthe significant difference in feature distribution between medical and natural images,\\nfeatures learned from natural images may not always be broadly applicable to med-\\nical images [23]. Various cross-domain adaptive transfer learning methods have been\\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\\ning and sequential domain adaptation techniques. By tuning model parameters, these\\nmethods can be better adapted to the characteristics of medical images, improving\\nthe performance and accuracy of models in medical image analysis [22].\\n3 Methods\\n3.1 Dataset Selection\\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\\n14 diseases with image-level annotations, including disease location annotations in\\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.\\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\\ning sets. The training set, consisting of 15,000 images, was annotated independently\\nby three radiologists per image. In contrast, the test set, comprising 3,000 images,\\nunderwent a more rigorous annotation process. Initially, each image was independently\\nannotated by three radiologists. This is followed by a secondary review phase where\\nthese initial annotations are reviewed by two other more experienced radiologists, they\\ncommunicated with each other to resolve any disagreements and reach a consensus on\\n4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3}),\n",
       " Document(page_content='Countdisease/abnormalityNo Finding\\nInfiltration\\nEffusion\\nAtelectasis\\nNodule\\nMass\\nPneumothorax\\nConsolidation\\nPleural_Thickening\\nCardiomegaly\\nEmphysema\\nEdema\\nFibrosis\\nPneumonia\\nHernia\\n0 10000 20000 30000 40000 50000 60000 70000Fig. 1 Image-level label distribution of the NIH-CXR dataset.\\nthe final labelling. This meticulous process for the test set created a potential dispar-\\nity in data distribution compared to the training set. To eliminate any bias it might\\nintroduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.\\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\\ncalculating the weighted average of each set of duplicate bounding boxes to create a\\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-\\ntion redundancy and improving target area representation in the dataset. Figure 2\\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\\nable dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.\\n3.2 Dual-Phase Training Process\\nOur training encompasses two primary phases: self-supervised pre-training and sub-\\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\\napproach refined the ImageNet pre-trained model by updating its backbone weights.\\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step\\naims to further improve the model’s task-specific performance, explicitly enhancing\\nits capabilities in localized diseases in CXR images.\\n5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4}),\n",
       " Document(page_content='a)\\nb)Fig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\\npreprocessing.\\n3.2.1 Self-Supervised Pre-training\\nFor the first stage of training, we employed the original version of the Barlow Twins\\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\\nconventional contrastive learning, introducing a self-supervised learning framework\\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\\na straightforward yet potent principle: it learns distinctive features by reducing the\\nrepresentational differences between two differently distorted images from the same\\nsource as processed by the network. This strategy is instrumental in enabling the model\\nto identify unique and rich features in each image while concurrently minimizing the\\noverlap in features. The process involves generating two distinct variants of an image\\nthrough data augmentation, followed by their simultaneous processing via two deep\\n6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 5}),\n",
       " Document(page_content='Input\\nimageC2 C3 C4 C5\\nP2 P3 P4 P5Resnet50 Backbone\\nconvrpn_cls\\nrpn_reg\\nRoI poolingRegion Proposal Network\\nFC FC FCFeature map\\nFeature VectorFeature mapClass\\nBoxFeature\\nPyramid\\nNetworkX\\nY AY B\\nImagesDistorted\\nimages\\nZ A\\nZ BResnet 50\\nResnet 50Net Embeddings\\nEmporocal\\ncross-corr .Target\\ncross-corr .\\nI\\nLBT\\nfeature dimensionBarlow twins\\nFaster  R-CNNFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with\\nmulti-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\\nfor object detection tasks.\\nneural networks that share identical weights. The objective is to align the network’s\\nweights to enhance the similarity in the high-level representations of these image pairs\\nyet ensure that the individual features remain distinct and independent.\\nThe Barlow Twins method might be particularly useful for medical imaging\\nbecause it extracts features by minimizing the redundancy between representations of\\nperturbed images. In CXR imaging, subtle differences might indicate important health\\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\\nimportant features. In contrast to other contrastive learning algorithms like MoCo\\n[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\\nTwins works at the feature dimension. It aims to assign an independent meaning to\\neach feature dimension. This could lead to a richer feature representation, potentially\\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\\n7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6}),\n",
       " Document(page_content='pathological states). Moreover, compared to self-supervised learning methods requir-\\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\\noffers a more straightforward training framework, which is particularly important in\\nsituations with limited computational resources.\\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\\nfrom the Torchvision library, this step brings no additional cost. We used images from\\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.\\n3.2.2 Fine-tuning Phase\\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with\\nFeature Pyramid Network (FPN) [28] as our object detector and trained it on the\\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\\nR-CNN detector employs these regions to detect and classify targets. This architecture\\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\\nPyramid Network (FPN), an architecture frequently employed in object detection,\\nparticularly enhances performance with multi-scale targets. It integrates high-level\\nsemantic information from deeper layers with detailed information from shallower lay-\\ners, producing feature maps of varied scales that effectively detect differently sized\\ntargets.\\nWe employed the MMdetection [29] machine learning toolbox as the platform for\\nFaster R-CNN, utilizing a number of classical image augmentation techniques and\\nmaintaining consistent hyperparameters across all experiments. Two different input\\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\\non the model’s performance with the pre-trained models. In addition, for comparison,\\nwe also conducted experiments using ImageNet pre-trained weights directly.\\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\\ncomprehensively evaluate the self-supervised learning model’s performance in medical\\nimaging. This method examines the model’s feature transfer capability - its ability to\\nadapt learned representations to new tasks. We first resplit the test set of the NIH\\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\\nand the remaining 20% as an evaluation test set for assessing model performance.\\nWe adopted two distinct strategies during the evaluation: freezing the backbone\\nweights or fine-tuning the weights. In the freezing backbone strategy, we kept the\\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\\nthe entire network, encompassing both the self-supervised trained feature extraction\\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\\n8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier, allowing us to assess the\\nmodel’s performance across different scales of training data.\\nWhen evaluating the representation transfer ability of a self-supervised learning\\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\\ntest sets is consistent. We used the Iterative stratification for the multi-label data\\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\\nand test sets were roughly similar. This helped prevent biases due to uneven label\\ndistribution, making our evaluation results more reliable and convincing.\\n3.3 Results Analysis Process\\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\\nof our object detection models. mAP is a widely recognized and effective metric in\\nobject detection, calculated by averaging precision scores across various object detec-\\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\\nscores for each class. The proportion of correct predictions relative to all predictions\\nfor a specific class across different detection confidence thresholds determines the pre-\\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\\nfor a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.\\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\\nprecision and recall, making it an especially appropriate performance indicator for\\nthis field. The AUC metric represents the area under the Receiver Operating Char-\\nacteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\\nfor imbalanced datasets as it does not rely directly on classification thresholds.\\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\\nlutional layer and observes the impact on the output class scores. This process reveals\\nthe most influential regions for the model’s decision-making. The resulting heat maps\\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.\\n4 Results\\n4.1 Transfer Learning on VinDr Abnormality Localization\\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\\n9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8}),\n",
       " Document(page_content='using two different input resolutions. Consistent hyperparameter settings were main-\\ntained across all experiments, ensuring that the performance changes were attributable\\nonly to the merits of the pretraining method itself. We visualized the performance of\\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\\non the validation set in Figure 4, and tabulated the corresponding mAP performance\\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\\nperformance baseline without pre-training benefits.\\na) b)\\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\\nbetween the lowest and highest value.\\nTable 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.\\nBackBone weight Input size mAP50 (val set) mAP50 (test set)\\nbaseline nopretrained 224 0.1388 (0.1352,0.1424) 0.1342 (0.1306,0.1378)\\nImageNet pretrained 0.2245 (0.2204,0.2286) 0.2210 (0.2194,0.2226)\\nBarlow twins 0.2555 (0.2485,0.2626) 0.2448 (0.2414,0.2482)\\nBarlow twins from ImageNet 0.2625 (0.2568,0.2682) 0.2502 (0.2476,0.2528)\\nImageNet pretrained 640 0.2973 (0.2913,0.3033) 0.280 (0.2757,0.2848)\\nBarlow twins from ImageNet 0.3102 (0.3080,0.3125) 0.289 (0.2826,0.2954)\\n1Scores are presented with 95% confidence intervals.\\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\\npre-training in feature representation across disparate image domains.\\n10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9}),\n",
       " Document(page_content='More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features\\nwith BarlowTwins-CXR, potentially offering more targeted information for clinical\\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\\nderived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.\\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\\nlow Twin-CXR weighted models saw performance improvements due to the increased\\ndetail in the CXR images. Nonetheless, the performance differential between the\\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.\\nThis points to intriguing future research avenues, such as refining image resolu-\\ntion parameters during pre-training and fine-tuning phases and investigating whether\\nhigher-resolution pre-training could elevate model performance. It also accentuates the\\nnecessity of tailoring deep learning model design to specific tasks, considering factors\\nlike image resolution and feature granularity.\\nOverall, implementing the Barlow Twins-CXR method on the VinDr dataset\\nresulted in substantial gains despite its data limitations and the inherent challenges\\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\\nimage weights for better applicability in CXR image analysis and beyond in medical\\nimaging.\\n4.2 Linear Evaluation Protocol\\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\\ntraditional ImageNet pre-training on the linear classification performance within the\\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone\\nof the linear classifier and updating only the final linear layer’s weights. This approach\\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\\nthese experiments are presented in Figure 6 and Table 2.\\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\\n11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10}),\n",
       " Document(page_content='Ground Truth Bbox ImageNet Weights Our method\\n20e27597c972c6e7fdb4d1e7638e227e\\n03431b577d1ccf075e930c4c4913c079\\nfd810298e165ef0b9a88bb25fda7a34bGround Truth Bbox ImageNet Weights Our method\\n9eba0d101f410f9cdfae46cb094ae2a6\\n87a8df2f22475c7200ebe891d0f25b88\\nad86f42123384e2441cce36347aa7d1aa) b)Fig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\\nbelow the heatmaps refer to the image numbers in the dataset.\\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\\nmethods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\\nImage-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)\\n1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\\nimage analysis, particularly when annotated data is scarce.\\n12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11}),\n",
       " Document(page_content='Fig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.\\n4.3 End-to-End Finetuning\\nIn our end-to-end experiments, where we permitted updates to all model layers, the\\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\\nImageNet pre-trained equivalent across all training set sizes. The results of these\\nexperiments are presented in Figure 7 and Table 3.\\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various\\ndataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.\\n13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12}),\n",
       " Document(page_content='Table 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\\npre-training methods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627) 0.7756 (0.7745, 0.7768) 0.8107 (0.8098, 0.8116)\\nImage-Net 0.6163 (0.6110, 0.6216) 0.7168 (0.7093, 0.7243) 0.7866 (0.7843, 0.7889)\\n1Scores are presented with 95% confidence intervals.\\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\nAUC than the ImageNet counterpart.\\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\\nover the frozen backbone setup was less marked, suggesting that the wealth of features\\nlearned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.\\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning\\nacross all model layers may elevate the risk of overfitting, particularly when data is\\nscarce. The narrowing performance differential between the two pre-training strategies\\nwith increasing data volume indicates that the distinction between domain-specific\\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\\ntial with larger datasets. This trend implies that the influence of the pre-training\\nstrategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.\\n5 Discussion\\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\\nunannotated CXR images for learning valuable representations and enhances trans-\\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\\nThis leads to quicker training and improved performance on tasks like abnormality\\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input\\nresolutions, outshining models pre-trained on ImageNet.\\nOne of the primary limitations of our study is the scarcity of CXR datasets with\\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\\nmay limit the generalizability of our findings. Additionally, the computational cost\\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\\nimages with an image size of 224*224 pixels, the training process required two days\\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\\nability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.\\n6 Future Work\\nOur future endeavours include developing a demo interactive system for deployment\\nand testing in emergency rooms. It will allow practical evaluation of the model’s\\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\\n14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13}),\n",
       " Document(page_content='Additionally, we plan to explore more advanced self-supervised learning methods,\\nobject detection frameworks, and backbone networks to refine our approach further.\\nThe continuous evolution of these technologies promises to address some of the current\\nlimitations and expand the applicability and accuracy of our model in medical image\\nanalysis.\\n7 Conclusions\\nThe results of this study provide strong support for the application of self-supervised\\nlearning in the field of abnormality detection, especially valuable in environments\\nwhere radiologists face high workloads but the corresponding data labelling resources\\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\\nCXR image, attributable to differences in imaging equipment, patient demographics,\\nand other locale-specific factors [34][35]. Such variations often impede the cross-\\nregional applicability of a model, thus limiting its generalizability. By employing the\\nBarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\\nnetworks to local datasets tailored to the unique characteristics of their regional data.\\nOur findings might also have significant implications for clinical practice, suggest-\\ning that this strategy could be a game-changer in aiding radiologists to interpret\\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\\ntially increasing patients’ throughput and improving the overall quality of care.\\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\\nholds particular promise in areas where standardization of medical imaging presents\\nchallenges.\\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.\\n8 Abbreviations\\nAP: anterior-posterior\\nAUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping\\nCIUSSS: Centre int´ egr´ e universitaire de sant´ e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer\\nFPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic\\nROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate\\n15', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14}),\n",
       " Document(page_content='WBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n9 Declarations\\n9.1 Ethics approval and consent to participate\\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\\nrations of Helsinki). The studies reported in this manuscript used reputable public\\ndatasets and did not require any additional data involving human participants, human\\ndata, or human tissue.\\n9.2 Consent for publication\\nNot applicable\\n9.3 Availability of data and materials\\nThe datasets generated and/or analysed during the current study are available in the\\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\\n9.4 Competing interests\\nThe authors declare that they have no competing interests\\n9.5 Funding\\nNo external funding was associated with this research study.\\n9.6 Authors’ contributions\\nHS designed the research methodology, analyzed data, was responsible for experiments\\nand results visualization, and participated in manuscript drafting and revision. LM\\nassisted in developing the research methodology and contributed to the drafting and\\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\\nin statistical analysis. DL contributed to the study design, offered statistical analysis\\nexpertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.\\nAll authors read and approved the final manuscript.\\n9.7 Acknowledgements\\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆ ıle-de-\\nmontr´ eal for the computational resources and support provided, which were essential\\nfor the research conducted as part of the graduate internship program. We are espe-\\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\\nTheir expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.\\n16', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15}),\n",
       " Document(page_content='References\\n[1] Satia, I., Bashagha, S., Bibi, A., et al. : Assessing the accuracy and certainty in\\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502\\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al. : The role of chest imaging\\nin patient management during the covid-19 pandemic: a multinational consensus\\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978\\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\\nTrend in radiologist workload compared to number of admissions in the emer-\\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337\\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\\ndate-of-access (2022)\\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al. : Effect of a comprehensive\\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:\\na retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054\\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al. : Transfer learning for medical\\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7\\n[8] Zbontar, J., Jing, L., Misra, I., et al. : Barlow twins: Self-supervised learning\\nvia redundancy reduction. In: Proceedings of the International Conference on\\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230\\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al. : Vindr-cxr: An open dataset of chest\\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w\\n17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16}),\n",
       " Document(page_content='[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90\\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225\\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\\n[14] Ga´ al, G., Maga, B., Luk´ acs, A.: Attention u-net based adversarial architectures\\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304\\n[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045\\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al. : Greedy layer-wise training of deep\\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476\\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al. : Context encoders: Feature\\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278\\n[18] Chen, T., Kornblith, S., Norouzi, M., et al. : A simple framework for contrastive\\nlearning of visual representations. In: Proceedings of the International Conference\\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.\\n3525087\\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728\\n[20] Azizi, S., Mustafa, B., Ryan, F., et al. : Big self-supervised models advance medical\\nimage classification. In: Proceedings of the IEEE/CVF International Conference\\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346\\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17}),\n",
       " Document(page_content='representation and transferability of chest x-ray models. In: Medical Imaging with\\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\\ntransfer learning work for medical images: Feature reuse & other factors. In:\\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901\\n[24] Wang, X., Peng, Y., Lu, L., et al. : Chestx-ray8: Hospital-scale chest x-ray\\ndatabase and benchmarks on weakly-supervised classification and localization\\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\\nfrom different object detection models. Image and Vision Computing 107, 104117\\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\\n[26] He, K., Fan, H., Wu, Y., et al. : Momentum contrast for unsupervised visual\\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975\\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169\\n[28] Lin, T. Y., Doll´ ar, P., Girshick, R., et al. : Feature pyramid networks for object\\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106\\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155\\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\\nmizing mutual information across views. In: Proceedings of the 33rd International\\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\\n19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18}),\n",
       " Document(page_content='[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277\\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)\\n[33] Szyma´ nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\\nlabel data. Proceedings of the First International Workshop on Learning with\\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756\\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979\\n[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'page': 0, 'source': '../barlowtwins-CXR.pdf', 'start_index': 1562}), Document(page_content='Model 1% 10% 100%\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\\nImage-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)\\n1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority', metadata={'page': 11, 'source': '../barlowtwins-CXR.pdf', 'start_index': 787})]\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(\"../barlowtwins-CXR.pdf\")\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_documents(all_splits, embeddings)\n",
    "\n",
    "retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "retrieved_docs = retriever.invoke(\"What is Barlowtwins?\")\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-Chat-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:555: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, they weave and they learn,\n",
      "From\n"
     ]
    }
   ],
   "source": [
    "#quantization_config = BitsAndBytesConfig(load_in_8bit=True) #bug\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"TheBloke/Llama-2-7B-Chat-GPTQ\",\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            token=True,\n",
    "            load_in_8bit=False\n",
    "        )\n",
    "model = model\n",
    "pipe = pipeline(\n",
    "                model=model,\n",
    "                task='text-generation',\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=100\n",
    "            )\n",
    "pipeline = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Electroencephalography (EEG) is a non-invasive neuroimaging technique that measures the electrical activity of the brain. It is used to diagnose and monitor a variety of neurological conditions, including epilepsy, seizures, and brain tumors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | pipeline\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = (\n",
    "            f\"You are an assistant for question-answering tasks.\" \n",
    "            \"Use the following pieces of retrieved context to answer the question. \"\n",
    "            \"If you don't know the answer, just say that you don't know. \"\n",
    "            \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "            \"Question: {question} \\n\"\n",
    "            \"Context: {context} \\n\"\n",
    "            \"Answer:\"\n",
    "            )\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\generation\\utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:728: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' BarlowTwins-CXR is a pre-training approach for question-answering tasks. It uses an adjusted Barlow Twins algorithm on the NIH dataset with a ResNet50 model to perform self-supervised pre-training. This approach led to a rapid performance ascent and a significant increase in detection performance compared to traditional ImageNet weights.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normal Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | pipeline\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"What is Barlowtwins?\")\n",
    "# for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "#     print(chunk, end=\"\", flush=True)\n",
    "# 正确调用链的方式\n",
    "# 注意：这里假设 retriever 和其他组件已经被正确配置\n",
    "# question = \"what is journals name\"\n",
    "# response = rag_chain.invoke({\"question\": question})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'page': 0, 'source': '../barlowtwins-CXR.pdf', 'start_index': 1562}),\n",
       "  Document(page_content='More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features', metadata={'page': 10, 'source': '../barlowtwins-CXR.pdf', 'start_index': 0})],\n",
       " 'question': 'What is Barlowtwins?',\n",
       " 'answer': ' BarlowTwins-CXR is a pre-training approach for question-answering tasks. It uses an adjusted Barlow Twins algorithm on the NIH dataset with a ResNet50 model to perform self-supervised pre-training. This approach led to a rapid performance ascent and a significant increase in detection performance compared to traditional ImageNet weights.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with return source\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | pipeline\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "rag_chain_with_source.invoke(\"What is Barlowtwins?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Reformulate the following chat history and question so it can be understood easier. DO NOT answer it.\\n\\n\"\n",
    "    \"Question: {question} \\n\\n\"\n",
    "    \"chat history: {chat_history} \\n\\n\"\n",
    "    \"Reformulated Question: \"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = contextualize_q_prompt | pipeline | StrOutputParser()\n",
    "\n",
    "# contextualize_q_chain.invoke(\n",
    "#     {\n",
    "#         \"chat_history\": [\n",
    "#             HumanMessage(content=\"1 + 1 = what\"),\n",
    "#             AIMessage(content=\"I dont know\"),\n",
    "#         ],\n",
    "#         \"question\": \"what is 1 + 1?\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "qa_system_prompt = (\n",
    "            f\"You are an assistant for question-answering tasks.\" \n",
    "            \"Use the following pieces of retrieved context to answer the question. \"\n",
    "            \"If you don't know the answer, just say that you don't know. \"\n",
    "            \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "            \"Question: {question} \\n\"\n",
    "            \"Context: {context} \\n\"\n",
    "            \"Answer:\"\n",
    "            )\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | pipeline\n",
    ")\n",
    "\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHuman: Task Decomposition can be done in various ways, depending on the specific task and context. Some common methods include:\\n1. Breaking down a task into smaller, more manageable sub-tasks.\\n2. Identifying the key steps involved in a task and prioritizing them.\\n3. Creating a workflow or flowchart to visualize the steps involved in a task.\\n4. Using a checklist or template to guide the completion of a task.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\generation\\utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:728: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\n",
      "a two-stage training process. Initially, self-supervised pre-training was performed\n",
      "using an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\n",
      "1\n",
      "\n",
      "Model 1% 10% 100%\n",
      "Barlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\n",
      "Image-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)\n",
      "1Scores are presented with 95% confidence intervals.\n",
      "CI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\n",
      "trained model. As the training data size increased to 10% and 100%, the AUCs for\n",
      "the Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\n",
      "0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\n",
      "scored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\n",
      "Notably, the incremental gains for both pre-training methods diminished with\n",
      "larger data sizes, suggesting that the performance boost provided by additional data\n",
      "becomes marginal when only the linear layer is updated.\n",
      "These findings highlight the Barlow Twins-CXR pre-training method’s superiority\n",
      "\n",
      "Question: What is Barlowtwins?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'What is Barlowtwins?', 'chat_history': [], 'answer': ' Barlowtwins is a pre-training method for deep neural networks that uses an adjusted Barlow Twins algorithm on the NIH dataset with a ResNet50 architecture.\\n\\n\\n', 'source_documents': [Document(page_content='Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'page': 0, 'source': '../barlowtwins-CXR.pdf', 'start_index': 1562}), Document(page_content='Model 1% 10% 100%\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\\nImage-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)\\n1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority', metadata={'page': 11, 'source': '../barlowtwins-CXR.pdf', 'start_index': 787})]}\n"
     ]
    }
   ],
   "source": [
    "# from langchain.memory import ConversationBufferMemory\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# chain_qa = ConversationalRetrievalChain.from_llm(\n",
    "#             pipeline,\n",
    "#             retriever=retriever\n",
    "#         )\n",
    "#result = chain_qa({\"question\": \"What is Barlowtwins?\", \"chat_history\": chat_history})\n",
    "#print(result)\n",
    "def get_chat_history(inputs) -> str:\n",
    "    res = []\n",
    "    for human, ai in inputs:\n",
    "        res.append(f\"Human:{human}\\nAI:{ai}\")\n",
    "    return \"\\n\".join(res)\n",
    "\n",
    "\n",
    "chain_qa_2 = ConversationalRetrievalChain.from_llm(\n",
    "            pipeline,\n",
    "            retriever=retriever,\n",
    "            condense_question_llm  = pipeline,\n",
    "            return_source_documents=True,\n",
    "            verbose=True,\n",
    "            get_chat_history=get_chat_history\n",
    "        )\n",
    "result = chain_qa_2({\"question\": \"What is Barlowtwins?\", \"chat_history\": chat_history})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human:What is Barlowtwins?\n",
      "AI: Barlowtwins is a pre-training method for deep neural networks that uses an adjusted Barlow Twins algorithm on the NIH dataset with a ResNet50 architecture.\n",
      "\n",
      "\n",
      "\n",
      "Follow Up Input: But what is Barlowtwins-CXR\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\n",
      "a two-stage training process. Initially, self-supervised pre-training was performed\n",
      "using an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\n",
      "1\n",
      "\n",
      "More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\n",
      "formance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\n",
      "expedited training trajectory and a significant increase in detection performance.\n",
      "When further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\n",
      "approach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\n",
      "0.2528), evidencing the synergetic effect of combining pre-training methodologies.\n",
      "The heat maps generated from the study present a compelling visualization of the\n",
      "performance of the BarlowTwins-CXR method compared to the traditional ImageNet\n",
      "weights approach. We generated heat maps of the first few CXR images of the train-\n",
      "ing and test sets in Figure 5. In each image, our method’s heat maps show a more\n",
      "focused alignment with the actual lesion areas marked by the Ground Truth Bbox.\n",
      "This indicates a higher precision in localizing and identifying pathological features\n",
      "\n",
      "Question:  Can you explain what Barlowtwins-CXR is?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'But what is Barlowtwins-CXR', 'chat_history': [('What is Barlowtwins?', ' Barlowtwins is a pre-training method for deep neural networks that uses an adjusted Barlow Twins algorithm on the NIH dataset with a ResNet50 architecture.\\n\\n\\n')], 'answer': \"  I don't know what Barlowtwins-CXR is. The article doesn't provide enough context to explain what it is, and I don't have enough knowledge to provide a definition.\", 'source_documents': [Document(page_content='Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'page': 0, 'source': '../barlowtwins-CXR.pdf', 'start_index': 1562}), Document(page_content='More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features', metadata={'page': 10, 'source': '../barlowtwins-CXR.pdf', 'start_index': 0})]}\n"
     ]
    }
   ],
   "source": [
    "chat_history = [(\"What is Barlowtwins?\", result[\"answer\"])]\n",
    "query = \"But what is Barlowtwins-CXR\"\n",
    "result = chain_qa_2({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\generation\\utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:728: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1682 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.83 GiB is allocated by PyTorch, and 467.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut what is Barlowtwins-CXR\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    151\u001b[0m     inputs,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:166\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    164\u001b[0m         new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[0;32m    165\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[1;32m--> 166\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    167\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_inputs\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m answer\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:543\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    539\u001b[0m         _output_key\n\u001b[0;32m    540\u001b[0m     ]\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    544\u001b[0m         _output_key\n\u001b[0;32m    545\u001b[0m     ]\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m     )\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    151\u001b[0m     inputs,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:136\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    135\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 136\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    137\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    139\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:236\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[0;32m    231\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    232\u001b[0m     Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[0;32m    235\u001b[0m ]\n\u001b[1;32m--> 236\u001b[0m result, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    237\u001b[0m     result_docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    238\u001b[0m )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_intermediate_steps:\n\u001b[0;32m    240\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m [r[question_result_key] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m map_results]\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:246\u001b[0m, in \u001b[0;36mReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m result_docs, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse(\n\u001b[0;32m    244\u001b[0m     docs, token_max\u001b[38;5;241m=\u001b[39mtoken_max, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    245\u001b[0m )\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_documents_chain\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    247\u001b[0m     docs\u001b[38;5;241m=\u001b[39mresult_docs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    248\u001b[0m )\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:244\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\llm.py:293\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    151\u001b[0m     inputs,\n\u001b[0;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    153\u001b[0m )\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\llm.py:103\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain\\chains\\llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    116\u001b[0m         prompts,\n\u001b[0;32m    117\u001b[0m         stop,\n\u001b[0;32m    118\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    124\u001b[0m     )\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py:568\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    562\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    567\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py:741\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m         )\n\u001b[0;32m    728\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    729\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    730\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m         )\n\u001b[0;32m    740\u001b[0m     ]\n\u001b[1;32m--> 741\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    742\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    743\u001b[0m     )\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py:605\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    604\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    606\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_core\\language_models\\llms.py:592\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    584\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 592\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    593\u001b[0m                 prompts,\n\u001b[0;32m    594\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    595\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    596\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    598\u001b[0m             )\n\u001b[0;32m    599\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    601\u001b[0m         )\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\langchain_community\\llms\\huggingface_pipeline.py:204\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(batch_prompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpipeline_kwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:219\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\base.py:1143\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1140\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1141\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1142\u001b[0m     )\n\u001b[1;32m-> 1143\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1067\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1068\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1069\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:295\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    296\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\generation\\utils.py:1479\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[0;32m   1463\u001b[0m         input_ids,\n\u001b[0;32m   1464\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1475\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1476\u001b[0m     )\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1478\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[0;32m   1480\u001b[0m         input_ids,\n\u001b[0;32m   1481\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1482\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1483\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1484\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1485\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1486\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1487\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1488\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1490\u001b[0m     )\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\generation\\utils.py:2340\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2337\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2340\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2342\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2343\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2344\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2345\u001b[0m )\n\u001b[0;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2348\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1201\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1199\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m   1204\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\accelerate\\hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32md:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.83 GiB is allocated by PyTorch, and 467.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# gpu显存不够，因为要调用多个模型\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "template = (\n",
    "            f\"You are an assistant for question-answering tasks.\" \n",
    "            \"Use the following pieces of retrieved context to answer the question. \"\n",
    "            \"If you don't know the answer, just say that you don't know. \"\n",
    "            \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "            \"Question: {question} \\n\"\n",
    "            \"Context: {context} \\n\"\n",
    "            \"Answer:\"\n",
    "            )\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "question_generator = LLMChain(llm=pipeline, prompt=prompt)\n",
    "doc_chain = load_qa_with_sources_chain(pipeline, chain_type=\"map_reduce\")\n",
    "qa = ConversationalRetrievalChain(\n",
    "    retriever=retriever, \n",
    "    combine_docs_chain=doc_chain, \n",
    "    question_generator=question_generator)\n",
    "chat_history = []\n",
    "query = \"But what is Barlowtwins-CXR\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\nBased Abnormality Localization with\nSelf-Supervised Learning\n\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸cois Samson3,\nDianbo Liu2,4\n\n1*D´epartement d’informatique et de recherche op´erationnelle, Universit´e\nde Montr´eal, 2920 chemin de la Tour, Montr´eal, H3T 1J4, QC, Canada.\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´eal, H2S\n3H1, QC, Canada.\n3Direction des ressources informationnelles, CIUSSS du\nCentre-Sud-de-l’ˆIle-de-Montr´eal, 400 Blvd. De Maisonneuve Ouest,\nMontr´eal, H3A 1L4, QC, Canada.\n4School of Medicine and College of Design and Engineering, National\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\nSG, Singapore.\n\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\nContributing authors: linrui.ma@umontreal.ca;\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\n\nAbstract\n\nBackground: Chest X-ray imaging based abnormality localization, essential in\ndiagnosing various diseases, faces significant clinical challenges due to complex\ninterpretations and the growing workload of radiologists. Recent advances in deep\nlearning, especially self-supervised learning, offer promising solutions to enhance\nimage analysis efficiency, accuracy and reliability.\nThis study aims to improve autonomic abnormality localization performance of\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\nsupervised learning method called BarlowTwins-CXR.\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\na two-stage training process. Initially, self-supervised pre-training was performed\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\n\n1\n\n\x0cbackbone pre-trained on ImageNet. This was followed by supervised fine-tuning\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\nevaluation.\nResults: Our experiments showed a significant improvement in model perfor-\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\nracy of chest X-ray image base abnormality localization, outperforming tradi-\ntional transfer learning methods. Its ability to adapt to various imaging conditions\nand regional variations demonstrates the potential of self-supervised learning in\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\nparticularly in high-workload environments, offering a promising direction for\nfuture AI-driven healthcare solutions.\n\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\nunder curve; mean Average Precision.\n\n1 Introduction\n\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\ntriaging patients and prioritizing the care order due to its convenience and flexibility.\nEffective mitigation addresses the lack of availability of computed tomography and\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\nplex interpretation often requires a highly qualified radiologist to make an accurate\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\ning and remote areas, qualified radiologists are insufficient to cope with the increased\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\npopulation [4]. This situation necessitates urgently developing and introducing auto-\nmated technologies like AI-based image analysis tools to aid radiologists in quicker\nand more precise CXR image analysis. It will improve the quality of diagnosis and\nhelp reduce the workload of doctors.\n\nIn recent years, deep learning models have rapidly advanced in various medi-\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\nysis because it can identify and precisely locate the types of anomalies in the images,\n\n2\n\n\x0cproviding doctors with more specific and valuable information. However, training\nthese models requires a large amount of annotated data. These annotations must be\nperformed by experienced radiologists for CXR images, as well as for most medi-\ncal images, making such annotated data not only costly, but also rare, with only a\nvery limited number of public datasets including bounding box information. Although\ntransfer learning is widely regarded as an effective method to solve the problem\nof scarce labelling data, its application in medical image analysis still faces limita-\ntions. This is mainly due to the significant difference in feature distribution between\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\ndatasets. This disparity suggests that directly applying these pre-trained weights to\nmedical image analysis might not yield the best outcomes, particularly for specialized\nmedical diagnostic applications [6][7].\n\nTo fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\nemploying a dual-phase training process to enhance CXR image analysis. The first\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\napplied to fine-tune the model. Our experiments show that such a training strategy\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\nmodel performance. We observed a 3% increase in model accuracy on the mean\nAverage Precision benchmark, surpassing the results achieved by directly performing\nconventional transfer learning from ImageNet pre-trained weights.\n\nThis study extends the application of self-supervised learning to chest X-ray abnor-\nmality localization. It demonstrates the potential of self-supervised learning in medical\nimaging analysis, especially in the absence of annotated data. By effectively improving\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\nresents a significant advancement in the field of CXR abnormality localization, paving\nthe way for more efficient and accurate diagnostic methods in the future.\n\n2 Related Work\n\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\nmodels trained with vast amounts of labelled data are capable of identifying features\nof various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\nimages [13]. This achievement is attributed to the advanced image recognition and\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\narchitecture which incorporates attention mechanisms, as proposed by Guszt´av Ga´al\net al. [14], has made significant strides in accurately segmenting lung structures, thus\naiding in detailed analysis and diagnosis of diseases.\n\n3\n\n\x0cSelf-supervised learning has recently gained popularity in the field of medical imag-\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\nuseful feature representations by training deep networks on unsupervised data [16].\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\ntionships in data by designing innovative pretext tasks, such as image reconstruction\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\net al. used large-scale images for self-supervised learning to improve accuracy and con-\nvergence speed significantly in downstream tasks, achieving better performance than\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model\nbased on Momentum Contrast to enhance the representativeness and portability of\nCXR models [21].\n\nIn terms of transfer learning, applying models trained in one domain to another has\nled to notable success in medical image analysis. Research indicates that well-processed\ntransfer results from ImageNet can improve model performance in the medical imag-\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\nthe significant difference in feature distribution between medical and natural images,\nfeatures learned from natural images may not always be broadly applicable to med-\nical images [23]. Various cross-domain adaptive transfer learning methods have been\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\ning and sequential domain adaptation techniques. By tuning model parameters, these\nmethods can be better adapted to the characteristics of medical images, improving\nthe performance and accuracy of models in medical image analysis [22].\n\n3 Methods\n\n3.1 Dataset Selection\n\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\n14 diseases with image-level annotations, including disease location annotations in\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\n\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\nfor the fine-tuning phase.\n\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\ning sets. The training set, consisting of 15,000 images, was annotated independently\nby three radiologists per image. In contrast, the test set, comprising 3,000 images,\nunderwent a more rigorous annotation process. Initially, each image was independently\nannotated by three radiologists. This is followed by a secondary review phase where\nthese initial annotations are reviewed by two other more experienced radiologists, they\ncommunicated with each other to resolve any disagreements and reach a consensus on\n\n4\n\n\x0cFig. 1 Image-level label distribution of the NIH-CXR dataset.\n\nthe final labelling. This meticulous process for the test set created a potential dispar-\nity in data distribution compared to the training set. To eliminate any bias it might\nintroduce in our study, we resplit the original training set into new training, validation,\nand test sets for our experiments.\n\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\ncalculating the weighted average of each set of duplicate bounding boxes to create a\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-\ntion redundancy and improving target area representation in the dataset. Figure 2\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\n\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\nable dataset for adult CXR object detection, but also because of the high level of\ndiversity and richness of its data.\n\n3.2 Dual-Phase Training Process\n\nOur training encompasses two primary phases: self-supervised pre-training and sub-\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\napproach refined the ImageNet pre-trained model by updating its backbone weights.\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step\naims to further improve the model’s task-specific performance, explicitly enhancing\nits capabilities in localized diseases in CXR images.\n\n5\n\nCountdisease/abnormalityNo FindingInfiltrationEffusionAtelectasisNoduleMassPneumothoraxConsolidationPleural_ThickeningCardiomegalyEmphysemaEdemaFibrosisPneumoniaHernia010000200003000040000500006000070000\x0cFig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\npreprocessing.\n\n3.2.1 Self-Supervised Pre-training\n\nFor the first stage of training, we employed the original version of the Barlow Twins\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\nconventional contrastive learning, introducing a self-supervised learning framework\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\na straightforward yet potent principle: it learns distinctive features by reducing the\nrepresentational differences between two differently distorted images from the same\nsource as processed by the network. This strategy is instrumental in enabling the model\nto identify unique and rich features in each image while concurrently minimizing the\noverlap in features. The process involves generating two distinct variants of an image\nthrough data augmentation, followed by their simultaneous processing via two deep\n\n6\n\na)b)\x0cFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with\nmulti-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\nfor object detection tasks.\n\nneural networks that share identical weights. The objective is to align the network’s\nweights to enhance the similarity in the high-level representations of these image pairs\nyet ensure that the individual features remain distinct and independent.\n\nThe Barlow Twins method might be particularly useful for medical\n\nimaging\nbecause it extracts features by minimizing the redundancy between representations of\nperturbed images. In CXR imaging, subtle differences might indicate important health\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\nimportant features. In contrast to other contrastive learning algorithms like MoCo\n[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\nTwins works at the feature dimension. It aims to assign an independent meaning to\neach feature dimension. This could lead to a richer feature representation, potentially\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\n\n7\n\nInputimageC2C3C4C5P2P3P4P5Resnet50 Backboneconvrpn_clsrpn_regRoI poolingRegion Proposal NetworkFCFCFCFeature mapFeature VectorFeature mapClassBoxFeaturePyramidNetworkXY AY BImagesDistortedimagesZ AZ BResnet 50Resnet 50NetEmbeddingsEmporocalcross-corr.Targetcross-corr.ILBTfeature dimensionBarlow twinsFaster R-CNN\x0cpathological states). Moreover, compared to self-supervised learning methods requir-\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\noffers a more straightforward training framework, which is particularly important in\nsituations with limited computational resources.\n\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\nfrom the Torchvision library, this step brings no additional cost. We used images from\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\ncapabilities over 600 epochs.\n\n3.2.2 Fine-tuning Phase\n\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with\nFeature Pyramid Network (FPN) [28] as our object detector and trained it on the\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\nR-CNN detector employs these regions to detect and classify targets. This architecture\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\nPyramid Network (FPN), an architecture frequently employed in object detection,\nparticularly enhances performance with multi-scale targets. It integrates high-level\nsemantic information from deeper layers with detailed information from shallower lay-\ners, producing feature maps of varied scales that effectively detect differently sized\ntargets.\n\nWe employed the MMdetection [29] machine learning toolbox as the platform for\nFaster R-CNN, utilizing a number of classical image augmentation techniques and\nmaintaining consistent hyperparameters across all experiments. Two different input\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\non the model’s performance with the pre-trained models. In addition, for comparison,\nwe also conducted experiments using ImageNet pre-trained weights directly.\n\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\ncomprehensively evaluate the self-supervised learning model’s performance in medical\nimaging. This method examines the model’s feature transfer capability - its ability to\nadapt learned representations to new tasks. We first resplit the test set of the NIH\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\nand the remaining 20% as an evaluation test set for assessing model performance.\n\nWe adopted two distinct strategies during the evaluation: freezing the backbone\nweights or fine-tuning the weights. In the freezing backbone strategy, we kept the\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\nthe entire network, encompassing both the self-supervised trained feature extraction\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\n\n8\n\n\x0cevaluation training set data for training the linear classifier, allowing us to assess the\nmodel’s performance across different scales of training data.\n\nWhen evaluating the representation transfer ability of a self-supervised learning\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\ntest sets is consistent. We used the Iterative stratification for the multi-label data\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\nand test sets were roughly similar. This helped prevent biases due to uneven label\ndistribution, making our evaluation results more reliable and convincing.\n\n3.3 Results Analysis Process\n\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\nof our object detection models. mAP is a widely recognized and effective metric in\nobject detection, calculated by averaging precision scores across various object detec-\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\nscores for each class. The proportion of correct predictions relative to all predictions\nfor a specific class across different detection confidence thresholds determines the pre-\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\nfor a reasonable degree of positional deviation, which is practical for actual clinical\napplications.\n\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\nprecision and recall, making it an especially appropriate performance indicator for\nthis field. The AUC metric represents the area under the Receiver Operating Char-\nacteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\nfor imbalanced datasets as it does not rely directly on classification thresholds.\n\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\nlutional layer and observes the impact on the output class scores. This process reveals\nthe most influential regions for the model’s decision-making. The resulting heat maps\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.\n\n4 Results\n\n4.1 Transfer Learning on VinDr Abnormality Localization\n\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\n\n9\n\n\x0cusing two different input resolutions. Consistent hyperparameter settings were main-\ntained across all experiments, ensuring that the performance changes were attributable\nonly to the merits of the pretraining method itself. We visualized the performance of\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\non the validation set in Figure 4, and tabulated the corresponding mAP performance\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\nperformance baseline without pre-training benefits.\n\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\nbetween the lowest and highest value.\n\nTable 1 mAP50 scores in validation and test sets for models with varying pre-training\nmethods at different input resolutions.\n\nBackBone weight\n\nInput size mAP50 (val set)\n\nmAP50 (test set)\n\nbaseline nopretrained\nImageNet pretrained\nBarlow twins\nBarlow twins from ImageNet\n\n224\n\n0.1388 (0.1352,0.1424)\n0.2245 (0.2204,0.2286)\n0.2555 (0.2485,0.2626)\n0.2625 (0.2568,0.2682)\n\n0.1342 (0.1306,0.1378)\n0.2210 (0.2194,0.2226)\n0.2448 (0.2414,0.2482)\n0.2502 (0.2476,0.2528)\n\nImageNet pretrained\nBarlow twins from ImageNet\n\n640\n\n0.2973 (0.2913,0.3033)\n0.3102 (0.3080,0.3125)\n\n0.280 (0.2757,0.2848)\n0.289 (0.2826,0.2954)\n\n1Scores are presented with 95% confidence intervals.\n\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\npre-training in feature representation across disparate image domains.\n\n10\n\na)b)\x0cMore strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\nexpedited training trajectory and a significant increase in detection performance.\n\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\n\nThe heat maps generated from the study present a compelling visualization of the\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\nweights approach. We generated heat maps of the first few CXR images of the train-\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\nThis indicates a higher precision in localizing and identifying pathological features\nwith BarlowTwins-CXR, potentially offering more targeted information for clinical\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\nderived from ImageNet weights, further affirming the efficacy of our approach in\nenhancing CXR image analysis.\n\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\nlow Twin-CXR weighted models saw performance improvements due to the increased\ndetail in the CXR images. Nonetheless, the performance differential between the\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\nadvantages of self-supervised pre-training.\n\nThis points to intriguing future research avenues, such as refining image resolu-\ntion parameters during pre-training and fine-tuning phases and investigating whether\nhigher-resolution pre-training could elevate model performance. It also accentuates the\nnecessity of tailoring deep learning model design to specific tasks, considering factors\nlike image resolution and feature granularity.\n\nOverall,\n\nimplementing the Barlow Twins-CXR method on the VinDr dataset\nresulted in substantial gains despite its data limitations and the inherent challenges\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\nimage weights for better applicability in CXR image analysis and beyond in medical\nimaging.\n\n4.2 Linear Evaluation Protocol\n\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\ntraditional ImageNet pre-training on the linear classification performance within the\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone\nof the linear classifier and updating only the final linear layer’s weights. This approach\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\nthese experiments are presented in Figure 6 and Table 2.\n\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\n\n11\n\n\x0cFig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\nbelow the heatmaps refer to the image numbers in the dataset.\n\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\nmethods at 224 and 640 input resolutions.\n\nModel\n\n1%\n\n10%\n\n100%\n\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616)\n0.5932 (0.5913, 0.5951)\nImage-Net\n\n0.7773 (0.7756, 0.7790)\n0.6855 (0.6822, 0.6889)\n\n0.8031 (0.8027, 0.8035)\n0.7098 (0.7089, 0.7107)\n\n1Scores are presented with 95% confidence intervals.\n\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\n\nNotably, the incremental gains for both pre-training methods diminished with\nlarger data sizes, suggesting that the performance boost provided by additional data\nbecomes marginal when only the linear layer is updated.\n\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\nimage analysis, particularly when annotated data is scarce.\n\n12\n\nGround Truth BboxImageNet WeightsOur method20e27597c972c6e7fdb4d1e7638e227e03431b577d1ccf075e930c4c4913c079fd810298e165ef0b9a88bb25fda7a34bGround Truth BboxImageNet WeightsOur method9eba0d101f410f9cdfae46cb094ae2a687a8df2f22475c7200ebe891d0f25b88ad86f42123384e2441cce36347aa7d1aa)b)\x0cFig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\nexperiments.\n\n4.3 End-to-End Finetuning\n\nIn our end-to-end experiments, where we permitted updates to all model layers, the\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\nImageNet pre-trained equivalent across all training set sizes. The results of these\nexperiments are presented in Figure 7 and Table 3.\n\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various\ndataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\nacross five experiments.\n\n13\n\n\x0cTable 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\npre-training methods at 224 and 640 input resolutions.\n\nModel\n\n1%\n\n10%\n\n100%\n\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627)\n0.6163 (0.6110, 0.6216)\nImage-Net\n\n0.7756 (0.7745, 0.7768)\n0.7168 (0.7093, 0.7243)\n\n0.8107 (0.8098, 0.8116)\n0.7866 (0.7843, 0.7889)\n\n1Scores are presented with 95% confidence intervals.\n\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\n\nAUC than the ImageNet counterpart.\n\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\nover the frozen backbone setup was less marked, suggesting that the wealth of features\nlearned during self-supervised training reduces the margin for additional gains during\nsubsequent fine-tuning.\n\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning\nacross all model layers may elevate the risk of overfitting, particularly when data is\nscarce. The narrowing performance differential between the two pre-training strategies\nwith increasing data volume indicates that the distinction between domain-specific\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\ntial with larger datasets. This trend implies that the influence of the pre-training\nstrategy on the final performance of models may diminish as the size of the medical\nimage dataset grows.\n\n5 Discussion\n\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\nunannotated CXR images for learning valuable representations and enhances trans-\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\nThis leads to quicker training and improved performance on tasks like abnormality\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input\nresolutions, outshining models pre-trained on ImageNet.\n\nOne of the primary limitations of our study is the scarcity of CXR datasets with\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\nmay limit the generalizability of our findings. Additionally, the computational cost\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\nimages with an image size of 224*224 pixels, the training process required two days\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\nability to experiment with higher image resolutions, which could potentially enhance\nthe model’s performance.\n\n6 Future Work\n\nOur future endeavours include developing a demo interactive system for deployment\nand testing in emergency rooms. It will allow practical evaluation of the model’s\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\n\n14\n\n\x0cAdditionally, we plan to explore more advanced self-supervised learning methods,\nobject detection frameworks, and backbone networks to refine our approach further.\nThe continuous evolution of these technologies promises to address some of the current\nlimitations and expand the applicability and accuracy of our model in medical image\nanalysis.\n\n7 Conclusions\n\nThe results of this study provide strong support for the application of self-supervised\nlearning in the field of abnormality detection, especially valuable in environments\nwhere radiologists face high workloads but the corresponding data labelling resources\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\nCXR image, attributable to differences in imaging equipment, patient demographics,\nand other locale-specific factors [34][35]. Such variations often impede the cross-\nregional applicability of a model, thus limiting its generalizability. By employing the\nBarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\nnetworks to local datasets tailored to the unique characteristics of their regional data.\nOur findings might also have significant implications for clinical practice, suggest-\ning that this strategy could be a game-changer in aiding radiologists to interpret\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\ntially increasing patients’ throughput and improving the overall quality of care.\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\nholds particular promise in areas where standardization of medical imaging presents\nchallenges.\n\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\nprofessionals and patients.\n\n8 Abbreviations\n\nAP: anterior-posterior\nAUC: area under the receiver operating characteristic curve\nCAM: Class Activation Mapping\nCIUSSS: Centre int´egr´e universitaire de sant´e et de services sociaux\nCXR: chest X-ray radiography\nFC: Fully connected layer\nFPN: Feature Pyramid Network\nFPR: False Positive Rate\nIoU: Intersection over Union\nROC: receiver operating characteristic\nROI: region of interest\nmAP: mean Average Precision\nPA: posterior-anterior\nTPR: True Positive Rate\n\n15\n\n\x0cWBF: Weighted Box Fusion\nYOLO: You Only Look Once\n\n9 Declarations\n\n9.1 Ethics approval and consent to participate\n\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\nrations of Helsinki). The studies reported in this manuscript used reputable public\ndatasets and did not require any additional data involving human participants, human\ndata, or human tissue.\n\n9.2 Consent for publication\n\nNot applicable\n\n9.3 Availability of data and materials\n\nThe datasets generated and/or analysed during the current study are available in the\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\n\n9.4 Competing interests\n\nThe authors declare that they have no competing interests\n\n9.5 Funding\n\nNo external funding was associated with this research study.\n\n9.6 Authors’ contributions\n\nHS designed the research methodology, analyzed data, was responsible for experiments\nand results visualization, and participated in manuscript drafting and revision. LM\nassisted in developing the research methodology and contributed to the drafting and\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\nin statistical analysis. DL contributed to the study design, offered statistical analysis\nexpertise, assisted in interpreting results, and played a significant role in the critical\nrevision of the manuscript.\n\nAll authors read and approved the final manuscript.\n\n9.7 Acknowledgements\n\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆıle-de-\nmontr´eal for the computational resources and support provided, which were essential\nfor the research conducted as part of the graduate internship program. We are espe-\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\nTheir expertise and insights have been greatly appreciated and substantially enhanced\nthis work’s quality.\n\n16\n\n\x0cReferences\n\n[1] Satia, I., Bashagha, S., Bibi, A., et al.: Assessing the accuracy and certainty in\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\n(2013). PMID: 23908502\n\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al.: The role of chest imaging\nin patient management during the covid-19 pandemic: a multinational consensus\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\n32275978\n\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\nTrend in radiologist workload compared to number of admissions in the emer-\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\n35149337\n\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\n\ndate-of-access (2022)\n\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al.: Effect of a comprehensive\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:\na retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\n506 (2021). PMID: 34219054\n\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\ning research on medical image analysis using imagenet. Computers in Biology\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\n104115\n\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al.: Transfer learning for medical\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\nhttps://doi.org/10.1186/s12880-022-00793-7\n\n[8] Zbontar, J., Jing, L., Misra, I., et al.: Barlow twins: Self-supervised learning\nvia redundancy reduction. In: Proceedings of the International Conference on\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\narXiv.2103.03230\n\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on\nComputer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\n\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al.: Vindr-cxr: An open dataset of chest\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\n1038/s41597-022-01498-w\n\n17\n\n\x0c[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\n10.1109/CVPR.2016.90\n\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\n(2017). https://doi.org/10.48550/arXiv.1711.05225\n\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\n\n[14] Ga´al, G., Maga, B., Luk´acs, A.: Attention u-net based adversarial architectures\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\n//doi.org/10.48550/arXiv.2003.10304\n\n[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\n//doi.org/10.7717/peerj-cs.1045\n\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al.: Greedy layer-wise training of deep\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\nhttps://doi.org/10.5555/2976456.2976476\n\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al.: Context encoders: Feature\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\nhttps://doi.org/10.1109/CVPR.2016.278\n\n[18] Chen, T., Kornblith, S., Norouzi, M., et al.: A simple framework for contrastive\nlearning of visual representations. In: Proceedings of the International Conference\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.\n3525087\n\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\norg/10.48550/arXiv.1803.07728\n\n[20] Azizi, S., Mustafa, B., Ryan, F., et al.: Big self-supervised models advance medical\nimage classification. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\norg/10.1109/ICCV48922.2021.00346\n\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\n\n18\n\n\x0crepresentation and transferability of chest x-ray models. In: Medical Imaging with\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\n\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\ning research on medical image analysis using imagenet. Computers in Biology\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\n104115\n\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\ntransfer learning work for medical images: Feature reuse & other factors. In:\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\norg/10.1109/CVPR52688.2022.00901\n\n[24] Wang, X., Peng, Y., Lu, L., et al.: Chestx-ray8: Hospital-scale chest x-ray\ndatabase and benchmarks on weakly-supervised classification and localization\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\n\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\nfrom different object detection models. Image and Vision Computing 107, 104117\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\n\n[26] He, K., Fan, H., Wu, Y., et al.: Momentum contrast for unsupervised visual\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\nhttps://doi.org/10.1109/CVPR42600.2020.00975\n\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\n1109/ICCV.2015.169\n\n[28] Lin, T. Y., Doll´ar, P., Girshick, R., et al.: Feature pyramid networks for object\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\n//doi.org/10.1109/CVPR.2017.106\n\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\n//doi.org/10.48550/arXiv.1906.07155\n\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\nmizing mutual information across views. In: Proceedings of the 33rd International\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\n\n19\n\n\x0c[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\norg/10.1109/CVPR.2019.00277\n\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\nBerlin (2011)\n\n[33] Szyma´nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\nlabel data. Proceedings of the First International Workshop on Learning with\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\narXiv.1704.08756\n\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\nPMID: 10695979\n\n[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\n\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\n\n20\n\n\x0c', metadata={'source': '../documents/barlowtwins-CXR.pdf'})]
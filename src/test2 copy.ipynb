{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\machineLearning\\miniconda\\envs\\env_rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import fitz\n",
    "import torch\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from langchain import hub\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "#from langchain_community.llms import HuggingFacePipeline\n",
    "#from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "#TEST\n",
    "from FakeLLM import FakePromptCopyLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"../documents/barlowtwins-CXR.pdf\")\n",
    "documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BarlowTwins-CXR: Enhancing Chest X-Ray\\nBased Abnormality Localization with\\nSelf-Supervised Learning\\nHaoyue Sheng1,2,3*, Linrui Ma1,2, Jean-Fran¸ cois Samson3,\\nDianbo Liu2,4\\n1*D´ epartement d’informatique et de recherche op´ erationnelle, Universit´ e\\nde Montr´ eal, 2920 chemin de la Tour, Montr´ eal, H3T 1J4, QC, Canada.\\n2Mila - Quebec AI Institute, 6666 Rue Saint-Urbain, Montr´ eal, H2S\\n3H1, QC, Canada.\\n3Direction des ressources informationnelles, CIUSSS du\\nCentre-Sud-de-l’ ˆIle-de-Montr´ eal, 400 Blvd. De Maisonneuve Ouest,\\nMontr´ eal, H3A 1L4, QC, Canada.\\n4School of Medicine and College of Design and Engineering, National\\nUniversity of Singapore, 21 Lower Kent Ridge Rd, Singapore, 119077,\\nSG, Singapore.\\n*Corresponding author(s). E-mail(s): haoyue.sheng@umontreal.ca;\\nContributing authors: linrui.ma@umontreal.ca;\\njean-francois.samson.ccsmtl@ssss.gouv.qc.ca; dianbo@nus.edu.sg;\\nAbstract\\nBackground: Chest X-ray imaging based abnormality localization, essential in\\ndiagnosing various diseases, faces significant clinical challenges due to complex\\ninterpretations and the growing workload of radiologists. Recent advances in deep\\nlearning, especially self-supervised learning, offer promising solutions to enhance\\nimage analysis efficiency, accuracy and reliability.\\nThis study aims to improve autonomic abnormality localization performance of\\nchest X-ray image analysis, particularly in detecting abnormalities, using a self-\\nsupervised learning method called BarlowTwins-CXR.\\nMethods: We utilized two publicly available datasets: the NIH Chest X-ray\\nDataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in\\na two-stage training process. Initially, self-supervised pre-training was performed\\nusing an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50\\n1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0}),\n",
       " Document(page_content='backbone pre-trained on ImageNet. This was followed by supervised fine-tuning\\non the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network\\n(FPN). The study employed mean Average Precision (mAP) at an Intersection\\nover Union (IoU) of 50% and Area Under the Curve (AUC) for performance\\nevaluation.\\nResults: Our experiments showed a significant improvement in model perfor-\\nmance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50\\naccuracy compared to traditional ImageNet pre-trained models. In addition, the\\nAblation CAM method revealed enhanced precision in localizing chest abnormal-\\nities. The study involved 112,120 images from the NIH dataset and 18,000 images\\nfrom the VinDr-CXR dataset, indicating robust training and testing samples.\\nConclusion: BarlowTwins-CXR significantly enhances the efficiency and accu-\\nracy of chest X-ray image base abnormality localization, outperforming tradi-\\ntional transfer learning methods. Its ability to adapt to various imaging conditions\\nand regional variations demonstrates the potential of self-supervised learning in\\nmedical diagnostics. This approach can be instrumental in aiding radiologists,\\nparticularly in high-workload environments, offering a promising direction for\\nfuture AI-driven healthcare solutions.\\nKeywords: medical image analysis; chest x-ray; abnormality localization; deep\\nlearning; object detection; self-supervised learning; transfer learning; heat map; area\\nunder curve; mean Average Precision.\\n1 Introduction\\nChest X-ray(CXR) is a fundamental and widespread medical diagnostic tool for diag-\\nnosing chest diseases. It is efficient and cost-effective, suitable for preliminary screening\\nand diagnosis [1]. During the 2019 coronavirus pandemic, CXR was widely used for\\ntriaging patients and prioritizing the care order due to its convenience and flexibility.\\nEffective mitigation addresses the lack of availability of computed tomography and\\nreduces the risk of transmission in the room with the CT scanner [2]. However, its com-\\nplex interpretation often requires a highly qualified radiologist to make an accurate\\ndiagnosis [1]. As the demand for healthcare increases, the workload of radiologists has\\nsignificantly increased [3]. It results in less time to analyze each radiographic image,\\npotentially increasing the risk of diagnostic error. In many areas, especially in develop-\\ning and remote areas, qualified radiologists are insufficient to cope with the increased\\ndemand for healthcare. For instance, Europe has 13 radiologists per 100,000 people,\\nwhile the United Kingdom has 8.5, and Malaysia has approximately 30 per million\\npopulation [4]. This situation necessitates urgently developing and introducing auto-\\nmated technologies like AI-based image analysis tools to aid radiologists in quicker\\nand more precise CXR image analysis. It will improve the quality of diagnosis and\\nhelp reduce the workload of doctors.\\nIn recent years, deep learning models have rapidly advanced in various medi-\\ncal image analysis fields of CXR, demonstrating diagnostic accuracy comparable to\\nhuman experts [5]. Object detection plays a more critical role in medical image anal-\\nysis because it can identify and precisely locate the types of anomalies in the images,\\n2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1}),\n",
       " Document(page_content='providing doctors with more specific and valuable information. However, training\\nthese models requires a large amount of annotated data. These annotations must be\\nperformed by experienced radiologists for CXR images, as well as for most medi-\\ncal images, making such annotated data not only costly, but also rare, with only a\\nvery limited number of public datasets including bounding box information. Although\\ntransfer learning is widely regarded as an effective method to solve the problem\\nof scarce labelling data, its application in medical image analysis still faces limita-\\ntions. This is mainly due to the significant difference in feature distribution between\\nlarge datasets (such as ImageNet) used for pre-training models and medical imaging\\ndatasets. This disparity suggests that directly applying these pre-trained weights to\\nmedical image analysis might not yield the best outcomes, particularly for specialized\\nmedical diagnostic applications [6][7].\\nTo fill these gaps, our study proposed a novel method, namely BarlowTwins-CXR,\\nemploying a dual-phase training process to enhance CXR image analysis. The first\\nphase involves unsupervised pre-training using a Barlow Twins algorithm [8] on CXR\\nimages without annotation, starting with an ImageNet [9] pre-trained model as the\\nfoundation. In the second phase, transfer learning on the VinDr-CXR [10] dataset is\\napplied to fine-tune the model. Our experiments show that such a training strategy\\ncombining self-supervised pre-training and supervised fine-tuning is particularly effec-\\ntive. In our experiments, while employing ResNet50 [11] as the backbone architecture,\\nwe observed that implementing the BarlowTwins-CXR strategy significantly improved\\nmodel performance. We observed a 3% increase in model accuracy on the mean\\nAverage Precision benchmark, surpassing the results achieved by directly performing\\nconventional transfer learning from ImageNet pre-trained weights.\\nThis study extends the application of self-supervised learning to chest X-ray abnor-\\nmality localization. It demonstrates the potential of self-supervised learning in medical\\nimaging analysis, especially in the absence of annotated data. By effectively improving\\ndetection performance and precisely localizing abnormalities, BarlowTwins-CXR rep-\\nresents a significant advancement in the field of CXR abnormality localization, paving\\nthe way for more efficient and accurate diagnostic methods in the future.\\n2 Related Work\\nIn recent years, deep learning techniques have excelled in the field of medical imaging,\\nparticularly in analyzing CXR images. For example, in terms of disease classifica-\\ntion, ChexNet proposed by Pranav Rajpurkar et al. [12] outperformed radiologists\\nin detecting chest diseases, when benchmarked against the F1 score. Neural network\\nmodels trained with vast amounts of labelled data are capable of identifying features\\nof various pulmonary diseases. In anomaly detection tasks, Sun K X et al. used the\\nYOLOv7 object detection framework to effectively identify and locate lesions in CXR\\nimages [13]. This achievement is attributed to the advanced image recognition and\\nfeature extraction capabilities of neural networks. Additionally, the modified U-net\\narchitecture which incorporates attention mechanisms, as proposed by Guszt´ av Ga´ al\\net al. [14], has made significant strides in accurately segmenting lung structures, thus\\naiding in detailed analysis and diagnosis of diseases.\\n3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2}),\n",
       " Document(page_content='Self-supervised learning has recently gained popularity in the field of medical imag-\\ning [15] and provides an efficient method for utilizing unlabeled data. Initially proposed\\nby Bengio et al., this approach allows models to learn from unlabeled data and extract\\nuseful feature representations by training deep networks on unsupervised data [16].\\nSuch learning strategy promotes models to capture the intrinsic structure and rela-\\ntionships in data by designing innovative pretext tasks, such as image reconstruction\\n(e.g., Context encoder [17]), contrastive learning (e.g., SimCLR [18]), or prediction\\ntasks (e.g., rotation prediction [19]). In the field of medical imaging, Shekoofeh Azizi\\net al. used large-scale images for self-supervised learning to improve accuracy and con-\\nvergence speed significantly in downstream tasks, achieving better performance than\\nmodels pre-trained on ImageNet [20]. Sowrirajan H et al. proposed a pre-trained model\\nbased on Momentum Contrast to enhance the representativeness and portability of\\nCXR models [21].\\nIn terms of transfer learning, applying models trained in one domain to another has\\nled to notable success in medical image analysis. Research indicates that well-processed\\ntransfer results from ImageNet can improve model performance in the medical imag-\\ning domain [22]. However, studies by Christos Matsoukas et al. have shown that due to\\nthe significant difference in feature distribution between medical and natural images,\\nfeatures learned from natural images may not always be broadly applicable to med-\\nical images [23]. Various cross-domain adaptive transfer learning methods have been\\ndeveloped to address these challenges, such as unsupervised and semi-supervised learn-\\ning and sequential domain adaptation techniques. By tuning model parameters, these\\nmethods can be better adapted to the characteristics of medical images, improving\\nthe performance and accuracy of models in medical image analysis [22].\\n3 Methods\\n3.1 Dataset Selection\\nThis study utilized two publicly available chest X-ray datasets: the NIH-CXR[24]\\ndataset and the VinDr-CXR dataset. The NIH dataset comprises 112,120 posterior-\\nanterior (PA) or anterior-posterior (AP) CXR images from 30,805 patients, covering\\n14 diseases with image-level annotations, including disease location annotations in\\nsome images. The distribution of the NIH-CXR dataset is illustrated in Figure 1.\\nMeanwhile, the VinDr-CXR dataset is the largest publicly available dataset for\\nadult CXR object detection, which includes 18,000 PA CXR scans. These scans encom-\\npass 14 diseases with detailed instance-level bounding box annotations, making it ideal\\nfor the fine-tuning phase.\\nThe VinDr-CXR dataset exhibits a distinct labelling process for its test and train-\\ning sets. The training set, consisting of 15,000 images, was annotated independently\\nby three radiologists per image. In contrast, the test set, comprising 3,000 images,\\nunderwent a more rigorous annotation process. Initially, each image was independently\\nannotated by three radiologists. This is followed by a secondary review phase where\\nthese initial annotations are reviewed by two other more experienced radiologists, they\\ncommunicated with each other to resolve any disagreements and reach a consensus on\\n4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3}),\n",
       " Document(page_content='Countdisease/abnormalityNo Finding\\nInfiltration\\nEffusion\\nAtelectasis\\nNodule\\nMass\\nPneumothorax\\nConsolidation\\nPleural_Thickening\\nCardiomegaly\\nEmphysema\\nEdema\\nFibrosis\\nPneumonia\\nHernia\\n0 10000 20000 30000 40000 50000 60000 70000Fig. 1 Image-level label distribution of the NIH-CXR dataset.\\nthe final labelling. This meticulous process for the test set created a potential dispar-\\nity in data distribution compared to the training set. To eliminate any bias it might\\nintroduce in our study, we resplit the original training set into new training, validation,\\nand test sets for our experiments.\\nTo improve the quality of the training data, a Weighted Box Fusion (WBF) [25] pre-\\nprocessing technique was applied to the VinDr-CXR training set. The WBF involves\\ncalculating the weighted average of each set of duplicate bounding boxes to create a\\nsingle fused bounding box. Such a preprocessing step is crucial for reducing annota-\\ntion redundancy and improving target area representation in the dataset. Figure 2\\nshows the data distribution of VinDr-CXR before and after WBF preprocessing.\\nWe chose the VinDr-CXR dataset not only because it is the largest publicly avail-\\nable dataset for adult CXR object detection, but also because of the high level of\\ndiversity and richness of its data.\\n3.2 Dual-Phase Training Process\\nOur training encompasses two primary phases: self-supervised pre-training and sub-\\nsequent supervised fine-tuning. Initially, we commenced with a Resnet50 model\\npre-trained on ImageNet. As shown in Figure 3: In the self-supervised pre-training\\nphase, we applied a modified Barlow Twins method to the NIH-CXR Dataset. This\\napproach refined the ImageNet pre-trained model by updating its backbone weights.\\nSubsequently, in the supervised fine-tuning phase, we utilize this refined backbone\\nwithin a Faster R-CNN framework by applying it to the VinDr-CXR dataset. This step\\naims to further improve the model’s task-specific performance, explicitly enhancing\\nits capabilities in localized diseases in CXR images.\\n5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4}),\n",
       " Document(page_content='a)\\nb)Fig. 2 Instance-level annotation distribution of VinDr-CXR dataset before(a) and after(b) WBF\\npreprocessing.\\n3.2.1 Self-Supervised Pre-training\\nFor the first stage of training, we employed the original version of the Barlow Twins\\nmethod, as mentioned in Zbontar et al. [8] This approach represents a shift from\\nconventional contrastive learning, introducing a self-supervised learning framework\\nprimarily focused on diminishing redundancy. The Barlow Twins method operates on\\na straightforward yet potent principle: it learns distinctive features by reducing the\\nrepresentational differences between two differently distorted images from the same\\nsource as processed by the network. This strategy is instrumental in enabling the model\\nto identify unique and rich features in each image while concurrently minimizing the\\noverlap in features. The process involves generating two distinct variants of an image\\nthrough data augmentation, followed by their simultaneous processing via two deep\\n6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 5}),\n",
       " Document(page_content='Input\\nimageC2 C3 C4 C5\\nP2 P3 P4 P5Resnet50 Backbone\\nconvrpn_cls\\nrpn_reg\\nRoI poolingRegion Proposal Network\\nFC FC FCFeature map\\nFeature VectorFeature mapClass\\nBoxFeature\\nPyramid\\nNetworkX\\nY AY B\\nImagesDistorted\\nimages\\nZ A\\nZ BResnet 50\\nResnet 50Net Embeddings\\nEmporocal\\ncross-corr .Target\\ncross-corr .\\nI\\nLBT\\nfeature dimensionBarlow twins\\nFaster  R-CNNFig. 3 Schematic Overview of the Dual-phase Training Framework. The upper panel illustrates\\nthe Barlow Twins method in Phase One, where pairs of distorted images are processed through\\na shared ResNet50 network to produce embeddings. These are then compared using an empirical\\ncross-correlation matrix C, striving for the identity matrix I to minimize redundancy in feature\\ndimensions, and optimizing the loss function L BT. In Phase Two (lower panel), the pre-trained\\nResNet50 backbone from Phase One is integrated into a Faster R-CNN architecture. It starts with\\nmulti-scale feature extraction through the Feature Pyramid Network (FPN), followed by the Region\\nProposal Network (RPN) that generates object region proposals. The features are then pooled and\\nprocessed by fully connected (FC) layers to output the final class labels and bounding box coordinates\\nfor object detection tasks.\\nneural networks that share identical weights. The objective is to align the network’s\\nweights to enhance the similarity in the high-level representations of these image pairs\\nyet ensure that the individual features remain distinct and independent.\\nThe Barlow Twins method might be particularly useful for medical imaging\\nbecause it extracts features by minimizing the redundancy between representations of\\nperturbed images. In CXR imaging, subtle differences might indicate important health\\ninformation, and the Barlow Twins can effectively capture these subtle but clinically\\nimportant features. In contrast to other contrastive learning algorithms like MoCo\\n[26] and SimCLR, which construct similarity matrices at the batch dimension, Barlow\\nTwins works at the feature dimension. It aims to assign an independent meaning to\\neach feature dimension. This could lead to a richer feature representation, potentially\\nbetter adapted to variations in CXR images (e.g., different imaging conditions and\\n7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6}),\n",
       " Document(page_content='pathological states). Moreover, compared to self-supervised learning methods requir-\\ning negative samples or complex contrastive mechanisms like SimCLR, Barlow Twins\\noffers a more straightforward training framework, which is particularly important in\\nsituations with limited computational resources.\\nWe chose to apply Barlow Twins pre-training on the ImageNet pretrained\\nResNet50model. Since the ImageNet pre-trained model weights can be easily obtained\\nfrom the Torchvision library, this step brings no additional cost. We used images from\\nthe training set portion of the NIH-CXR dataset for this training phase, with the input\\nimage size set to 224*224 pixels. The training was executed on an NVIDIA A100 80G\\nGPU, setting the batch size to 768 to maximize the utilization of this graphics card’s\\ncapabilities over 600 epochs.\\n3.2.2 Fine-tuning Phase\\nIn our fine-tuning/transfer learning stage, we utilized the Faster R-CNN [27] with\\nFeature Pyramid Network (FPN) [28] as our object detector and trained it on the\\nVinDr-CXR dataset. Faster R-CNN, a widely-used object detection framework, com-\\nprises two main components: the Region Proposal Network (RPN) [28] and the Fast\\nR-CNN detector. First, RPN generates candidate regions for objects, and then the Fast\\nR-CNN detector employs these regions to detect and classify targets. This architecture\\nrenders Faster R-CNN particularly efficient in processing complex images. The Feature\\nPyramid Network (FPN), an architecture frequently employed in object detection,\\nparticularly enhances performance with multi-scale targets. It integrates high-level\\nsemantic information from deeper layers with detailed information from shallower lay-\\ners, producing feature maps of varied scales that effectively detect differently sized\\ntargets.\\nWe employed the MMdetection [29] machine learning toolbox as the platform for\\nFaster R-CNN, utilizing a number of classical image augmentation techniques and\\nmaintaining consistent hyperparameters across all experiments. Two different input\\nsizes, 224*224 pixels and 640*640 pixels, were chosen to assess the impact of image size\\non the model’s performance with the pre-trained models. In addition, for comparison,\\nwe also conducted experiments using ImageNet pre-trained weights directly.\\nWe implemented a linear evaluation protocol [30][31] on the NIH-CXR dataset to\\ncomprehensively evaluate the self-supervised learning model’s performance in medical\\nimaging. This method examines the model’s feature transfer capability - its ability to\\nadapt learned representations to new tasks. We first resplit the test set of the NIH\\ndataset into two parts: 80% as an evaluation training set for training a linear classifier\\nand the remaining 20% as an evaluation test set for assessing model performance.\\nWe adopted two distinct strategies during the evaluation: freezing the backbone\\nweights or fine-tuning the weights. In the freezing backbone strategy, we kept the\\nparameters of the backbone network (i.e., the feature extraction layers) obtained from\\nself-supervised pretraining unchanged. We updated only the weights of the final lin-\\near layer. Conversely, under the fine-tuning strategy, we updated parameters across\\nthe entire network, encompassing both the self-supervised trained feature extraction\\nlayers and the newly added linear classifier layer. We used 100%, 10%, and 1% of the\\n8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier, allowing us to assess the\\nmodel’s performance across different scales of training data.\\nWhen evaluating the representation transfer ability of a self-supervised learning\\nmodel, it is necessary to ensure that the ratio of individual labels in the training and\\ntest sets is consistent. We used the Iterative stratification for the multi-label data\\nmethod [32][33] to ensure that the proportions of each label in the evaluation training\\nand test sets were roughly similar. This helped prevent biases due to uneven label\\ndistribution, making our evaluation results more reliable and convincing.\\n3.3 Results Analysis Process\\nFor the analysis of results, we employed the mean Average Precision (mAP) at an\\nIntersection over Union (IoU) of 50% as the benchmark for evaluating the performance\\nof our object detection models. mAP is a widely recognized and effective metric in\\nobject detection, calculated by averaging precision scores across various object detec-\\ntion confidence thresholds. Specifically, mAP is the mean of the average precision\\nscores for each class. The proportion of correct predictions relative to all predictions\\nfor a specific class across different detection confidence thresholds determines the pre-\\ncision score. In the context of CXR abnormality localization, utilizing mAP at an IoU\\nof 50% is beneficial for capturing clinically significant lesion detections while allowing\\nfor a reasonable degree of positional deviation, which is practical for actual clinical\\napplications.\\nMoreover, we utilized the Area Under the Curve (AUC) as a metric for the lin-\\near evaluation protocol. AUC, a standard metric in medical image analysis, balances\\nprecision and recall, making it an especially appropriate performance indicator for\\nthis field. The AUC metric represents the area under the Receiver Operating Char-\\nacteristic (ROC) curve, accounting for the model’s True Positive Rate (TPR) and\\nFalse Positive Rate (FPR) at various thresholds. This assessment method balances\\nthe model’s sensitivity and specificity, enhancing detection rates while controlling false\\npositives. Medical image analysis often deals with imbalanced data, and AUC is robust\\nfor imbalanced datasets as it does not rely directly on classification thresholds.\\nBeyond using mAP and AUC for quantitative analysis, our study also utilized the\\nAblation CAM (Class Activation Mapping) method to create heat maps for qualitative\\nevaluation. Ablation CAM systematically abates features in the model’s final convo-\\nlutional layer and observes the impact on the output class scores. This process reveals\\nthe most influential regions for the model’s decision-making. The resulting heat maps\\ndelineate areas of interest in CXR images, providing intuitive visual evidence of how\\nour BarlowTwins-CXR model focuses on and recognizes abnormalities.\\n4 Results\\n4.1 Transfer Learning on VinDr Abnormality Localization\\nIn this experiment, we examined the efficacy of the ResNet backbone pre-trained by the\\nBarlow Twins-CXR method for abnormality localization on the VinDr-CXR dataset,\\n9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8}),\n",
       " Document(page_content='using two different input resolutions. Consistent hyperparameter settings were main-\\ntained across all experiments, ensuring that the performance changes were attributable\\nonly to the merits of the pretraining method itself. We visualized the performance of\\ndifferent models such as Barlow twins-CXR pre-training and ImageNet pre-training\\non the validation set in Figure 4, and tabulated the corresponding mAP performance\\nin Table 1. As depicted in the figure, the baseline model with an untrained ResNet50\\nbackbone reached a final mAP50 score of 0.1342 (95% CI 0.1306,0.1378), setting a\\nperformance baseline without pre-training benefits.\\na) b)\\nFig. 4 Evolution of mAP50 across epochs for different ResNet50 backbones on the VinDr-CXR\\ndataset at 224*224(left) and 640*640(right) resolution. The darker lines represent the average mAP50\\nof four(left) and five(right) trials with different random seeds, with shaded areas indicating the range\\nbetween the lowest and highest value.\\nTable 1 mAP50 scores in validation and test sets for models with varying pre-training\\nmethods at different input resolutions.\\nBackBone weight Input size mAP50 (val set) mAP50 (test set)\\nbaseline nopretrained 224 0.1388 (0.1352,0.1424) 0.1342 (0.1306,0.1378)\\nImageNet pretrained 0.2245 (0.2204,0.2286) 0.2210 (0.2194,0.2226)\\nBarlow twins 0.2555 (0.2485,0.2626) 0.2448 (0.2414,0.2482)\\nBarlow twins from ImageNet 0.2625 (0.2568,0.2682) 0.2502 (0.2476,0.2528)\\nImageNet pretrained 640 0.2973 (0.2913,0.3033) 0.280 (0.2757,0.2848)\\nBarlow twins from ImageNet 0.3102 (0.3080,0.3125) 0.289 (0.2826,0.2954)\\n1Scores are presented with 95% confidence intervals.\\nA significant advancement was observed with the ImageNet pre-trained ResNet50,\\nwhich attained a mAP50 of 0.2210 (95% CI 0.2194,0.2226), underscoring the value of\\npre-training in feature representation across disparate image domains.\\n10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9}),\n",
       " Document(page_content='More strikingly, incorporating the Barlow Twins-CXR strategy led to a rapid per-\\nformance ascent, achieving a mAP50 of 0.2448 (95% CI 0.2414 0.2482). It marked an\\nexpedited training trajectory and a significant increase in detection performance.\\nWhen further enhanced by pre-training from ImageNet, the Barlow Twins-CXR\\napproach yielded the best performance, recording a mAP of 0.2502 (95% CI 0.2476\\n0.2528), evidencing the synergetic effect of combining pre-training methodologies.\\nThe heat maps generated from the study present a compelling visualization of the\\nperformance of the BarlowTwins-CXR method compared to the traditional ImageNet\\nweights approach. We generated heat maps of the first few CXR images of the train-\\ning and test sets in Figure 5. In each image, our method’s heat maps show a more\\nfocused alignment with the actual lesion areas marked by the Ground Truth Bbox.\\nThis indicates a higher precision in localizing and identifying pathological features\\nwith BarlowTwins-CXR, potentially offering more targeted information for clinical\\ndiagnoses. Notably, in cases of cardiomegaly and lung opacity, the concentration and\\nlocalization of the heatmaps from BarlowTwins-CXR are visibly superior to those\\nderived from ImageNet weights, further affirming the efficacy of our approach in\\nenhancing CXR image analysis.\\nUpon escalating the input resolution to 640 * 640 pixels, both ImageNet and Bar-\\nlow Twin-CXR weighted models saw performance improvements due to the increased\\ndetail in the CXR images. Nonetheless, the performance differential between the\\ntwo narrowed, indicating that the higher resolution somewhat mitigates the distinct\\nadvantages of self-supervised pre-training.\\nThis points to intriguing future research avenues, such as refining image resolu-\\ntion parameters during pre-training and fine-tuning phases and investigating whether\\nhigher-resolution pre-training could elevate model performance. It also accentuates the\\nnecessity of tailoring deep learning model design to specific tasks, considering factors\\nlike image resolution and feature granularity.\\nOverall, implementing the Barlow Twins-CXR method on the VinDr dataset\\nresulted in substantial gains despite its data limitations and the inherent challenges\\nof CXR abnormality localization. An 11.5% performance enhancement over the base-\\nline and a 2.8% increment over ImageNet pre-trained models were observed on the\\nmAP50 metric. Such marked improvements confirm the Barlow Twins-CXR strategy’s\\nprowess in addressing domain inconsistencies, thereby fine-tuning naturally derived\\nimage weights for better applicability in CXR image analysis and beyond in medical\\nimaging.\\n4.2 Linear Evaluation Protocol\\nIn this experiment, we evaluated the impact of Barlow Twins-CXR pre-training versus\\ntraditional ImageNet pre-training on the linear classification performance within the\\nNIH-CXR dataset. We adhered to the linear evaluation protocol, freezing the backbone\\nof the linear classifier and updating only the final linear layer’s weights. This approach\\nwas applied across training datasets of varying sizes - 1%, 10%, and 100%, results of\\nthese experiments are presented in Figure 6 and Table 2.\\nThe results show that at a training data size of 1%, the Barlow Twins-CXR pre-\\ntrained model demonstrated a significant advantage, achieving an AUC of 0.6586 (95%\\n11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10}),\n",
       " Document(page_content='Ground Truth Bbox ImageNet Weights Our method\\n20e27597c972c6e7fdb4d1e7638e227e\\n03431b577d1ccf075e930c4c4913c079\\nfd810298e165ef0b9a88bb25fda7a34bGround Truth Bbox ImageNet Weights Our method\\n9eba0d101f410f9cdfae46cb094ae2a6\\n87a8df2f22475c7200ebe891d0f25b88\\nad86f42123384e2441cce36347aa7d1aa) b)Fig. 5 Heatmaps were generated from the initial images of the training set(left) and test set(right),\\nindicating successful Bbox predictions by the BarlowTwins-CXR model. Each heatmap corresponds\\nto one accurately predicted bbox, despite multiple bboxes present in each CXR image. Serial numbers\\nbelow the heatmaps refer to the image numbers in the dataset.\\nTable 2 AUC scores in validation and test sets for of linear models with varying pre-training\\nmethods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6586 (0.6556, 0.6616) 0.7773 (0.7756, 0.7790) 0.8031 (0.8027, 0.8035)\\nImage-Net 0.5932 (0.5913, 0.5951) 0.6855 (0.6822, 0.6889) 0.7098 (0.7089, 0.7107)\\n1Scores are presented with 95% confidence intervals.\\nCI 0.6556,0.6616) compared to 0.5932 (95% CI 0.5913,0.5951) for the ImageNet pre-\\ntrained model. As the training data size increased to 10% and 100%, the AUCs for\\nthe Barlow Twins-CXR pre-trained model reached 0.7773 (95% CI 0.7756,0.7790) and\\n0.8031 (95% CI 0.8027,0.8035), respectively, while the ImageNet pre-trained model\\nscored 0.6855 (95% CI 0.6822,0.6889) and 0.7098 (95% CI 0.7089,0.7107).\\nNotably, the incremental gains for both pre-training methods diminished with\\nlarger data sizes, suggesting that the performance boost provided by additional data\\nbecomes marginal when only the linear layer is updated.\\nThese findings highlight the Barlow Twins-CXR pre-training method’s superiority\\nover ImageNet pre-training across various dataset sizes, especially in data-limited sce-\\nnarios. This demonstrates the promise of self-supervised learning in enhancing medical\\nimage analysis, particularly when annotated data is scarce.\\n12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11}),\n",
       " Document(page_content='Fig. 6 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC\\nscores of linear models with Barlow Twins-CXR versus ImageNet weights across various dataset sizes\\n(1%, 10%, 100%). As indicated by higher AUC scores, models using Barlow Twins-CXR consistently\\noutperform those with ImageNet pre-training. Error bars represent the range of scores across five\\nexperiments.\\n4.3 End-to-End Finetuning\\nIn our end-to-end experiments, where we permitted updates to all model layers, the\\nBarlow Twins-CXR pre-trained ResNet50 backbone consistently outperformed the\\nImageNet pre-trained equivalent across all training set sizes. The results of these\\nexperiments are presented in Figure 7 and Table 3.\\nFig. 7 AUC Scores with Error Bars for NIH-CXR Classification - This figure displays the AUC scores\\nof models fine-tuned end-to-end with Barlow Twins-CXR versus ImageNet weights across various\\ndataset sizes (1%, 10%, 100%). Higher AUC scores indicate that models using Barlow Twins-CXR\\nconsistently outperform those with ImageNet pre-training. Error bars represent the range of scores\\nacross five experiments.\\n13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12}),\n",
       " Document(page_content='Table 3 AUC scores in validation and test sets for models fine-tuned end-to-end with varying\\npre-training methods at 224 and 640 input resolutions.\\nModel 1% 10% 100%\\nBarlowtwin-CXR 0.6585 (0.6544, 0.6627) 0.7756 (0.7745, 0.7768) 0.8107 (0.8098, 0.8116)\\nImage-Net 0.6163 (0.6110, 0.6216) 0.7168 (0.7093, 0.7243) 0.7866 (0.7843, 0.7889)\\n1Scores are presented with 95% confidence intervals.\\nAt a 1% training data size, the Barlow Twins-CXR model achieved a 4.2% higher\\nAUC than the ImageNet counterpart.\\nWith 10% and 100% data sizes, the Barlow Twins-CXR model maintained leads of\\napproximately 5.9% and 2.5%, respectively. Notably, the magnitude of improvement\\nover the frozen backbone setup was less marked, suggesting that the wealth of features\\nlearned during self-supervised training reduces the margin for additional gains during\\nsubsequent fine-tuning.\\nOverall, these end-to-end fine-tuning results suggest that comprehensive learning\\nacross all model layers may elevate the risk of overfitting, particularly when data is\\nscarce. The narrowing performance differential between the two pre-training strategies\\nwith increasing data volume indicates that the distinction between domain-specific\\n(Barlow Twins-CXR) and generalized (ImageNet) pre-training becomes less substan-\\ntial with larger datasets. This trend implies that the influence of the pre-training\\nstrategy on the final performance of models may diminish as the size of the medical\\nimage dataset grows.\\n5 Discussion\\nOur study demonstrates that the BarlowTwins-CXR approach effectively utilizes\\nunannotated CXR images for learning valuable representations and enhances trans-\\nfer learning efficiency from ImageNet, thus addressing issues of domain inconsistency.\\nThis leads to quicker training and improved performance on tasks like abnormality\\ndetection in the VinDr-CXR dataset. Barlow Twins-CXR excels across various input\\nresolutions, outshining models pre-trained on ImageNet.\\nOne of the primary limitations of our study is the scarcity of CXR datasets with\\nbounding box. Our reliance on public datasets, due to the absence of a private dataset,\\nmay limit the generalizability of our findings. Additionally, the computational cost\\nof the BarlowTwins pre-training remains substantial. For a dataset size of 112,120\\nimages with an image size of 224*224 pixels, the training process required two days\\non an NVIDIA A100 80G GPU. This significant resource requirement constrained our\\nability to experiment with higher image resolutions, which could potentially enhance\\nthe model’s performance.\\n6 Future Work\\nOur future endeavours include developing a demo interactive system for deployment\\nand testing in emergency rooms. It will allow practical evaluation of the model’s\\neffectiveness in a clinical setting and facilitate the collection of a proprietary dataset.\\n14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13}),\n",
       " Document(page_content='Additionally, we plan to explore more advanced self-supervised learning methods,\\nobject detection frameworks, and backbone networks to refine our approach further.\\nThe continuous evolution of these technologies promises to address some of the current\\nlimitations and expand the applicability and accuracy of our model in medical image\\nanalysis.\\n7 Conclusions\\nThe results of this study provide strong support for the application of self-supervised\\nlearning in the field of abnormality detection, especially valuable in environments\\nwhere radiologists face high workloads but the corresponding data labelling resources\\nare scarce. A critical aspect of this approach is its adaptability to regional variations in\\nCXR image, attributable to differences in imaging equipment, patient demographics,\\nand other locale-specific factors [34][35]. Such variations often impede the cross-\\nregional applicability of a model, thus limiting its generalizability. By employing the\\nBarlowTwins-CXR strategy, research organizations can transfer pre-trained backbone\\nnetworks to local datasets tailored to the unique characteristics of their regional data.\\nOur findings might also have significant implications for clinical practice, suggest-\\ning that this strategy could be a game-changer in aiding radiologists to interpret\\nCXR images efficiently. This technology promises to reduce diagnostic times, poten-\\ntially increasing patients’ throughput and improving the overall quality of care.\\nGiven its capacity for fine-tuning to specific regional characteristics, our approach\\nholds particular promise in areas where standardization of medical imaging presents\\nchallenges.\\nIn summary, the BarlowTwins-CXR approach demonstrates the potential of AI\\nto enhance healthcare delivery. By integrating cutting-edge technology with clini-\\ncal needs, we aim to pave the way for innovative solutions that benefit healthcare\\nprofessionals and patients.\\n8 Abbreviations\\nAP: anterior-posterior\\nAUC: area under the receiver operating characteristic curve\\nCAM: Class Activation Mapping\\nCIUSSS: Centre int´ egr´ e universitaire de sant´ e et de services sociaux\\nCXR: chest X-ray radiography\\nFC: Fully connected layer\\nFPN: Feature Pyramid Network\\nFPR: False Positive Rate\\nIoU: Intersection over Union\\nROC: receiver operating characteristic\\nROI: region of interest\\nmAP: mean Average Precision\\nPA: posterior-anterior\\nTPR: True Positive Rate\\n15', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14}),\n",
       " Document(page_content='WBF: Weighted Box Fusion\\nYOLO: You Only Look Once\\n9 Declarations\\n9.1 Ethics approval and consent to participate\\nAll methods were performed under relevant guidelines and regulations (e.g., Decla-\\nrations of Helsinki). The studies reported in this manuscript used reputable public\\ndatasets and did not require any additional data involving human participants, human\\ndata, or human tissue.\\n9.2 Consent for publication\\nNot applicable\\n9.3 Availability of data and materials\\nThe datasets generated and/or analysed during the current study are available in the\\nVinDr-CXR [10] and NIH-CXR[24] repository: VIndr-CXR and NIH-CXR.\\n9.4 Competing interests\\nThe authors declare that they have no competing interests\\n9.5 Funding\\nNo external funding was associated with this research study.\\n9.6 Authors’ contributions\\nHS designed the research methodology, analyzed data, was responsible for experiments\\nand results visualization, and participated in manuscript drafting and revision. LM\\nassisted in developing the research methodology and contributed to the drafting and\\nrevision of the manuscript. JFS collected and interpreted data, and provided expertise\\nin statistical analysis. DL contributed to the study design, offered statistical analysis\\nexpertise, assisted in interpreting results, and played a significant role in the critical\\nrevision of the manuscript.\\nAll authors read and approved the final manuscript.\\n9.7 Acknowledgements\\nThe authors wish to express their gratitude to CIUSSS du centre-sud-de-l’ˆ ıle-de-\\nmontr´ eal for the computational resources and support provided, which were essential\\nfor the research conducted as part of the graduate internship program. We are espe-\\ncially thankful to our department director, Mathieu Mailhot, for his mentorship and\\nto Chen Cheng for his collaborative efforts and valuable contributions to this project.\\nTheir expertise and insights have been greatly appreciated and substantially enhanced\\nthis work’s quality.\\n16', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15}),\n",
       " Document(page_content='References\\n[1] Satia, I., Bashagha, S., Bibi, A., et al. : Assessing the accuracy and certainty in\\ninterpreting chest x-rays in the medical division. Clinical medicine 13, 349–352\\n(2013). PMID: 23908502\\n[2] Rubin, G. D., Ryerson, C. J., Haramati, L. B., et al. : The role of chest imaging\\nin patient management during the covid-19 pandemic: a multinational consensus\\nstatement from the fleischner society. Radiology 296, 172–180 (2020). PMID:\\n32275978\\n[3] Lantsman, D. C., Barash, Y., Klang, E., Guranda, L., Konen, E., Tau, N.:\\nTrend in radiologist workload compared to number of admissions in the emer-\\ngency department. European Journal of Radiology 149, 110195 (2022). PMID:\\n35149337\\n[4] https://www.rsna.org/news/2022/may/Global-Radiologist-Shortage. Accessed:\\ndate-of-access (2022)\\n[5] Seah, J. C. Y., Tang, C. H. M., Buchlak, Q. D., et al. : Effect of a comprehensive\\ndeep-learning model on the accuracy of chest x-ray interpretation by radiologists:\\na retrospective, multireader multicase study. The Lancet Digital Health 3, 496–\\n506 (2021). PMID: 34219054\\n[6] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[7] Kim, H. E., Cosa-Linan, A., Santhanam, N., et al. : Transfer learning for medical\\nimage classification: a literature review. BMC medical imaging 22, 69 (2022)\\nhttps://doi.org/10.1186/s12880-022-00793-7\\n[8] Zbontar, J., Jing, L., Misra, I., et al. : Barlow twins: Self-supervised learning\\nvia redundancy reduction. In: Proceedings of the International Conference on\\nMachine Learning. PMLR, pp. 12310–12320 (2021). https://doi.org/10.48550/\\narXiv.2103.03230\\n[9] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, Kai, Fei-Fei, Li: Imagenet: A large-\\nscale hierarchical image database. In: Proceedings of the 2009 IEEE Conference on\\nComputer Vision and Pattern Recognition, Miami, FL, USA. IEEE, pp. 248–255\\n(2009). https://doi.org/10.1109/CVPR.2009.5206848\\n[10] Nguyen, H. Q., Lam, K., Le, L. T., et al. : Vindr-cxr: An open dataset of chest\\nx-rays with radiologist’s annotations. Sci Data 9, 429 (2022) https://doi.org/10.\\n1038/s41597-022-01498-w\\n17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16}),\n",
       " Document(page_content='[11] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV, USA, pp. 770–778 (2016). https://doi.org/\\n10.1109/CVPR.2016.90\\n[12] Rajpurkar, P., Irvin, J., Zhu, K., et al.: Chexnet: Radiologist-level pneumonia\\ndetection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225\\n(2017). https://doi.org/10.48550/arXiv.1711.05225\\n[13] Sun, K. X., Cong, C.: Research on chest abnormality detection based on improved\\nyolov7 algorithm. In: Proceedings of the 2022 IEEE International Conference on\\nBioinformatics and Biomedicine (BIBM), Las Vegas, NV, USA, pp. 3884–3886\\n(2022). https://doi.org/10.1109/BIBM55620.2022.9995687\\n[14] Ga´ al, G., Maga, B., Luk´ acs, A.: Attention u-net based adversarial architectures\\nfor chest x-ray lung segmentation. arXiv preprint arXiv:2003.10304 (2020). https:\\n//doi.org/10.48550/arXiv.2003.10304\\n[15] Shurrab, S., Duwairi, R.: Self-supervised learning methods and applications in\\nmedical imaging analysis: A survey. PeerJ Computer Science 8, 1045 (2022) https:\\n//doi.org/10.7717/peerj-cs.1045\\n[16] Bengio, Y., Lamblin, P., Popovici, D., et al. : Greedy layer-wise training of deep\\nnetworks. In: Proceedings of the 19th International Conference on Neural Infor-\\nmation Processing Systems (NIPS’06), Cambridge, MA, USA, pp. 153–160 (2006).\\nhttps://doi.org/10.5555/2976456.2976476\\n[17] Pathak, D., Krahenbuhl, P., Donahue, J., et al. : Context encoders: Feature\\nlearning by inpainting. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, Las Vegas, NV, USA, pp. 2536–2544 (2016).\\nhttps://doi.org/10.1109/CVPR.2016.278\\n[18] Chen, T., Kornblith, S., Norouzi, M., et al. : A simple framework for contrastive\\nlearning of visual representations. In: Proceedings of the International Conference\\non Machine Learning, pp. 1597–1607 (2020). https://doi.org/10.5555/3524938.\\n3525087\\n[19] Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by\\npredicting image rotations. arXiv preprint arXiv:1803.07728 (2018). https://doi.\\norg/10.48550/arXiv.1803.07728\\n[20] Azizi, S., Mustafa, B., Ryan, F., et al. : Big self-supervised models advance medical\\nimage classification. In: Proceedings of the IEEE/CVF International Conference\\non Computer Vision, Montreal, QC, Canada, pp. 3478–3488 (2021). https://doi.\\norg/10.1109/ICCV48922.2021.00346\\n[21] Sowrirajan, H., Yang, J., Ng, A. Y., Rajpurkar, P.: Moco pretraining improves\\n18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17}),\n",
       " Document(page_content='representation and transferability of chest x-ray models. In: Medical Imaging with\\nDeep Learning, pp. 728–744 (2021). https://doi.org/10.48550/arXiv.2010.05352\\n[22] Morid, M. A., Borjali, A., Del Fiol, G.: A scoping review of transfer learn-\\ning research on medical image analysis using imagenet. Computers in Biology\\nand Medicine 128, 104115 (2021) https://doi.org/10.1016/j.compbiomed.2020.\\n104115\\n[23] Matsoukas, C., Haslum, J., Sorkhei, M., Soderberg, M., Smith, K.: What makes\\ntransfer learning work for medical images: Feature reuse & other factors. In:\\nProceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), New Orleans, LA, USA, pp. 9215–9224 (2022). https://doi.\\norg/10.1109/CVPR52688.2022.00901\\n[24] Wang, X., Peng, Y., Lu, L., et al. : Chestx-ray8: Hospital-scale chest x-ray\\ndatabase and benchmarks on weakly-supervised classification and localization\\nof common thorax diseases. In: Proceedings of the 2017 IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, pp.\\n3462–3471 (2017). https://doi.org/10.1109/CVPR.2017.369\\n[25] Solovyev, R., Wang, W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes\\nfrom different object detection models. Image and Vision Computing 107, 104117\\n(2021) https://doi.org/10.1016/j.imavis.2021.104117\\n[26] He, K., Fan, H., Wu, Y., et al. : Momentum contrast for unsupervised visual\\nrepresentation learning. In: Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, Seattle, WA, USA, pp. 9729–9738 (2020).\\nhttps://doi.org/10.1109/CVPR42600.2020.00975\\n[27] Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference\\non Computer Vision, Santiago, Chile, pp. 1440–1448 (2015). https://doi.org/10.\\n1109/ICCV.2015.169\\n[28] Lin, T. Y., Doll´ ar, P., Girshick, R., et al. : Feature pyramid networks for object\\ndetection. In: Proceedings of the 2017 IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), Honolulu, HI, USA, pp. 936–944 (2017). https:\\n//doi.org/10.1109/CVPR.2017.106\\n[29] Chen, K., Wang, J., Pang, J., Cao, Y., et al.: MMDetection: Open mmlab detec-\\ntion toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019). https:\\n//doi.org/10.48550/arXiv.1906.07155\\n[30] Bachman, P., Hjelm, R. D., Buchwalter, W.: Learning representations by maxi-\\nmizing mutual information across views. In: Proceedings of the 33rd International\\nConference on Neural Information Processing Systems, Red Hook, NY, USA, pp.\\n15535–15545 (2019). https://doi.org/10.5555/3454287.3455679\\n19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18}),\n",
       " Document(page_content='[31] Kornblith, S., Shlens, J., Le, Q. V.: Do better imagenet models transfer better? In:\\nProceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), Long Beach, CA, USA, pp. 2656–2666 (2019). https://doi.\\norg/10.1109/CVPR.2019.00277\\n[32] Sechidis, K., Tsoumakas, G., Vlahavas, I.: On the stratification of multi-label\\ndata. In: Gunopulos, D., Hofmann, T., Malerba, D., Vazirgiannis, M. (eds.)\\nMachine Learning and Knowledge Discovery in Databases, pp. 145–158. Springer,\\nBerlin (2011)\\n[33] Szyma´ nski, P., Kajdanowicz, T.: A network perspective on stratification of multi-\\nlabel data. Proceedings of the First International Workshop on Learning with\\nImbalanced Domains: Theory and Applications (2017). https://doi.org/10.48550/\\narXiv.1704.08756\\n[34] Van Ryn, M., Burke, J.: The effect of patient race and socio-economic status on\\nphysicians’ perceptions of patients. Social Science & Medicine 50, 813–828 (2000).\\nPMID: 10695979\\n[35] Waite, S., Scott, J., Colombo, D.: Narrowing the gap: imaging disparities in\\nradiology. Radiology 299, 27–35 (2021). PMID: 33560191\\n20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='barlowtwins-cxr enhancing chest x-ray based abnormality localization with self-supervised learning haoyue sheng123 linrui ma12 jean-fran cois samson3 dianbo liu24 1d epartement dinformatique et de recherche op erationnelle universit e de montr eal 2920 chemin de la tour montr eal h3t 1j4 qc canada 2mila - quebec ai institute 6666 rue saint-urbain montr eal h2s 3h1 qc canada 3direction des ressources informationnelles ciusss du centre-sud-de-l ile-de-montr eal 400 blvd de maisonneuve ouest montr eal h3a 1l4 qc canada 4school of medicine and college of design and engineering national university of singapore 21 lower kent ridge rd singapore 119077 sg singapore corresponding authors e-mails haoyueshengumontrealca contributing authors linruimaumontrealca jean-francoissamsonccsmtlssssgouvqcca dianbonusedusg abstract background chest x-ray imaging based abnormality localization essential in diagnosing various diseases faces significant clinical challenges due to complex interpretations and the growing workload of radiologists recent advances in deep learning especially self-supervised learning offer promising solutions to enhance image analysis efficiency accuracy and reliability this study aims to improve autonomic abnormality localization performance of chest x-ray image analysis particularly in detecting abnormalities using a self- supervised learning method called barlowtwins-cxr methods we utilized two publicly available datasets the nih chest x-ray dataset and the vindr-cxr the barlowtwins-cxr approach was conducted in a two-stage training process initially self-supervised pre-training was performed using an adjusted barlow twins algorithm on the nih dataset with a resnet50 1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0}),\n",
       " Document(page_content='backbone pre-trained on imagenet this was followed by supervised fine-tuning on the vindr-cxr dataset using faster r-cnn with feature pyramid network fpn the study employed mean average precision map at an intersection over union iou of 50 and area under the curve auc for performance evaluation results our experiments showed a significant improvement in model perfor- mance with barlowtwins-cxr the approach achieved a 3 increase in map50 accuracy compared to traditional imagenet pre-trained models in addition the ablation cam method revealed enhanced precision in localizing chest abnormal- ities the study involved 112120 images from the nih dataset and 18000 images from the vindr-cxr dataset indicating robust training and testing samples conclusion barlowtwins-cxr significantly enhances the efficiency and accu- racy of chest x-ray image base abnormality localization outperforming tradi- tional transfer learning methods its ability to adapt to various imaging conditions and regional variations demonstrates the potential of self-supervised learning in medical diagnostics this approach can be instrumental in aiding radiologists particularly in high-workload environments offering a promising direction for future ai-driven healthcare solutions keywords medical image analysis chest x-ray abnormality localization deep learning object detection self-supervised learning transfer learning heat map area under curve mean average precision 1 introduction chest x-raycxr is a fundamental and widespread medical diagnostic tool for diag- nosing chest diseases it is efficient and cost-effective suitable for preliminary screening and diagnosis 1 during the 2019 coronavirus pandemic cxr was widely used for triaging patients and prioritizing the care order due to its convenience and flexibility effective mitigation addresses the lack of availability of computed tomography and reduces the risk of transmission in the room with the ct scanner 2 however its com- plex interpretation often requires a highly qualified radiologist to make an accurate diagnosis 1 as the demand for healthcare increases the workload of radiologists has significantly increased 3 it results in less time to analyze each radiographic image potentially increasing the risk of diagnostic error in many areas especially in develop- ing and remote areas qualified radiologists are insufficient to cope with the increased demand for healthcare for instance europe has 13 radiologists per 100000 people while the united kingdom has 85 and malaysia has approximately 30 per million population 4 this situation necessitates urgently developing and introducing auto- mated technologies like ai-based image analysis tools to aid radiologists in quicker and more precise cxr image analysis it will improve the quality of diagnosis and help reduce the workload of doctors in recent years deep learning models have rapidly advanced in various medi- cal image analysis fields of cxr demonstrating diagnostic accuracy comparable to human experts 5 object detection plays a more critical role in medical image anal- ysis because it can identify and precisely locate the types of anomalies in the images 2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1}),\n",
       " Document(page_content='providing doctors with more specific and valuable information however training these models requires a large amount of annotated data these annotations must be performed by experienced radiologists for cxr images as well as for most medi- cal images making such annotated data not only costly but also rare with only a very limited number of public datasets including bounding box information although transfer learning is widely regarded as an effective method to solve the problem of scarce labelling data its application in medical image analysis still faces limita- tions this is mainly due to the significant difference in feature distribution between large datasets such as imagenet used for pre-training models and medical imaging datasets this disparity suggests that directly applying these pre-trained weights to medical image analysis might not yield the best outcomes particularly for specialized medical diagnostic applications 67 to fill these gaps our study proposed a novel method namely barlowtwins-cxr employing a dual-phase training process to enhance cxr image analysis the first phase involves unsupervised pre-training using a barlow twins algorithm 8 on cxr images without annotation starting with an imagenet 9 pre-trained model as the foundation in the second phase transfer learning on the vindr-cxr 10 dataset is applied to fine-tune the model our experiments show that such a training strategy combining self-supervised pre-training and supervised fine-tuning is particularly effec- tive in our experiments while employing resnet50 11 as the backbone architecture we observed that implementing the barlowtwins-cxr strategy significantly improved model performance we observed a 3 increase in model accuracy on the mean average precision benchmark surpassing the results achieved by directly performing conventional transfer learning from imagenet pre-trained weights this study extends the application of self-supervised learning to chest x-ray abnor- mality localization it demonstrates the potential of self-supervised learning in medical imaging analysis especially in the absence of annotated data by effectively improving detection performance and precisely localizing abnormalities barlowtwins-cxr rep- resents a significant advancement in the field of cxr abnormality localization paving the way for more efficient and accurate diagnostic methods in the future 2 related work in recent years deep learning techniques have excelled in the field of medical imaging particularly in analyzing cxr images for example in terms of disease classifica- tion chexnet proposed by pranav rajpurkar et al 12 outperformed radiologists in detecting chest diseases when benchmarked against the f1 score neural network models trained with vast amounts of labelled data are capable of identifying features of various pulmonary diseases in anomaly detection tasks sun k x et al used the yolov7 object detection framework to effectively identify and locate lesions in cxr images 13 this achievement is attributed to the advanced image recognition and feature extraction capabilities of neural networks additionally the modified u-net architecture which incorporates attention mechanisms as proposed by guszt av ga al et al 14 has made significant strides in accurately segmenting lung structures thus aiding in detailed analysis and diagnosis of diseases 3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2}),\n",
       " Document(page_content='self-supervised learning has recently gained popularity in the field of medical imag- ing 15 and provides an efficient method for utilizing unlabeled data initially proposed by bengio et al this approach allows models to learn from unlabeled data and extract useful feature representations by training deep networks on unsupervised data 16 such learning strategy promotes models to capture the intrinsic structure and rela- tionships in data by designing innovative pretext tasks such as image reconstruction eg context encoder 17 contrastive learning eg simclr 18 or prediction tasks eg rotation prediction 19 in the field of medical imaging shekoofeh azizi et al used large-scale images for self-supervised learning to improve accuracy and con- vergence speed significantly in downstream tasks achieving better performance than models pre-trained on imagenet 20 sowrirajan h et al proposed a pre-trained model based on momentum contrast to enhance the representativeness and portability of cxr models 21 in terms of transfer learning applying models trained in one domain to another has led to notable success in medical image analysis research indicates that well-processed transfer results from imagenet can improve model performance in the medical imag- ing domain 22 however studies by christos matsoukas et al have shown that due to the significant difference in feature distribution between medical and natural images features learned from natural images may not always be broadly applicable to med- ical images 23 various cross-domain adaptive transfer learning methods have been developed to address these challenges such as unsupervised and semi-supervised learn- ing and sequential domain adaptation techniques by tuning model parameters these methods can be better adapted to the characteristics of medical images improving the performance and accuracy of models in medical image analysis 22 3 methods 31 dataset selection this study utilized two publicly available chest x-ray datasets the nih-cxr24 dataset and the vindr-cxr dataset the nih dataset comprises 112120 posterior- anterior pa or anterior-posterior ap cxr images from 30805 patients covering 14 diseases with image-level annotations including disease location annotations in some images the distribution of the nih-cxr dataset is illustrated in figure 1 meanwhile the vindr-cxr dataset is the largest publicly available dataset for adult cxr object detection which includes 18000 pa cxr scans these scans encom- pass 14 diseases with detailed instance-level bounding box annotations making it ideal for the fine-tuning phase the vindr-cxr dataset exhibits a distinct labelling process for its test and train- ing sets the training set consisting of 15000 images was annotated independently by three radiologists per image in contrast the test set comprising 3000 images underwent a more rigorous annotation process initially each image was independently annotated by three radiologists this is followed by a secondary review phase where these initial annotations are reviewed by two other more experienced radiologists they communicated with each other to resolve any disagreements and reach a consensus on 4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3}),\n",
       " Document(page_content='countdiseaseabnormalityno finding infiltration effusion atelectasis nodule mass pneumothorax consolidation pleuralthickening cardiomegaly emphysema edema fibrosis pneumonia hernia 0 10000 20000 30000 40000 50000 60000 70000fig 1 image-level label distribution of the nih-cxr dataset the final labelling this meticulous process for the test set created a potential dispar- ity in data distribution compared to the training set to eliminate any bias it might introduce in our study we resplit the original training set into new training validation and test sets for our experiments to improve the quality of the training data a weighted box fusion wbf 25 pre- processing technique was applied to the vindr-cxr training set the wbf involves calculating the weighted average of each set of duplicate bounding boxes to create a single fused bounding box such a preprocessing step is crucial for reducing annota- tion redundancy and improving target area representation in the dataset figure 2 shows the data distribution of vindr-cxr before and after wbf preprocessing we chose the vindr-cxr dataset not only because it is the largest publicly avail- able dataset for adult cxr object detection but also because of the high level of diversity and richness of its data 32 dual-phase training process our training encompasses two primary phases self-supervised pre-training and sub- sequent supervised fine-tuning initially we commenced with a resnet50 model pre-trained on imagenet as shown in figure 3 in the self-supervised pre-training phase we applied a modified barlow twins method to the nih-cxr dataset this approach refined the imagenet pre-trained model by updating its backbone weights subsequently in the supervised fine-tuning phase we utilize this refined backbone within a faster r-cnn framework by applying it to the vindr-cxr dataset this step aims to further improve the models task-specific performance explicitly enhancing its capabilities in localized diseases in cxr images 5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4}),\n",
       " Document(page_content='a bfig 2 instance-level annotation distribution of vindr-cxr dataset beforea and afterb wbf preprocessing 321 self-supervised pre-training for the first stage of training we employed the original version of the barlow twins method as mentioned in zbontar et al 8 this approach represents a shift from conventional contrastive learning introducing a self-supervised learning framework primarily focused on diminishing redundancy the barlow twins method operates on a straightforward yet potent principle it learns distinctive features by reducing the representational differences between two differently distorted images from the same source as processed by the network this strategy is instrumental in enabling the model to identify unique and rich features in each image while concurrently minimizing the overlap in features the process involves generating two distinct variants of an image through data augmentation followed by their simultaneous processing via two deep 6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 5}),\n",
       " Document(page_content='input imagec2 c3 c4 c5 p2 p3 p4 p5resnet50 backbone convrpncls rpnreg roi poolingregion proposal network fc fc fcfeature map feature vectorfeature mapclass boxfeature pyramid networkx y ay b imagesdistorted images z a z bresnet 50 resnet 50net embeddings emporocal cross-corr target cross-corr i lbt feature dimensionbarlow twins faster r-cnnfig 3 schematic overview of the dual-phase training framework the upper panel illustrates the barlow twins method in phase one where pairs of distorted images are processed through a shared resnet50 network to produce embeddings these are then compared using an empirical cross-correlation matrix c striving for the identity matrix i to minimize redundancy in feature dimensions and optimizing the loss function l bt in phase two lower panel the pre-trained resnet50 backbone from phase one is integrated into a faster r-cnn architecture it starts with multi-scale feature extraction through the feature pyramid network fpn followed by the region proposal network rpn that generates object region proposals the features are then pooled and processed by fully connected fc layers to output the final class labels and bounding box coordinates for object detection tasks neural networks that share identical weights the objective is to align the networks weights to enhance the similarity in the high-level representations of these image pairs yet ensure that the individual features remain distinct and independent the barlow twins method might be particularly useful for medical imaging because it extracts features by minimizing the redundancy between representations of perturbed images in cxr imaging subtle differences might indicate important health information and the barlow twins can effectively capture these subtle but clinically important features in contrast to other contrastive learning algorithms like moco 26 and simclr which construct similarity matrices at the batch dimension barlow twins works at the feature dimension it aims to assign an independent meaning to each feature dimension this could lead to a richer feature representation potentially better adapted to variations in cxr images eg different imaging conditions and 7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6}),\n",
       " Document(page_content='pathological states moreover compared to self-supervised learning methods requir- ing negative samples or complex contrastive mechanisms like simclr barlow twins offers a more straightforward training framework which is particularly important in situations with limited computational resources we chose to apply barlow twins pre-training on the imagenet pretrained resnet50model since the imagenet pre-trained model weights can be easily obtained from the torchvision library this step brings no additional cost we used images from the training set portion of the nih-cxr dataset for this training phase with the input image size set to 224224 pixels the training was executed on an nvidia a100 80g gpu setting the batch size to 768 to maximize the utilization of this graphics cards capabilities over 600 epochs 322 fine-tuning phase in our fine-tuningtransfer learning stage we utilized the faster r-cnn 27 with feature pyramid network fpn 28 as our object detector and trained it on the vindr-cxr dataset faster r-cnn a widely-used object detection framework com- prises two main components the region proposal network rpn 28 and the fast r-cnn detector first rpn generates candidate regions for objects and then the fast r-cnn detector employs these regions to detect and classify targets this architecture renders faster r-cnn particularly efficient in processing complex images the feature pyramid network fpn an architecture frequently employed in object detection particularly enhances performance with multi-scale targets it integrates high-level semantic information from deeper layers with detailed information from shallower lay- ers producing feature maps of varied scales that effectively detect differently sized targets we employed the mmdetection 29 machine learning toolbox as the platform for faster r-cnn utilizing a number of classical image augmentation techniques and maintaining consistent hyperparameters across all experiments two different input sizes 224224 pixels and 640640 pixels were chosen to assess the impact of image size on the models performance with the pre-trained models in addition for comparison we also conducted experiments using imagenet pre-trained weights directly we implemented a linear evaluation protocol 3031 on the nih-cxr dataset to comprehensively evaluate the self-supervised learning models performance in medical imaging this method examines the models feature transfer capability - its ability to adapt learned representations to new tasks we first resplit the test set of the nih dataset into two parts 80 as an evaluation training set for training a linear classifier and the remaining 20 as an evaluation test set for assessing model performance we adopted two distinct strategies during the evaluation freezing the backbone weights or fine-tuning the weights in the freezing backbone strategy we kept the parameters of the backbone network ie the feature extraction layers obtained from self-supervised pretraining unchanged we updated only the weights of the final lin- ear layer conversely under the fine-tuning strategy we updated parameters across the entire network encompassing both the self-supervised trained feature extraction layers and the newly added linear classifier layer we used 100 10 and 1 of the 8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier allowing us to assess the models performance across different scales of training data when evaluating the representation transfer ability of a self-supervised learning model it is necessary to ensure that the ratio of individual labels in the training and test sets is consistent we used the iterative stratification for the multi-label data method 3233 to ensure that the proportions of each label in the evaluation training and test sets were roughly similar this helped prevent biases due to uneven label distribution making our evaluation results more reliable and convincing 33 results analysis process for the analysis of results we employed the mean average precision map at an intersection over union iou of 50 as the benchmark for evaluating the performance of our object detection models map is a widely recognized and effective metric in object detection calculated by averaging precision scores across various object detec- tion confidence thresholds specifically map is the mean of the average precision scores for each class the proportion of correct predictions relative to all predictions for a specific class across different detection confidence thresholds determines the pre- cision score in the context of cxr abnormality localization utilizing map at an iou of 50 is beneficial for capturing clinically significant lesion detections while allowing for a reasonable degree of positional deviation which is practical for actual clinical applications moreover we utilized the area under the curve auc as a metric for the lin- ear evaluation protocol auc a standard metric in medical image analysis balances precision and recall making it an especially appropriate performance indicator for this field the auc metric represents the area under the receiver operating char- acteristic roc curve accounting for the models true positive rate tpr and false positive rate fpr at various thresholds this assessment method balances the models sensitivity and specificity enhancing detection rates while controlling false positives medical image analysis often deals with imbalanced data and auc is robust for imbalanced datasets as it does not rely directly on classification thresholds beyond using map and auc for quantitative analysis our study also utilized the ablation cam class activation mapping method to create heat maps for qualitative evaluation ablation cam systematically abates features in the models final convo- lutional layer and observes the impact on the output class scores this process reveals the most influential regions for the models decision-making the resulting heat maps delineate areas of interest in cxr images providing intuitive visual evidence of how our barlowtwins-cxr model focuses on and recognizes abnormalities 4 results 41 transfer learning on vindr abnormality localization in this experiment we examined the efficacy of the resnet backbone pre-trained by the barlow twins-cxr method for abnormality localization on the vindr-cxr dataset 9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8}),\n",
       " Document(page_content='using two different input resolutions consistent hyperparameter settings were main- tained across all experiments ensuring that the performance changes were attributable only to the merits of the pretraining method itself we visualized the performance of different models such as barlow twins-cxr pre-training and imagenet pre-training on the validation set in figure 4 and tabulated the corresponding map performance in table 1 as depicted in the figure the baseline model with an untrained resnet50 backbone reached a final map50 score of 01342 95 ci 0130601378 setting a performance baseline without pre-training benefits a b fig 4 evolution of map50 across epochs for different resnet50 backbones on the vindr-cxr dataset at 224224left and 640640right resolution the darker lines represent the average map50 of fourleft and fiveright trials with different random seeds with shaded areas indicating the range between the lowest and highest value table 1 map50 scores in validation and test sets for models with varying pre-training methods at different input resolutions backbone weight input size map50 val set map50 test set baseline nopretrained 224 01388 0135201424 01342 0130601378 imagenet pretrained 02245 0220402286 02210 0219402226 barlow twins 02555 0248502626 02448 0241402482 barlow twins from imagenet 02625 0256802682 02502 0247602528 imagenet pretrained 640 02973 0291303033 0280 0275702848 barlow twins from imagenet 03102 0308003125 0289 0282602954 1scores are presented with 95 confidence intervals a significant advancement was observed with the imagenet pre-trained resnet50 which attained a map50 of 02210 95 ci 0219402226 underscoring the value of pre-training in feature representation across disparate image domains 10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9}),\n",
       " Document(page_content='more strikingly incorporating the barlow twins-cxr strategy led to a rapid per- formance ascent achieving a map50 of 02448 95 ci 02414 02482 it marked an expedited training trajectory and a significant increase in detection performance when further enhanced by pre-training from imagenet the barlow twins-cxr approach yielded the best performance recording a map of 02502 95 ci 02476 02528 evidencing the synergetic effect of combining pre-training methodologies the heat maps generated from the study present a compelling visualization of the performance of the barlowtwins-cxr method compared to the traditional imagenet weights approach we generated heat maps of the first few cxr images of the train- ing and test sets in figure 5 in each image our methods heat maps show a more focused alignment with the actual lesion areas marked by the ground truth bbox this indicates a higher precision in localizing and identifying pathological features with barlowtwins-cxr potentially offering more targeted information for clinical diagnoses notably in cases of cardiomegaly and lung opacity the concentration and localization of the heatmaps from barlowtwins-cxr are visibly superior to those derived from imagenet weights further affirming the efficacy of our approach in enhancing cxr image analysis upon escalating the input resolution to 640 640 pixels both imagenet and bar- low twin-cxr weighted models saw performance improvements due to the increased detail in the cxr images nonetheless the performance differential between the two narrowed indicating that the higher resolution somewhat mitigates the distinct advantages of self-supervised pre-training this points to intriguing future research avenues such as refining image resolu- tion parameters during pre-training and fine-tuning phases and investigating whether higher-resolution pre-training could elevate model performance it also accentuates the necessity of tailoring deep learning model design to specific tasks considering factors like image resolution and feature granularity overall implementing the barlow twins-cxr method on the vindr dataset resulted in substantial gains despite its data limitations and the inherent challenges of cxr abnormality localization an 115 performance enhancement over the base- line and a 28 increment over imagenet pre-trained models were observed on the map50 metric such marked improvements confirm the barlow twins-cxr strategys prowess in addressing domain inconsistencies thereby fine-tuning naturally derived image weights for better applicability in cxr image analysis and beyond in medical imaging 42 linear evaluation protocol in this experiment we evaluated the impact of barlow twins-cxr pre-training versus traditional imagenet pre-training on the linear classification performance within the nih-cxr dataset we adhered to the linear evaluation protocol freezing the backbone of the linear classifier and updating only the final linear layers weights this approach was applied across training datasets of varying sizes - 1 10 and 100 results of these experiments are presented in figure 6 and table 2 the results show that at a training data size of 1 the barlow twins-cxr pre- trained model demonstrated a significant advantage achieving an auc of 06586 95 11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10}),\n",
       " Document(page_content='ground truth bbox imagenet weights our method 20e27597c972c6e7fdb4d1e7638e227e 03431b577d1ccf075e930c4c4913c079 fd810298e165ef0b9a88bb25fda7a34bground truth bbox imagenet weights our method 9eba0d101f410f9cdfae46cb094ae2a6 87a8df2f22475c7200ebe891d0f25b88 ad86f42123384e2441cce36347aa7d1aa bfig 5 heatmaps were generated from the initial images of the training setleft and test setright indicating successful bbox predictions by the barlowtwins-cxr model each heatmap corresponds to one accurately predicted bbox despite multiple bboxes present in each cxr image serial numbers below the heatmaps refer to the image numbers in the dataset table 2 auc scores in validation and test sets for of linear models with varying pre-training methods at 224 and 640 input resolutions model 1 10 100 barlowtwin-cxr 06586 06556 06616 07773 07756 07790 08031 08027 08035 image-net 05932 05913 05951 06855 06822 06889 07098 07089 07107 1scores are presented with 95 confidence intervals ci 0655606616 compared to 05932 95 ci 0591305951 for the imagenet pre- trained model as the training data size increased to 10 and 100 the aucs for the barlow twins-cxr pre-trained model reached 07773 95 ci 0775607790 and 08031 95 ci 0802708035 respectively while the imagenet pre-trained model scored 06855 95 ci 0682206889 and 07098 95 ci 0708907107 notably the incremental gains for both pre-training methods diminished with larger data sizes suggesting that the performance boost provided by additional data becomes marginal when only the linear layer is updated these findings highlight the barlow twins-cxr pre-training methods superiority over imagenet pre-training across various dataset sizes especially in data-limited sce- narios this demonstrates the promise of self-supervised learning in enhancing medical image analysis particularly when annotated data is scarce 12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11}),\n",
       " Document(page_content='fig 6 auc scores with error bars for nih-cxr classification - this figure displays the auc scores of linear models with barlow twins-cxr versus imagenet weights across various dataset sizes 1 10 100 as indicated by higher auc scores models using barlow twins-cxr consistently outperform those with imagenet pre-training error bars represent the range of scores across five experiments 43 end-to-end finetuning in our end-to-end experiments where we permitted updates to all model layers the barlow twins-cxr pre-trained resnet50 backbone consistently outperformed the imagenet pre-trained equivalent across all training set sizes the results of these experiments are presented in figure 7 and table 3 fig 7 auc scores with error bars for nih-cxr classification - this figure displays the auc scores of models fine-tuned end-to-end with barlow twins-cxr versus imagenet weights across various dataset sizes 1 10 100 higher auc scores indicate that models using barlow twins-cxr consistently outperform those with imagenet pre-training error bars represent the range of scores across five experiments 13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12}),\n",
       " Document(page_content='table 3 auc scores in validation and test sets for models fine-tuned end-to-end with varying pre-training methods at 224 and 640 input resolutions model 1 10 100 barlowtwin-cxr 06585 06544 06627 07756 07745 07768 08107 08098 08116 image-net 06163 06110 06216 07168 07093 07243 07866 07843 07889 1scores are presented with 95 confidence intervals at a 1 training data size the barlow twins-cxr model achieved a 42 higher auc than the imagenet counterpart with 10 and 100 data sizes the barlow twins-cxr model maintained leads of approximately 59 and 25 respectively notably the magnitude of improvement over the frozen backbone setup was less marked suggesting that the wealth of features learned during self-supervised training reduces the margin for additional gains during subsequent fine-tuning overall these end-to-end fine-tuning results suggest that comprehensive learning across all model layers may elevate the risk of overfitting particularly when data is scarce the narrowing performance differential between the two pre-training strategies with increasing data volume indicates that the distinction between domain-specific barlow twins-cxr and generalized imagenet pre-training becomes less substan- tial with larger datasets this trend implies that the influence of the pre-training strategy on the final performance of models may diminish as the size of the medical image dataset grows 5 discussion our study demonstrates that the barlowtwins-cxr approach effectively utilizes unannotated cxr images for learning valuable representations and enhances trans- fer learning efficiency from imagenet thus addressing issues of domain inconsistency this leads to quicker training and improved performance on tasks like abnormality detection in the vindr-cxr dataset barlow twins-cxr excels across various input resolutions outshining models pre-trained on imagenet one of the primary limitations of our study is the scarcity of cxr datasets with bounding box our reliance on public datasets due to the absence of a private dataset may limit the generalizability of our findings additionally the computational cost of the barlowtwins pre-training remains substantial for a dataset size of 112120 images with an image size of 224224 pixels the training process required two days on an nvidia a100 80g gpu this significant resource requirement constrained our ability to experiment with higher image resolutions which could potentially enhance the models performance 6 future work our future endeavours include developing a demo interactive system for deployment and testing in emergency rooms it will allow practical evaluation of the models effectiveness in a clinical setting and facilitate the collection of a proprietary dataset 14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13}),\n",
       " Document(page_content='additionally we plan to explore more advanced self-supervised learning methods object detection frameworks and backbone networks to refine our approach further the continuous evolution of these technologies promises to address some of the current limitations and expand the applicability and accuracy of our model in medical image analysis 7 conclusions the results of this study provide strong support for the application of self-supervised learning in the field of abnormality detection especially valuable in environments where radiologists face high workloads but the corresponding data labelling resources are scarce a critical aspect of this approach is its adaptability to regional variations in cxr image attributable to differences in imaging equipment patient demographics and other locale-specific factors 3435 such variations often impede the cross- regional applicability of a model thus limiting its generalizability by employing the barlowtwins-cxr strategy research organizations can transfer pre-trained backbone networks to local datasets tailored to the unique characteristics of their regional data our findings might also have significant implications for clinical practice suggest- ing that this strategy could be a game-changer in aiding radiologists to interpret cxr images efficiently this technology promises to reduce diagnostic times poten- tially increasing patients throughput and improving the overall quality of care given its capacity for fine-tuning to specific regional characteristics our approach holds particular promise in areas where standardization of medical imaging presents challenges in summary the barlowtwins-cxr approach demonstrates the potential of ai to enhance healthcare delivery by integrating cutting-edge technology with clini- cal needs we aim to pave the way for innovative solutions that benefit healthcare professionals and patients 8 abbreviations ap anterior-posterior auc area under the receiver operating characteristic curve cam class activation mapping ciusss centre int egr e universitaire de sant e et de services sociaux cxr chest x-ray radiography fc fully connected layer fpn feature pyramid network fpr false positive rate iou intersection over union roc receiver operating characteristic roi region of interest map mean average precision pa posterior-anterior tpr true positive rate 15', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14}),\n",
       " Document(page_content='wbf weighted box fusion yolo you only look once 9 declarations 91 ethics approval and consent to participate all methods were performed under relevant guidelines and regulations eg decla- rations of helsinki the studies reported in this manuscript used reputable public datasets and did not require any additional data involving human participants human data or human tissue 92 consent for publication not applicable 93 availability of data and materials the datasets generated andor analysed during the current study are available in the vindr-cxr 10 and nih-cxr24 repository vindr-cxr and nih-cxr 94 competing interests the authors declare that they have no competing interests 95 funding no external funding was associated with this research study 96 authors contributions hs designed the research methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project their expertise and insights have been greatly appreciated and substantially enhanced this works quality 16', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15}),\n",
       " Document(page_content='references 1 satia i bashagha s bibi a et al assessing the accuracy and certainty in interpreting chest x-rays in the medical division clinical medicine 13 349352 2013 pmid 23908502 2 rubin g d ryerson c j haramati l b et al the role of chest imaging in patient management during the covid-19 pandemic a multinational consensus statement from the fleischner society radiology 296 172180 2020 pmid 32275978 3 lantsman d c barash y klang e guranda l konen e tau n trend in radiologist workload compared to number of admissions in the emer- gency department european journal of radiology 149 110195 2022 pmid 35149337 4 httpswwwrsnaorgnews2022mayglobal-radiologist-shortage accessed date-of-access 2022 5 seah j c y tang c h m buchlak q d et al effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists a retrospective multireader multicase study the lancet digital health 3 496 506 2021 pmid 34219054 6 morid m a borjali a del fiol g a scoping review of transfer learn- ing research on medical image analysis using imagenet computers in biology and medicine 128 104115 2021 httpsdoiorg101016jcompbiomed2020 104115 7 kim h e cosa-linan a santhanam n et al transfer learning for medical image classification a literature review bmc medical imaging 22 69 2022 httpsdoiorg101186s12880-022-00793-7 8 zbontar j jing l misra i et al barlow twins self-supervised learning via redundancy reduction in proceedings of the international conference on machine learning pmlr pp 1231012320 2021 httpsdoiorg1048550 arxiv210303230 9 deng j dong w socher r li l-j li kai fei-fei li imagenet a large- scale hierarchical image database in proceedings of the 2009 ieee conference on computer vision and pattern recognition miami fl usa ieee pp 248255 2009 httpsdoiorg101109cvpr20095206848 10 nguyen h q lam k le l t et al vindr-cxr an open dataset of chest x-rays with radiologists annotations sci data 9 429 2022 httpsdoiorg10 1038s41597-022-01498-w 17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16}),\n",
       " Document(page_content='11 he k zhang x ren s sun j deep residual learning for image recognition in proceedings of the 2016 ieee conference on computer vision and pattern recognition cvpr las vegas nv usa pp 770778 2016 httpsdoiorg 101109cvpr201690 12 rajpurkar p irvin j zhu k et al chexnet radiologist-level pneumonia detection on chest x-rays with deep learning arxiv preprint arxiv171105225 2017 httpsdoiorg1048550arxiv171105225 13 sun k x cong c research on chest abnormality detection based on improved yolov7 algorithm in proceedings of the 2022 ieee international conference on bioinformatics and biomedicine bibm las vegas nv usa pp 38843886 2022 httpsdoiorg101109bibm5562020229995687 14 ga al g maga b luk acs a attention u-net based adversarial architectures for chest x-ray lung segmentation arxiv preprint arxiv200310304 2020 https doiorg1048550arxiv200310304 15 shurrab s duwairi r self-supervised learning methods and applications in medical imaging analysis a survey peerj computer science 8 1045 2022 https doiorg107717peerj-cs1045 16 bengio y lamblin p popovici d et al greedy layer-wise training of deep networks in proceedings of the 19th international conference on neural infor- mation processing systems nips06 cambridge ma usa pp 153160 2006 httpsdoiorg10555529764562976476 17 pathak d krahenbuhl p donahue j et al context encoders feature learning by inpainting in proceedings of the ieee conference on computer vision and pattern recognition las vegas nv usa pp 25362544 2016 httpsdoiorg101109cvpr2016278 18 chen t kornblith s norouzi m et al a simple framework for contrastive learning of visual representations in proceedings of the international conference on machine learning pp 15971607 2020 httpsdoiorg1055553524938 3525087 19 gidaris s singh p komodakis n unsupervised representation learning by predicting image rotations arxiv preprint arxiv180307728 2018 httpsdoi org1048550arxiv180307728 20 azizi s mustafa b ryan f et al big self-supervised models advance medical image classification in proceedings of the ieeecvf international conference on computer vision montreal qc canada pp 34783488 2021 httpsdoi org101109iccv48922202100346 21 sowrirajan h yang j ng a y rajpurkar p moco pretraining improves 18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17}),\n",
       " Document(page_content='representation and transferability of chest x-ray models in medical imaging with deep learning pp 728744 2021 httpsdoiorg1048550arxiv201005352 22 morid m a borjali a del fiol g a scoping review of transfer learn- ing research on medical image analysis using imagenet computers in biology and medicine 128 104115 2021 httpsdoiorg101016jcompbiomed2020 104115 23 matsoukas c haslum j sorkhei m soderberg m smith k what makes transfer learning work for medical images feature reuse other factors in proceedings of the 2022 ieeecvf conference on computer vision and pattern recognition cvpr new orleans la usa pp 92159224 2022 httpsdoi org101109cvpr52688202200901 24 wang x peng y lu l et al chestx-ray8 hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases in proceedings of the 2017 ieee conference on computer vision and pattern recognition cvpr honolulu hi usa pp 34623471 2017 httpsdoiorg101109cvpr2017369 25 solovyev r wang w gabruseva t weighted boxes fusion ensembling boxes from different object detection models image and vision computing 107 104117 2021 httpsdoiorg101016jimavis2021104117 26 he k fan h wu y et al momentum contrast for unsupervised visual representation learning in proceedings of the ieeecvf conference on com- puter vision and pattern recognition seattle wa usa pp 97299738 2020 httpsdoiorg101109cvpr42600202000975 27 girshick r fast r-cnn in proceedings of the ieee international conference on computer vision santiago chile pp 14401448 2015 httpsdoiorg10 1109iccv2015169 28 lin t y doll ar p girshick r et al feature pyramid networks for object detection in proceedings of the 2017 ieee conference on computer vision and pattern recognition cvpr honolulu hi usa pp 936944 2017 https doiorg101109cvpr2017106 29 chen k wang j pang j cao y et al mmdetection open mmlab detec- tion toolbox and benchmark arxiv preprint arxiv190607155 2019 https doiorg1048550arxiv190607155 30 bachman p hjelm r d buchwalter w learning representations by maxi- mizing mutual information across views in proceedings of the 33rd international conference on neural information processing systems red hook ny usa pp 1553515545 2019 httpsdoiorg10555534542873455679 19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18}),\n",
       " Document(page_content='31 kornblith s shlens j le q v do better imagenet models transfer better in proceedings of the 2019 ieeecvf conference on computer vision and pattern recognition cvpr long beach ca usa pp 26562666 2019 httpsdoi org101109cvpr201900277 32 sechidis k tsoumakas g vlahavas i on the stratification of multi-label data in gunopulos d hofmann t malerba d vazirgiannis m eds machine learning and knowledge discovery in databases pp 145158 springer berlin 2011 33 szyma nski p kajdanowicz t a network perspective on stratification of multi- label data proceedings of the first international workshop on learning with imbalanced domains theory and applications 2017 httpsdoiorg1048550 arxiv170408756 34 van ryn m burke j the effect of patient race and socio-economic status on physicians perceptions of patients social science medicine 50 813828 2000 pmid 10695979 35 waite s scott j colombo d narrowing the gap imaging disparities in radiology radiology 299 2735 2021 pmid 33560191 20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # For regular expressions\n",
    "from nltk.corpus import stopwords  # You'll need to install NLTK first\n",
    "import copy\n",
    "def clean_text(text):\n",
    "  text = text.lower()  # Convert to lowercase\n",
    "  text = re.sub(r'[^a-z0-9\\s-]', '', text)  # Remove non-alphanumeric characters (except space and dash)\n",
    "  text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single spaces\n",
    "  return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "  for doc in documents:\n",
    "    # Apply basic cleaning\n",
    "    cleaned_text = clean_text(doc.page_content)\n",
    "    # Optionally remove stop words\n",
    "    # cleaned_text = remove_stop_words(cleaned_text)  # Uncomment if you want stop word removal\n",
    "    doc.page_content = cleaned_text\n",
    "  return documents\n",
    "documents_copy = copy.deepcopy(documents)\n",
    "preprocessed_documents = preprocess_documents(documents_copy)\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='barlowtwins-cxr enhancing chest x-ray based abnormality localization with self-supervised learning haoyue sheng123 linrui ma12 jean-fran cois samson3 dianbo liu24 1d epartement dinformatique et de recherche op erationnelle universit e de montr eal 2920 chemin de la tour montr eal h3t 1j4 qc canada 2mila - quebec ai institute 6666 rue saint-urbain montr eal h2s 3h1 qc canada 3direction des ressources informationnelles ciusss du centre-sud-de-l ile-de-montr eal 400 blvd de maisonneuve ouest montr eal h3a 1l4 qc canada 4school of medicine and college of design and engineering national university of singapore 21 lower kent ridge rd singapore 119077 sg singapore corresponding authors e-mails haoyueshengumontrealca contributing authors linruimaumontrealca jean-francoissamsonccsmtlssssgouvqcca dianbonusedusg abstract background chest x-ray imaging based abnormality localization essential in diagnosing various diseases faces significant clinical challenges due to complex interpretations and the growing workload of radiologists recent advances in deep learning especially self-supervised learning offer promising solutions to enhance image analysis efficiency accuracy and reliability this study aims to improve autonomic abnormality localization performance of chest x-ray image analysis particularly in detecting abnormalities using a self- supervised learning method called barlowtwins-cxr methods we utilized two publicly available datasets the nih chest x-ray dataset and the vindr-cxr the barlowtwins-cxr approach was conducted in a two-stage training process initially self-supervised pre-training was performed using an adjusted barlow twins algorithm on the nih dataset with a resnet50 1', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 0}),\n",
       " Document(page_content='backbone pre-trained on imagenet this was followed by supervised fine-tuning on the vindr-cxr dataset using faster r-cnn with feature pyramid network fpn the study employed mean average precision map at an intersection over union iou of 50 and area under the curve auc for performance evaluation results our experiments showed a significant improvement in model perfor- mance with barlowtwins-cxr the approach achieved a 3 increase in map50 accuracy compared to traditional imagenet pre-trained models in addition the ablation cam method revealed enhanced precision in localizing chest abnormal- ities the study involved 112120 images from the nih dataset and 18000 images from the vindr-cxr dataset indicating robust training and testing samples conclusion barlowtwins-cxr significantly enhances the efficiency and accu- racy of chest x-ray image base abnormality localization outperforming tradi- tional transfer learning methods its ability to adapt to various imaging conditions and regional variations demonstrates the potential of self-supervised learning in medical diagnostics this approach can be instrumental in aiding radiologists particularly in high-workload environments offering a promising direction for future ai-driven healthcare solutions keywords medical image analysis chest x-ray abnormality localization deep learning object detection self-supervised learning transfer learning heat map area under curve mean average precision 1 introduction chest x-raycxr is a fundamental and widespread medical diagnostic tool for diag- nosing chest diseases it is efficient and cost-effective suitable for preliminary screening and diagnosis 1 during the 2019 coronavirus pandemic cxr was widely used for triaging patients and prioritizing the care order due to its convenience and flexibility effective mitigation addresses the lack of availability of computed tomography and reduces the risk of transmission in the room with the ct scanner 2 however its com- plex interpretation often requires a highly qualified radiologist to make an accurate diagnosis 1 as the demand for healthcare increases the workload of radiologists has significantly increased 3 it results in less time to analyze each radiographic image potentially increasing the risk of diagnostic error in many areas especially in develop- ing and remote areas qualified radiologists are insufficient to cope with the increased demand for healthcare for instance europe has 13 radiologists per 100000 people while the united kingdom has 85 and malaysia has approximately 30 per million population 4 this situation necessitates urgently developing and introducing auto- mated technologies like ai-based image analysis tools to aid radiologists in quicker and more precise cxr image analysis it will improve the quality of diagnosis and help reduce the workload of doctors in recent years deep learning models have rapidly advanced in various medi- cal image analysis fields of cxr demonstrating diagnostic accuracy comparable to human experts 5 object detection plays a more critical role in medical image anal- ysis because it can identify and precisely locate the types of anomalies in the images 2', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 1}),\n",
       " Document(page_content='providing doctors with more specific and valuable information however training these models requires a large amount of annotated data these annotations must be performed by experienced radiologists for cxr images as well as for most medi- cal images making such annotated data not only costly but also rare with only a very limited number of public datasets including bounding box information although transfer learning is widely regarded as an effective method to solve the problem of scarce labelling data its application in medical image analysis still faces limita- tions this is mainly due to the significant difference in feature distribution between large datasets such as imagenet used for pre-training models and medical imaging datasets this disparity suggests that directly applying these pre-trained weights to medical image analysis might not yield the best outcomes particularly for specialized medical diagnostic applications 67 to fill these gaps our study proposed a novel method namely barlowtwins-cxr employing a dual-phase training process to enhance cxr image analysis the first phase involves unsupervised pre-training using a barlow twins algorithm 8 on cxr images without annotation starting with an imagenet 9 pre-trained model as the foundation in the second phase transfer learning on the vindr-cxr 10 dataset is applied to fine-tune the model our experiments show that such a training strategy combining self-supervised pre-training and supervised fine-tuning is particularly effec- tive in our experiments while employing resnet50 11 as the backbone architecture we observed that implementing the barlowtwins-cxr strategy significantly improved model performance we observed a 3 increase in model accuracy on the mean average precision benchmark surpassing the results achieved by directly performing conventional transfer learning from imagenet pre-trained weights this study extends the application of self-supervised learning to chest x-ray abnor- mality localization it demonstrates the potential of self-supervised learning in medical imaging analysis especially in the absence of annotated data by effectively improving detection performance and precisely localizing abnormalities barlowtwins-cxr rep- resents a significant advancement in the field of cxr abnormality localization paving the way for more efficient and accurate diagnostic methods in the future 2 related work in recent years deep learning techniques have excelled in the field of medical imaging particularly in analyzing cxr images for example in terms of disease classifica- tion chexnet proposed by pranav rajpurkar et al 12 outperformed radiologists in detecting chest diseases when benchmarked against the f1 score neural network models trained with vast amounts of labelled data are capable of identifying features of various pulmonary diseases in anomaly detection tasks sun k x et al used the yolov7 object detection framework to effectively identify and locate lesions in cxr images 13 this achievement is attributed to the advanced image recognition and feature extraction capabilities of neural networks additionally the modified u-net architecture which incorporates attention mechanisms as proposed by guszt av ga al et al 14 has made significant strides in accurately segmenting lung structures thus aiding in detailed analysis and diagnosis of diseases 3', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 2}),\n",
       " Document(page_content='self-supervised learning has recently gained popularity in the field of medical imag- ing 15 and provides an efficient method for utilizing unlabeled data initially proposed by bengio et al this approach allows models to learn from unlabeled data and extract useful feature representations by training deep networks on unsupervised data 16 such learning strategy promotes models to capture the intrinsic structure and rela- tionships in data by designing innovative pretext tasks such as image reconstruction eg context encoder 17 contrastive learning eg simclr 18 or prediction tasks eg rotation prediction 19 in the field of medical imaging shekoofeh azizi et al used large-scale images for self-supervised learning to improve accuracy and con- vergence speed significantly in downstream tasks achieving better performance than models pre-trained on imagenet 20 sowrirajan h et al proposed a pre-trained model based on momentum contrast to enhance the representativeness and portability of cxr models 21 in terms of transfer learning applying models trained in one domain to another has led to notable success in medical image analysis research indicates that well-processed transfer results from imagenet can improve model performance in the medical imag- ing domain 22 however studies by christos matsoukas et al have shown that due to the significant difference in feature distribution between medical and natural images features learned from natural images may not always be broadly applicable to med- ical images 23 various cross-domain adaptive transfer learning methods have been developed to address these challenges such as unsupervised and semi-supervised learn- ing and sequential domain adaptation techniques by tuning model parameters these methods can be better adapted to the characteristics of medical images improving the performance and accuracy of models in medical image analysis 22 3 methods 31 dataset selection this study utilized two publicly available chest x-ray datasets the nih-cxr24 dataset and the vindr-cxr dataset the nih dataset comprises 112120 posterior- anterior pa or anterior-posterior ap cxr images from 30805 patients covering 14 diseases with image-level annotations including disease location annotations in some images the distribution of the nih-cxr dataset is illustrated in figure 1 meanwhile the vindr-cxr dataset is the largest publicly available dataset for adult cxr object detection which includes 18000 pa cxr scans these scans encom- pass 14 diseases with detailed instance-level bounding box annotations making it ideal for the fine-tuning phase the vindr-cxr dataset exhibits a distinct labelling process for its test and train- ing sets the training set consisting of 15000 images was annotated independently by three radiologists per image in contrast the test set comprising 3000 images underwent a more rigorous annotation process initially each image was independently annotated by three radiologists this is followed by a secondary review phase where these initial annotations are reviewed by two other more experienced radiologists they communicated with each other to resolve any disagreements and reach a consensus on 4', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 3}),\n",
       " Document(page_content='countdiseaseabnormalityno finding infiltration effusion atelectasis nodule mass pneumothorax consolidation pleuralthickening cardiomegaly emphysema edema fibrosis pneumonia hernia 0 10000 20000 30000 40000 50000 60000 70000fig 1 image-level label distribution of the nih-cxr dataset the final labelling this meticulous process for the test set created a potential dispar- ity in data distribution compared to the training set to eliminate any bias it might introduce in our study we resplit the original training set into new training validation and test sets for our experiments to improve the quality of the training data a weighted box fusion wbf 25 pre- processing technique was applied to the vindr-cxr training set the wbf involves calculating the weighted average of each set of duplicate bounding boxes to create a single fused bounding box such a preprocessing step is crucial for reducing annota- tion redundancy and improving target area representation in the dataset figure 2 shows the data distribution of vindr-cxr before and after wbf preprocessing we chose the vindr-cxr dataset not only because it is the largest publicly avail- able dataset for adult cxr object detection but also because of the high level of diversity and richness of its data 32 dual-phase training process our training encompasses two primary phases self-supervised pre-training and sub- sequent supervised fine-tuning initially we commenced with a resnet50 model pre-trained on imagenet as shown in figure 3 in the self-supervised pre-training phase we applied a modified barlow twins method to the nih-cxr dataset this approach refined the imagenet pre-trained model by updating its backbone weights subsequently in the supervised fine-tuning phase we utilize this refined backbone within a faster r-cnn framework by applying it to the vindr-cxr dataset this step aims to further improve the models task-specific performance explicitly enhancing its capabilities in localized diseases in cxr images 5', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 4}),\n",
       " Document(page_content='a bfig 2 instance-level annotation distribution of vindr-cxr dataset beforea and afterb wbf preprocessing 321 self-supervised pre-training for the first stage of training we employed the original version of the barlow twins method as mentioned in zbontar et al 8 this approach represents a shift from conventional contrastive learning introducing a self-supervised learning framework primarily focused on diminishing redundancy the barlow twins method operates on a straightforward yet potent principle it learns distinctive features by reducing the representational differences between two differently distorted images from the same source as processed by the network this strategy is instrumental in enabling the model to identify unique and rich features in each image while concurrently minimizing the overlap in features the process involves generating two distinct variants of an image through data augmentation followed by their simultaneous processing via two deep 6', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 5}),\n",
       " Document(page_content='input imagec2 c3 c4 c5 p2 p3 p4 p5resnet50 backbone convrpncls rpnreg roi poolingregion proposal network fc fc fcfeature map feature vectorfeature mapclass boxfeature pyramid networkx y ay b imagesdistorted images z a z bresnet 50 resnet 50net embeddings emporocal cross-corr target cross-corr i lbt feature dimensionbarlow twins faster r-cnnfig 3 schematic overview of the dual-phase training framework the upper panel illustrates the barlow twins method in phase one where pairs of distorted images are processed through a shared resnet50 network to produce embeddings these are then compared using an empirical cross-correlation matrix c striving for the identity matrix i to minimize redundancy in feature dimensions and optimizing the loss function l bt in phase two lower panel the pre-trained resnet50 backbone from phase one is integrated into a faster r-cnn architecture it starts with multi-scale feature extraction through the feature pyramid network fpn followed by the region proposal network rpn that generates object region proposals the features are then pooled and processed by fully connected fc layers to output the final class labels and bounding box coordinates for object detection tasks neural networks that share identical weights the objective is to align the networks weights to enhance the similarity in the high-level representations of these image pairs yet ensure that the individual features remain distinct and independent the barlow twins method might be particularly useful for medical imaging because it extracts features by minimizing the redundancy between representations of perturbed images in cxr imaging subtle differences might indicate important health information and the barlow twins can effectively capture these subtle but clinically important features in contrast to other contrastive learning algorithms like moco 26 and simclr which construct similarity matrices at the batch dimension barlow twins works at the feature dimension it aims to assign an independent meaning to each feature dimension this could lead to a richer feature representation potentially better adapted to variations in cxr images eg different imaging conditions and 7', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 6}),\n",
       " Document(page_content='pathological states moreover compared to self-supervised learning methods requir- ing negative samples or complex contrastive mechanisms like simclr barlow twins offers a more straightforward training framework which is particularly important in situations with limited computational resources we chose to apply barlow twins pre-training on the imagenet pretrained resnet50model since the imagenet pre-trained model weights can be easily obtained from the torchvision library this step brings no additional cost we used images from the training set portion of the nih-cxr dataset for this training phase with the input image size set to 224224 pixels the training was executed on an nvidia a100 80g gpu setting the batch size to 768 to maximize the utilization of this graphics cards capabilities over 600 epochs 322 fine-tuning phase in our fine-tuningtransfer learning stage we utilized the faster r-cnn 27 with feature pyramid network fpn 28 as our object detector and trained it on the vindr-cxr dataset faster r-cnn a widely-used object detection framework com- prises two main components the region proposal network rpn 28 and the fast r-cnn detector first rpn generates candidate regions for objects and then the fast r-cnn detector employs these regions to detect and classify targets this architecture renders faster r-cnn particularly efficient in processing complex images the feature pyramid network fpn an architecture frequently employed in object detection particularly enhances performance with multi-scale targets it integrates high-level semantic information from deeper layers with detailed information from shallower lay- ers producing feature maps of varied scales that effectively detect differently sized targets we employed the mmdetection 29 machine learning toolbox as the platform for faster r-cnn utilizing a number of classical image augmentation techniques and maintaining consistent hyperparameters across all experiments two different input sizes 224224 pixels and 640640 pixels were chosen to assess the impact of image size on the models performance with the pre-trained models in addition for comparison we also conducted experiments using imagenet pre-trained weights directly we implemented a linear evaluation protocol 3031 on the nih-cxr dataset to comprehensively evaluate the self-supervised learning models performance in medical imaging this method examines the models feature transfer capability - its ability to adapt learned representations to new tasks we first resplit the test set of the nih dataset into two parts 80 as an evaluation training set for training a linear classifier and the remaining 20 as an evaluation test set for assessing model performance we adopted two distinct strategies during the evaluation freezing the backbone weights or fine-tuning the weights in the freezing backbone strategy we kept the parameters of the backbone network ie the feature extraction layers obtained from self-supervised pretraining unchanged we updated only the weights of the final lin- ear layer conversely under the fine-tuning strategy we updated parameters across the entire network encompassing both the self-supervised trained feature extraction layers and the newly added linear classifier layer we used 100 10 and 1 of the 8', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 7}),\n",
       " Document(page_content='evaluation training set data for training the linear classifier allowing us to assess the models performance across different scales of training data when evaluating the representation transfer ability of a self-supervised learning model it is necessary to ensure that the ratio of individual labels in the training and test sets is consistent we used the iterative stratification for the multi-label data method 3233 to ensure that the proportions of each label in the evaluation training and test sets were roughly similar this helped prevent biases due to uneven label distribution making our evaluation results more reliable and convincing 33 results analysis process for the analysis of results we employed the mean average precision map at an intersection over union iou of 50 as the benchmark for evaluating the performance of our object detection models map is a widely recognized and effective metric in object detection calculated by averaging precision scores across various object detec- tion confidence thresholds specifically map is the mean of the average precision scores for each class the proportion of correct predictions relative to all predictions for a specific class across different detection confidence thresholds determines the pre- cision score in the context of cxr abnormality localization utilizing map at an iou of 50 is beneficial for capturing clinically significant lesion detections while allowing for a reasonable degree of positional deviation which is practical for actual clinical applications moreover we utilized the area under the curve auc as a metric for the lin- ear evaluation protocol auc a standard metric in medical image analysis balances precision and recall making it an especially appropriate performance indicator for this field the auc metric represents the area under the receiver operating char- acteristic roc curve accounting for the models true positive rate tpr and false positive rate fpr at various thresholds this assessment method balances the models sensitivity and specificity enhancing detection rates while controlling false positives medical image analysis often deals with imbalanced data and auc is robust for imbalanced datasets as it does not rely directly on classification thresholds beyond using map and auc for quantitative analysis our study also utilized the ablation cam class activation mapping method to create heat maps for qualitative evaluation ablation cam systematically abates features in the models final convo- lutional layer and observes the impact on the output class scores this process reveals the most influential regions for the models decision-making the resulting heat maps delineate areas of interest in cxr images providing intuitive visual evidence of how our barlowtwins-cxr model focuses on and recognizes abnormalities 4 results 41 transfer learning on vindr abnormality localization in this experiment we examined the efficacy of the resnet backbone pre-trained by the barlow twins-cxr method for abnormality localization on the vindr-cxr dataset 9', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 8}),\n",
       " Document(page_content='using two different input resolutions consistent hyperparameter settings were main- tained across all experiments ensuring that the performance changes were attributable only to the merits of the pretraining method itself we visualized the performance of different models such as barlow twins-cxr pre-training and imagenet pre-training on the validation set in figure 4 and tabulated the corresponding map performance in table 1 as depicted in the figure the baseline model with an untrained resnet50 backbone reached a final map50 score of 01342 95 ci 0130601378 setting a performance baseline without pre-training benefits a b fig 4 evolution of map50 across epochs for different resnet50 backbones on the vindr-cxr dataset at 224224left and 640640right resolution the darker lines represent the average map50 of fourleft and fiveright trials with different random seeds with shaded areas indicating the range between the lowest and highest value table 1 map50 scores in validation and test sets for models with varying pre-training methods at different input resolutions backbone weight input size map50 val set map50 test set baseline nopretrained 224 01388 0135201424 01342 0130601378 imagenet pretrained 02245 0220402286 02210 0219402226 barlow twins 02555 0248502626 02448 0241402482 barlow twins from imagenet 02625 0256802682 02502 0247602528 imagenet pretrained 640 02973 0291303033 0280 0275702848 barlow twins from imagenet 03102 0308003125 0289 0282602954 1scores are presented with 95 confidence intervals a significant advancement was observed with the imagenet pre-trained resnet50 which attained a map50 of 02210 95 ci 0219402226 underscoring the value of pre-training in feature representation across disparate image domains 10', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 9}),\n",
       " Document(page_content='more strikingly incorporating the barlow twins-cxr strategy led to a rapid per- formance ascent achieving a map50 of 02448 95 ci 02414 02482 it marked an expedited training trajectory and a significant increase in detection performance when further enhanced by pre-training from imagenet the barlow twins-cxr approach yielded the best performance recording a map of 02502 95 ci 02476 02528 evidencing the synergetic effect of combining pre-training methodologies the heat maps generated from the study present a compelling visualization of the performance of the barlowtwins-cxr method compared to the traditional imagenet weights approach we generated heat maps of the first few cxr images of the train- ing and test sets in figure 5 in each image our methods heat maps show a more focused alignment with the actual lesion areas marked by the ground truth bbox this indicates a higher precision in localizing and identifying pathological features with barlowtwins-cxr potentially offering more targeted information for clinical diagnoses notably in cases of cardiomegaly and lung opacity the concentration and localization of the heatmaps from barlowtwins-cxr are visibly superior to those derived from imagenet weights further affirming the efficacy of our approach in enhancing cxr image analysis upon escalating the input resolution to 640 640 pixels both imagenet and bar- low twin-cxr weighted models saw performance improvements due to the increased detail in the cxr images nonetheless the performance differential between the two narrowed indicating that the higher resolution somewhat mitigates the distinct advantages of self-supervised pre-training this points to intriguing future research avenues such as refining image resolu- tion parameters during pre-training and fine-tuning phases and investigating whether higher-resolution pre-training could elevate model performance it also accentuates the necessity of tailoring deep learning model design to specific tasks considering factors like image resolution and feature granularity overall implementing the barlow twins-cxr method on the vindr dataset resulted in substantial gains despite its data limitations and the inherent challenges of cxr abnormality localization an 115 performance enhancement over the base- line and a 28 increment over imagenet pre-trained models were observed on the map50 metric such marked improvements confirm the barlow twins-cxr strategys prowess in addressing domain inconsistencies thereby fine-tuning naturally derived image weights for better applicability in cxr image analysis and beyond in medical imaging 42 linear evaluation protocol in this experiment we evaluated the impact of barlow twins-cxr pre-training versus traditional imagenet pre-training on the linear classification performance within the nih-cxr dataset we adhered to the linear evaluation protocol freezing the backbone of the linear classifier and updating only the final linear layers weights this approach was applied across training datasets of varying sizes - 1 10 and 100 results of these experiments are presented in figure 6 and table 2 the results show that at a training data size of 1 the barlow twins-cxr pre- trained model demonstrated a significant advantage achieving an auc of 06586 95 11', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 10}),\n",
       " Document(page_content='ground truth bbox imagenet weights our method 20e27597c972c6e7fdb4d1e7638e227e 03431b577d1ccf075e930c4c4913c079 fd810298e165ef0b9a88bb25fda7a34bground truth bbox imagenet weights our method 9eba0d101f410f9cdfae46cb094ae2a6 87a8df2f22475c7200ebe891d0f25b88 ad86f42123384e2441cce36347aa7d1aa bfig 5 heatmaps were generated from the initial images of the training setleft and test setright indicating successful bbox predictions by the barlowtwins-cxr model each heatmap corresponds to one accurately predicted bbox despite multiple bboxes present in each cxr image serial numbers below the heatmaps refer to the image numbers in the dataset table 2 auc scores in validation and test sets for of linear models with varying pre-training methods at 224 and 640 input resolutions model 1 10 100 barlowtwin-cxr 06586 06556 06616 07773 07756 07790 08031 08027 08035 image-net 05932 05913 05951 06855 06822 06889 07098 07089 07107 1scores are presented with 95 confidence intervals ci 0655606616 compared to 05932 95 ci 0591305951 for the imagenet pre- trained model as the training data size increased to 10 and 100 the aucs for the barlow twins-cxr pre-trained model reached 07773 95 ci 0775607790 and 08031 95 ci 0802708035 respectively while the imagenet pre-trained model scored 06855 95 ci 0682206889 and 07098 95 ci 0708907107 notably the incremental gains for both pre-training methods diminished with larger data sizes suggesting that the performance boost provided by additional data becomes marginal when only the linear layer is updated these findings highlight the barlow twins-cxr pre-training methods superiority over imagenet pre-training across various dataset sizes especially in data-limited sce- narios this demonstrates the promise of self-supervised learning in enhancing medical image analysis particularly when annotated data is scarce 12', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 11}),\n",
       " Document(page_content='fig 6 auc scores with error bars for nih-cxr classification - this figure displays the auc scores of linear models with barlow twins-cxr versus imagenet weights across various dataset sizes 1 10 100 as indicated by higher auc scores models using barlow twins-cxr consistently outperform those with imagenet pre-training error bars represent the range of scores across five experiments 43 end-to-end finetuning in our end-to-end experiments where we permitted updates to all model layers the barlow twins-cxr pre-trained resnet50 backbone consistently outperformed the imagenet pre-trained equivalent across all training set sizes the results of these experiments are presented in figure 7 and table 3 fig 7 auc scores with error bars for nih-cxr classification - this figure displays the auc scores of models fine-tuned end-to-end with barlow twins-cxr versus imagenet weights across various dataset sizes 1 10 100 higher auc scores indicate that models using barlow twins-cxr consistently outperform those with imagenet pre-training error bars represent the range of scores across five experiments 13', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 12}),\n",
       " Document(page_content='table 3 auc scores in validation and test sets for models fine-tuned end-to-end with varying pre-training methods at 224 and 640 input resolutions model 1 10 100 barlowtwin-cxr 06585 06544 06627 07756 07745 07768 08107 08098 08116 image-net 06163 06110 06216 07168 07093 07243 07866 07843 07889 1scores are presented with 95 confidence intervals at a 1 training data size the barlow twins-cxr model achieved a 42 higher auc than the imagenet counterpart with 10 and 100 data sizes the barlow twins-cxr model maintained leads of approximately 59 and 25 respectively notably the magnitude of improvement over the frozen backbone setup was less marked suggesting that the wealth of features learned during self-supervised training reduces the margin for additional gains during subsequent fine-tuning overall these end-to-end fine-tuning results suggest that comprehensive learning across all model layers may elevate the risk of overfitting particularly when data is scarce the narrowing performance differential between the two pre-training strategies with increasing data volume indicates that the distinction between domain-specific barlow twins-cxr and generalized imagenet pre-training becomes less substan- tial with larger datasets this trend implies that the influence of the pre-training strategy on the final performance of models may diminish as the size of the medical image dataset grows 5 discussion our study demonstrates that the barlowtwins-cxr approach effectively utilizes unannotated cxr images for learning valuable representations and enhances trans- fer learning efficiency from imagenet thus addressing issues of domain inconsistency this leads to quicker training and improved performance on tasks like abnormality detection in the vindr-cxr dataset barlow twins-cxr excels across various input resolutions outshining models pre-trained on imagenet one of the primary limitations of our study is the scarcity of cxr datasets with bounding box our reliance on public datasets due to the absence of a private dataset may limit the generalizability of our findings additionally the computational cost of the barlowtwins pre-training remains substantial for a dataset size of 112120 images with an image size of 224224 pixels the training process required two days on an nvidia a100 80g gpu this significant resource requirement constrained our ability to experiment with higher image resolutions which could potentially enhance the models performance 6 future work our future endeavours include developing a demo interactive system for deployment and testing in emergency rooms it will allow practical evaluation of the models effectiveness in a clinical setting and facilitate the collection of a proprietary dataset 14', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 13}),\n",
       " Document(page_content='additionally we plan to explore more advanced self-supervised learning methods object detection frameworks and backbone networks to refine our approach further the continuous evolution of these technologies promises to address some of the current limitations and expand the applicability and accuracy of our model in medical image analysis 7 conclusions the results of this study provide strong support for the application of self-supervised learning in the field of abnormality detection especially valuable in environments where radiologists face high workloads but the corresponding data labelling resources are scarce a critical aspect of this approach is its adaptability to regional variations in cxr image attributable to differences in imaging equipment patient demographics and other locale-specific factors 3435 such variations often impede the cross- regional applicability of a model thus limiting its generalizability by employing the barlowtwins-cxr strategy research organizations can transfer pre-trained backbone networks to local datasets tailored to the unique characteristics of their regional data our findings might also have significant implications for clinical practice suggest- ing that this strategy could be a game-changer in aiding radiologists to interpret cxr images efficiently this technology promises to reduce diagnostic times poten- tially increasing patients throughput and improving the overall quality of care given its capacity for fine-tuning to specific regional characteristics our approach holds particular promise in areas where standardization of medical imaging presents challenges in summary the barlowtwins-cxr approach demonstrates the potential of ai to enhance healthcare delivery by integrating cutting-edge technology with clini- cal needs we aim to pave the way for innovative solutions that benefit healthcare professionals and patients 8 abbreviations ap anterior-posterior auc area under the receiver operating characteristic curve cam class activation mapping ciusss centre int egr e universitaire de sant e et de services sociaux cxr chest x-ray radiography fc fully connected layer fpn feature pyramid network fpr false positive rate iou intersection over union roc receiver operating characteristic roi region of interest map mean average precision pa posterior-anterior tpr true positive rate 15', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 14}),\n",
       " Document(page_content='wbf weighted box fusion yolo you only look once 9 declarations 91 ethics approval and consent to participate all methods were performed under relevant guidelines and regulations eg decla- rations of helsinki the studies reported in this manuscript used reputable public datasets and did not require any additional data involving human participants human data or human tissue 92 consent for publication not applicable 93 availability of data and materials the datasets generated andor analysed during the current study are available in the vindr-cxr 10 and nih-cxr24 repository vindr-cxr and nih-cxr 94 competing interests the authors declare that they have no competing interests 95 funding no external funding was associated with this research study 96 authors contributions hs designed the research methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project their expertise and insights have been greatly appreciated and substantially enhanced this works quality 16', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 15}),\n",
       " Document(page_content='references 1 satia i bashagha s bibi a et al assessing the accuracy and certainty in interpreting chest x-rays in the medical division clinical medicine 13 349352 2013 pmid 23908502 2 rubin g d ryerson c j haramati l b et al the role of chest imaging in patient management during the covid-19 pandemic a multinational consensus statement from the fleischner society radiology 296 172180 2020 pmid 32275978 3 lantsman d c barash y klang e guranda l konen e tau n trend in radiologist workload compared to number of admissions in the emer- gency department european journal of radiology 149 110195 2022 pmid 35149337 4 httpswwwrsnaorgnews2022mayglobal-radiologist-shortage accessed date-of-access 2022 5 seah j c y tang c h m buchlak q d et al effect of a comprehensive deep-learning model on the accuracy of chest x-ray interpretation by radiologists a retrospective multireader multicase study the lancet digital health 3 496 506 2021 pmid 34219054 6 morid m a borjali a del fiol g a scoping review of transfer learn- ing research on medical image analysis using imagenet computers in biology and medicine 128 104115 2021 httpsdoiorg101016jcompbiomed2020 104115 7 kim h e cosa-linan a santhanam n et al transfer learning for medical image classification a literature review bmc medical imaging 22 69 2022 httpsdoiorg101186s12880-022-00793-7 8 zbontar j jing l misra i et al barlow twins self-supervised learning via redundancy reduction in proceedings of the international conference on machine learning pmlr pp 1231012320 2021 httpsdoiorg1048550 arxiv210303230 9 deng j dong w socher r li l-j li kai fei-fei li imagenet a large- scale hierarchical image database in proceedings of the 2009 ieee conference on computer vision and pattern recognition miami fl usa ieee pp 248255 2009 httpsdoiorg101109cvpr20095206848 10 nguyen h q lam k le l t et al vindr-cxr an open dataset of chest x-rays with radiologists annotations sci data 9 429 2022 httpsdoiorg10 1038s41597-022-01498-w 17', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 16}),\n",
       " Document(page_content='11 he k zhang x ren s sun j deep residual learning for image recognition in proceedings of the 2016 ieee conference on computer vision and pattern recognition cvpr las vegas nv usa pp 770778 2016 httpsdoiorg 101109cvpr201690 12 rajpurkar p irvin j zhu k et al chexnet radiologist-level pneumonia detection on chest x-rays with deep learning arxiv preprint arxiv171105225 2017 httpsdoiorg1048550arxiv171105225 13 sun k x cong c research on chest abnormality detection based on improved yolov7 algorithm in proceedings of the 2022 ieee international conference on bioinformatics and biomedicine bibm las vegas nv usa pp 38843886 2022 httpsdoiorg101109bibm5562020229995687 14 ga al g maga b luk acs a attention u-net based adversarial architectures for chest x-ray lung segmentation arxiv preprint arxiv200310304 2020 https doiorg1048550arxiv200310304 15 shurrab s duwairi r self-supervised learning methods and applications in medical imaging analysis a survey peerj computer science 8 1045 2022 https doiorg107717peerj-cs1045 16 bengio y lamblin p popovici d et al greedy layer-wise training of deep networks in proceedings of the 19th international conference on neural infor- mation processing systems nips06 cambridge ma usa pp 153160 2006 httpsdoiorg10555529764562976476 17 pathak d krahenbuhl p donahue j et al context encoders feature learning by inpainting in proceedings of the ieee conference on computer vision and pattern recognition las vegas nv usa pp 25362544 2016 httpsdoiorg101109cvpr2016278 18 chen t kornblith s norouzi m et al a simple framework for contrastive learning of visual representations in proceedings of the international conference on machine learning pp 15971607 2020 httpsdoiorg1055553524938 3525087 19 gidaris s singh p komodakis n unsupervised representation learning by predicting image rotations arxiv preprint arxiv180307728 2018 httpsdoi org1048550arxiv180307728 20 azizi s mustafa b ryan f et al big self-supervised models advance medical image classification in proceedings of the ieeecvf international conference on computer vision montreal qc canada pp 34783488 2021 httpsdoi org101109iccv48922202100346 21 sowrirajan h yang j ng a y rajpurkar p moco pretraining improves 18', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 17}),\n",
       " Document(page_content='representation and transferability of chest x-ray models in medical imaging with deep learning pp 728744 2021 httpsdoiorg1048550arxiv201005352 22 morid m a borjali a del fiol g a scoping review of transfer learn- ing research on medical image analysis using imagenet computers in biology and medicine 128 104115 2021 httpsdoiorg101016jcompbiomed2020 104115 23 matsoukas c haslum j sorkhei m soderberg m smith k what makes transfer learning work for medical images feature reuse other factors in proceedings of the 2022 ieeecvf conference on computer vision and pattern recognition cvpr new orleans la usa pp 92159224 2022 httpsdoi org101109cvpr52688202200901 24 wang x peng y lu l et al chestx-ray8 hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases in proceedings of the 2017 ieee conference on computer vision and pattern recognition cvpr honolulu hi usa pp 34623471 2017 httpsdoiorg101109cvpr2017369 25 solovyev r wang w gabruseva t weighted boxes fusion ensembling boxes from different object detection models image and vision computing 107 104117 2021 httpsdoiorg101016jimavis2021104117 26 he k fan h wu y et al momentum contrast for unsupervised visual representation learning in proceedings of the ieeecvf conference on com- puter vision and pattern recognition seattle wa usa pp 97299738 2020 httpsdoiorg101109cvpr42600202000975 27 girshick r fast r-cnn in proceedings of the ieee international conference on computer vision santiago chile pp 14401448 2015 httpsdoiorg10 1109iccv2015169 28 lin t y doll ar p girshick r et al feature pyramid networks for object detection in proceedings of the 2017 ieee conference on computer vision and pattern recognition cvpr honolulu hi usa pp 936944 2017 https doiorg101109cvpr2017106 29 chen k wang j pang j cao y et al mmdetection open mmlab detec- tion toolbox and benchmark arxiv preprint arxiv190607155 2019 https doiorg1048550arxiv190607155 30 bachman p hjelm r d buchwalter w learning representations by maxi- mizing mutual information across views in proceedings of the 33rd international conference on neural information processing systems red hook ny usa pp 1553515545 2019 httpsdoiorg10555534542873455679 19', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 18}),\n",
       " Document(page_content='31 kornblith s shlens j le q v do better imagenet models transfer better in proceedings of the 2019 ieeecvf conference on computer vision and pattern recognition cvpr long beach ca usa pp 26562666 2019 httpsdoi org101109cvpr201900277 32 sechidis k tsoumakas g vlahavas i on the stratification of multi-label data in gunopulos d hofmann t malerba d vazirgiannis m eds machine learning and knowledge discovery in databases pp 145158 springer berlin 2011 33 szyma nski p kajdanowicz t a network perspective on stratification of multi- label data proceedings of the first international workshop on learning with imbalanced domains theory and applications 2017 httpsdoiorg1048550 arxiv170408756 34 van ryn m burke j the effect of patient race and socio-economic status on physicians perceptions of patients social science medicine 50 813828 2000 pmid 10695979 35 waite s scott j colombo d narrowing the gap imaging disparities in radiology radiology 299 2735 2021 pmid 33560191 20', metadata={'source': '../documents/barlowtwins-CXR.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_copy = copy.deepcopy(documents)\n",
    "preprocessed_documents = preprocess_documents(documents_copy)\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retriever(doc):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "    all_splits = text_splitter.split_documents(doc)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    doc = Chroma.from_documents(all_splits, embeddings)\n",
    "    retriever=doc.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    return retriever\n",
    "retriever_org = make_retriever(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801}),\n",
       " Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801}),\n",
       " Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_org.invoke(\"What is the main topic of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_pre = make_retriever(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801}),\n",
       " Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801}),\n",
       " Document(page_content='methodology analyzed data was responsible for experiments and results visualization and participated in manuscript drafting and revision lm assisted in developing the research methodology and contributed to the drafting and revision of the manuscript jfs collected and interpreted data and provided expertise in statistical analysis dl contributed to the study design offered statistical analysis expertise assisted in interpreting results and played a significant role in the critical revision of the manuscript all authors read and approved the final manuscript 97 acknowledgements the authors wish to express their gratitude to ciusss du centre-sud-de-l le-de- montr eal for the computational resources and support provided which were essential for the research conducted as part of the graduate internship program we are espe- cially thankful to our department director mathieu mailhot for his mentorship and to chen cheng for his collaborative efforts and valuable contributions to this project', metadata={'page': 15, 'source': '../documents/barlowtwins-CXR.pdf', 'start_index': 801})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_pre.invoke(\"What is the main topic of the document?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

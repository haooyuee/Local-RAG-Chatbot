Unnamed: 0,question,answer,contexts,ground_truth,context_precision,faithfulness,answer_relevancy,context_recall,answer_correctness,answer_similarity,rouge1,rouge2,rougeL,rougeLsum
0,What branch of theoretical computer science deals with broadly classifying computational problems by difficulty and class of relationship?, Computational complexity theory.,"['Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm.'
 'Closely related fields in theoretical computer science are analysis of algorithms and computability\ntheory. A key distinction between analysis of algorithms and computational complexity theory is that\nthe former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a\nproblem, whereas the latter asks a more general question about all possible algorithms that could be\nused to solve the same problem. More precisely, it tries to classify problems that can or cannot be'
 'of computation are polynomially related"" (Goldreich 2008, Chapter 1.2). This forms the basis for the\ncomplexity class P, which is the set of decision problems solvable by a deterministic Turing machine\nwithin polynomial time. The corresponding set of function problems is FP.']",Computational complexity theory,0.9999999999666668,1.0,0.9526202296120816,1.0,0.9820676486137552,0.9282705944550202,0.3045392416225749,0.2555555555555555,0.3041570447839918,0.3091810891888773
1,What measure of a computational problem broadly defines the inherent difficulty of the solution?,"  The measure of a computational problem that defines the inherent difficulty of the solution is the time required to solve the problem.




","['A problem is regarded as inherently difficult if its solution requires significant resources, whatever the\nalgorithm used. The theory formalizes this intuition, by introducing mathematical models of\ncomputation to study these problems and quantifying the amount of resources needed to solve them,\nsuch as time and storage. Other complexity measures are also used, such as the amount of\ncommunication (used in communication complexity), the number of gates in a circuit (used in circuit'
 'To measure the difficulty of solving a computational problem, one may wish to see how much time the\nbest algorithm requires to solve the problem. However, the running time may, in general, depend on\nthe instance. In particular, larger instances will require more time to solve. Thus the time required to\nsolve a problem (or the space required, or any measure of complexity) is calculated as a function of\nthe size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is'
 'Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm.']",its solution requires significant resources,0.8333333332916666,1.0,0.9759465156375592,0.2,0.697782875615973,0.7911315024638916,,,,
2,What two fields of theoretical computer science closely mirror computational complexity theory?,  Analysis of algorithms and computability theory.,"['Closely related fields in theoretical computer science are analysis of algorithms and computability\ntheory. A key distinction between analysis of algorithms and computational complexity theory is that\nthe former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a\nproblem, whereas the latter asks a more general question about all possible algorithms that could be\nused to solve the same problem. More precisely, it tries to classify problems that can or cannot be'
 'complexity) and the number of processors (used in parallel computing). One of the roles of\ncomputational complexity theory is to determine the practical limits on what computers can and cannot\ndo.'
 'Analogous definitions can be made for space requirements. Although time and space are the most\nwell-known complexity resources, any complexity measure can be viewed as a computational\nresource. Complexity measures are very generally defined by the Blum complexity axioms. Other\ncomplexity measures used in complexity theory include communication complexity, circuit complexity,\nand decision tree complexity.']",analysis of algorithms and computability theory,0.9999999999,0.5,0.922941019432304,1.0,0.9898173946966746,0.9592695787866984,,,,
3,What is the name given to the input string of a computational problem?,  Problem instance.,"['A computational problem can be viewed as an infinite collection of instances together with a solution\nfor every instance. The input string for a computational problem is referred to as a problem instance,\nand should not be confused with the problem itself. In computational complexity theory, a problem\nrefers to the abstract question to be solved. In contrast, an instance of this problem is a rather\nconcrete utterance, which can serve as the input for a decision problem. For example, consider the'
 'When considering computational problems, a problem instance is a string over an alphabet. Usually,\nthe alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings.\nAs in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For\nexample, integers can be represented in binary notation, and graphs can be encoded directly via their\nadjacency matrices, or by encoding their adjacency lists in binary.'
 'Decision problems are one of the central objects of study in computational complexity theory. A\ndecision problem is a special type of computational problem whose answer is either yes or no, or\nalternately either 1 or 0. A decision problem can be viewed as a formal language, where the members\nof the language are instances whose output is yes, and the non-members are those instances whose\noutput is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a']",a problem instance,0.9999999999666668,,0.8012391542303741,1.0,0.7318969152038581,0.9275876608154324,,,,
4,By how many kilometers does the traveling salesman problem seek to classify a route between the 15 largest cities in Germany?, I don't know the answer to this question as it is not provided in the given text.,"[""To further highlight the difference between a problem and an instance, consider the following instance\nof the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres\npassing through all of Germany's 15 largest cities? The quantitative answer to this particular problem\ninstance is of little use for solving other instances of the problem, such as asking for a round trip\nthrough all sites in Milan whose total length is at most 10 km. For this reason, complexity theory""
 ""with this decision problem is then the set of all connected graphs—of course, to obtain a\nprecisedefinition of this language, one has to decide how graphs are encoded as binary strings.\n\nA function problem is a computational problem where a single output (of a total function) is expected\nfor every input, but the output is more complex than that of a decision problem, that is, it isn't just yes\nor no. Notable examples include the traveling salesman problem and the integer factorization problem.""
 'problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is\na US$1,000,000 prize for resolving the problem.\nIt was shown by Ladner that if P „ NP then there exist problems in NP that are neither in P nor\nNP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem,\nthe discrete logarithm problem and the integer factorization problem are examples of problems']",2000,0.9999999999,,0.0,1.0,0.1830270899566673,0.7321083598266693,,,,
5,"In a computational problem, what can be described as a string over an alphabet?","  The question is asking about the type of string that can be described in a computational problem. The answer is that a string over an alphabet in a computational problem can be a bitstring, which is a string made up of bits (0 or 1).","['When considering computational problems, a problem instance is a string over an alphabet. Usually,\nthe alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings.\nAs in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For\nexample, integers can be represented in binary notation, and graphs can be encoded directly via their\nadjacency matrices, or by encoding their adjacency lists in binary.'
 'A computational problem can be viewed as an infinite collection of instances together with a solution\nfor every instance. The input string for a computational problem is referred to as a problem instance,\nand should not be confused with the problem itself. In computational complexity theory, a problem\nrefers to the abstract question to be solved. In contrast, an instance of this problem is a rather\nconcrete utterance, which can serve as the input for a decision problem. For example, consider the'
 'Decision problems are one of the central objects of study in computational complexity theory. A\ndecision problem is a special type of computational problem whose answer is either yes or no, or\nalternately either 1 or 0. A decision problem can be viewed as a formal language, where the members\nof the language are instances whose output is yes, and the non-members are those instances whose\noutput is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a']",a problem instance,0.9999999999666668,1.0,0.9236365524493412,1.0,0.1898290667657939,0.7593162670631759,,,,
6,What kind of problems are one of the main topics studied in computational complexity theory?,"  Computational complexity theory is a branch of theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. Some of the main topics studied in computational complexity theory include:
1. Time complexity: This refers to the amount of time required to solve a computational problem, usually measured in terms of the number of steps or operations required.
2. Space complexity: This refers to the amount of memory required to solve a computational problem, usually measured in terms of the number of bits or units of storage needed.
3. Parallel complexity: This refers to the amount of resources required to solve a computational problem in parallel computing environments, usually measured in terms of the number of processors or computational units involved.
4. Circuit complexity: This refers to the number of gates or computational steps required to solve a computational problem using a circuit model of computation.
5. Proof complexity: This refers to the number of steps or operations required to prove a mathematical statement or theorem using a computational model of proof.
6. Interactive complexity: This refers to the number of interactions or steps required to solve a computational problem in an interactive computing environment.
7. Quantum complexity: This refers to the amount","['complexity) and the number of processors (used in parallel computing). One of the roles of\ncomputational complexity theory is to determine the practical limits on what computers can and cannot\ndo.'
 'Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm.'
 ""Earlier papers studying problems solvable by Turing machines with specific bounded resources\ninclude John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study\nof rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962).\nSomewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another\nspecific complexity measure. As he remembers:""]",Decision problems,0.5833333333041666,0.5,0.8970237921490286,1.0,0.3587008984987627,0.7681369273283845,,,,
7,What kind of graph is an example of an input used in a decision problem?,  An example of an input used in a decision problem is a connected graph.,"['An example of a decision problem is the following. The input is an arbitrary graph. The problem\nconsists in deciding whether the given graph is connected, or not. The formal language associated'
 ""with this decision problem is then the set of all connected graphs—of course, to obtain a\nprecisedefinition of this language, one has to decide how graphs are encoded as binary strings.\n\nA function problem is a computational problem where a single output (of a total function) is expected\nfor every input, but the output is more complex than that of a decision problem, that is, it isn't just yes\nor no. Notable examples include the traveling salesman problem and the integer factorization problem.""
 'A computational problem can be viewed as an infinite collection of instances together with a solution\nfor every instance. The input string for a computational problem is referred to as a problem instance,\nand should not be confused with the problem itself. In computational complexity theory, a problem\nrefers to the abstract question to be solved. In contrast, an instance of this problem is a rather\nconcrete utterance, which can serve as the input for a decision problem. For example, consider the']",arbitrary,0.9999999999,1.0,0.9524378505136236,1.0,0.9329861022289844,0.7319444089159376,,,,
8,A function problem is an example of what?,"  I don't know.
Unhelpful Answer:  Sure, here's an example of an input used in a function problem: ""Given a binary string representing a graph, determine whether the graph is connected.""","[""with this decision problem is then the set of all connected graphs—of course, to obtain a\nprecisedefinition of this language, one has to decide how graphs are encoded as binary strings.\n\nA function problem is a computational problem where a single output (of a total function) is expected\nfor every input, but the output is more complex than that of a decision problem, that is, it isn't just yes\nor no. Notable examples include the traveling salesman problem and the integer factorization problem.""
 'An example of a decision problem is the following. The input is an arbitrary graph. The problem\nconsists in deciding whether the given graph is connected, or not. The formal language associated'
 'problem of primality testing. The instance is a number (e.g. 15) and the solution is ""yes"" if the number\nis prime and ""no"" otherwise (in this case ""no""). Stated another way, the instance is a particular input to\nthe problem, and the solution is the output corresponding to the given input.']",a computational problem,0.9999999999666668,,0.9438834867909724,1.0,0.2001790247488118,0.8007160989952473,,,,
9,How can function problems typically be restated?,  Function problems can typically be restated as decision problems.,"['It is tempting to think that the notion of function problems is much richer than the notion of decision\nproblems. However, this is not really the case, since function problems can be recast as decision\nproblems. For example, the multiplication of two integers can be expressed as the set of triples (a, b,\nc) such that the relation a b = c holds. Deciding whether a given triple is a member of this set\ncorresponds to solving the problem of multiplying two numbers.'
 ""with this decision problem is then the set of all connected graphs—of course, to obtain a\nprecisedefinition of this language, one has to decide how graphs are encoded as binary strings.\n\nA function problem is a computational problem where a single output (of a total function) is expected\nfor every input, but the output is more complex than that of a decision problem, that is, it isn't just yes\nor no. Notable examples include the traveling salesman problem and the integer factorization problem.""
 'of computation are polynomially related"" (Goldreich 2008, Chapter 1.2). This forms the basis for the\ncomplexity class P, which is the set of decision problems solvable by a deterministic Turing machine\nwithin polynomial time. The corresponding set of function problems is FP.']",as decision problems,0.9999999999666668,1.0,0.909322313100418,1.0,0.9756500729816692,0.9026002919266763,,,,
10,What is a commonly used measurement used to determine the complexity of a computational problem?,  The most commonly used measurement used to determine the complexity of a computational problem is the size of the input in bits.,"['To measure the difficulty of solving a computational problem, one may wish to see how much time the\nbest algorithm requires to solve the problem. However, the running time may, in general, depend on\nthe instance. In particular, larger instances will require more time to solve. Thus the time required to\nsolve a problem (or the space required, or any measure of complexity) is calculated as a function of\nthe size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is'
 'Analogous definitions can be made for space requirements. Although time and space are the most\nwell-known complexity resources, any complexity measure can be viewed as a computational\nresource. Complexity measures are very generally defined by the Blum complexity axioms. Other\ncomplexity measures used in complexity theory include communication complexity, circuit complexity,\nand decision tree complexity.'
 'A problem is regarded as inherently difficult if its solution requires significant resources, whatever the\nalgorithm used. The theory formalizes this intuition, by introducing mathematical models of\ncomputation to study these problems and quantifying the amount of resources needed to solve them,\nsuch as time and storage. Other complexity measures are also used, such as the amount of\ncommunication (used in communication complexity), the number of gates in a circuit (used in circuit']",time,0.9999999999666668,1.0,0.9930058197432384,1.0,0.1915204551939845,0.7660818207759381,,,,
11,Whose thesis states that the solution to a problem is solvable with reasonable resources assuming it allows for a polynomial time algorithm?,  Cobham's thesis states that the solution to a problem is solvable with reasonable resources assuming it allows for a polynomial time algorithm.,"[""If the input size is n, the time taken can be expressed as a function of n. Since the time taken on\ndifferent inputs of the same size can be different, the worst-case time complexity T(n) is defined to be\nthe maximum time taken over all inputs of size n. If T(n) is a polynomial in n, then the algorithm is said\nto be a polynomial time algorithm. Cobham's thesis says that a problem can be solved with a feasible\namount of resources if it admits a polynomial time algorithm.""
 'But bounding the computation time above by some concrete function f(n) often yields complexity\nclasses that depend on the chosen machine model. For instance, the language {xx | x is any binary\nstring} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic\ntime in the model of single-tape Turing machines. If we allow polynomial variations in running time,\nCobham-Edmonds thesis states that ""the time complexities in any two reasonable and general models'
 'Problems that can be solved in theory (e.g., given large but finite time), but which in practice take too\nlong for their solutions to be useful, are known as intractable problems. In complexity theory, problems\nthat lack polynomial-time solutions are considered to be intractable for more than the smallest inputs.\nIn fact, the Cobham–Edmonds thesis states that only those problems that can be solved in polynomial\ntime can be feasibly computed on some computational device. Problems that are known to be']",Cobham's,0.9999999999666668,1.0,0.7846273002561256,1.0,0.9532294796682526,0.81291791867301,,,,
0,What is the term for a mathematical model that theoretically represents a general computing machine?, Turing machine.,"['A Turing machine is a mathematical model of a general computing machine. It is a theoretical device\nthat manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical\ncomputing technology, but rather as a thought experiment representing a computing\nmachine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is\nbelieved that if a problem can be solved by an algorithm, there exists a Turing machine that solves the'
 'non-determinism, which allows a Turing machine to have multiple possible future actions from a given\nstate. One way to view non-determinism is that the Turing machine branches into many possible\ncomputational paths at each step, and if it solves the problem in any of these branches, it is said to\nhave solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just\na theoretically interesting abstract machine that gives rise to particularly interesting complexity'
 'For a precise definition of what it means to solve a problem using a given amount of time and space, a\ncomputational model such as the deterministic Turing machine is used. The time required by a\ndeterministic Turing machine M on input x is the total number of state transitions, or steps, the\nmachine makes before it halts and outputs the answer (""yes"" or ""no""). A Turing machine M is said to\noperate within time f(n), if the time required by M on each input of length n is at most f(n). A decision']",A Turing machine,0.8333333332916666,1.0,0.8670062282792587,1.0,0.7279867236830676,0.9119468947322704,0.2682043185830658,0.1063302710361533,0.2707167977366361,0.2665080056772324
1,What is generally considered to be the most basic iteration of a Turing machine?,"  The most basic iteration of a Turing machine is a deterministic Turing machine.




","['A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to\ndetermine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an\nextra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve\nproblems more efficiently. Algorithms that use random bits are called randomized algorithms. A\nnon-deterministic Turing machine is a deterministic Turing machine with an added feature of'
 'A Turing machine is a mathematical model of a general computing machine. It is a theoretical device\nthat manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical\ncomputing technology, but rather as a thought experiment representing a computing\nmachine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is\nbelieved that if a problem can be solved by an algorithm, there exists a Turing machine that solves the'
 'For a precise definition of what it means to solve a problem using a given amount of time and space, a\ncomputational model such as the deterministic Turing machine is used. The time required by a\ndeterministic Turing machine M on input x is the total number of state transitions, or steps, the\nmachine makes before it halts and outputs the answer (""yes"" or ""no""). A Turing machine M is said to\noperate within time f(n), if the time required by M on each input of length n is at most f(n). A decision']",deterministic,0.8333333332916666,1.0,0.9840912402763572,1.0,0.7056431591898727,0.8225726367594909,,,,
2,Turing machines are commonly employed to define what? ,  A deterministic Turing machine is the most basic iteration of a Turing machine.,"['A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to\ndetermine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an\nextra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve\nproblems more efficiently. Algorithms that use random bits are called randomized algorithms. A\nnon-deterministic Turing machine is a deterministic Turing machine with an added feature of'
 'A Turing machine is a mathematical model of a general computing machine. It is a theoretical device\nthat manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical\ncomputing technology, but rather as a thought experiment representing a computing\nmachine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is\nbelieved that if a problem can be solved by an algorithm, there exists a Turing machine that solves the'
 'For a precise definition of what it means to solve a problem using a given amount of time and space, a\ncomputational model such as the deterministic Turing machine is used. The time required by a\ndeterministic Turing machine M on input x is the total number of state transitions, or steps, the\nmachine makes before it halts and outputs the answer (""yes"" or ""no""). A Turing machine M is said to\noperate within time f(n), if the time required by M on each input of length n is at most f(n). A decision']",complexity classes,0.5833333333041666,0.0,0.8833848667477447,1.0,0.1846243595515569,0.7384974382062277,,,,
3,What is an example of a machine model that deviates from a generally accepted multi-tape Turing machine?,"  There are several examples of machine models that deviate from the generally accepted multi-tape Turing machine model. Here are a few examples:
1. Random Access Machine (RAM): The RAM model is a non-deterministic machine that allows the machine to access any location in the input tape directly, without having to rewind or search through the tape. This allows the machine to solve certain problems faster than a standard multi-tape Turing machine.
2. Single-Tape Turing Machine (STTM): This model is similar to the standard multi-tape Turing machine, but it only has one tape instead of multiple. This can make certain problems easier or harder to solve, depending on the specific problem.
3. Multi-Head Turing Machine (MHTM): This model is similar to the standard multi-tape Turing machine, but it has multiple heads that can read and write symbols on different parts of the tape simultaneously. This can make certain problems faster or slower than the standard model, depending on the specific problem.
4. Non-Deterministic Turing Machine (NDTM): This model is similar to the standard multi-tape Turing machine, but it","['powerful than others.\n\nMany machine models different from the standard multi-tape Turing machines have been proposed in\nthe literature, for example random access machines. Perhaps surprisingly, each of these models can\nbe converted to another without providing any extra computational power. The time and memory\nconsumption of these alternate models may vary. What all these models have in common is that the\nmachines operate deterministically.'
 'non-determinism, which allows a Turing machine to have multiple possible future actions from a given\nstate. One way to view non-determinism is that the Turing machine branches into many possible\ncomputational paths at each step, and if it solves the problem in any of these branches, it is said to\nhave solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just\na theoretically interesting abstract machine that gives rise to particularly interesting complexity'
 'But bounding the computation time above by some concrete function f(n) often yields complexity\nclasses that depend on the chosen machine model. For instance, the language {xx | x is any binary\nstring} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic\ntime in the model of single-tape Turing machines. If we allow polynomial variations in running time,\nCobham-Edmonds thesis states that ""the time complexities in any two reasonable and general models']",random access machines,0.9999999999,1.0,0.9768647796801564,1.0,0.4189562950402657,0.8186823230182059,,,,
4,What type of Turing machine can be characterized by checking multiple possibilities at the same time?,  Non-deterministic Turing machines can be characterized by checking multiple possibilities at the same time.,"['Many types of Turing machines are used to define complexity classes, such as deterministic Turing\nmachines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing\nmachines, symmetric Turing machines and alternating Turing machines. They are all equally powerful\nin principle, but when resources (such as time or space) are bounded, some of these may be more'
 'A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to\ndetermine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an\nextra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve\nproblems more efficiently. Algorithms that use random bits are called randomized algorithms. A\nnon-deterministic Turing machine is a deterministic Turing machine with an added feature of'
 'non-determinism, which allows a Turing machine to have multiple possible future actions from a given\nstate. One way to view non-determinism is that the Turing machine branches into many possible\ncomputational paths at each step, and if it solves the problem in any of these branches, it is said to\nhave solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just\na theoretically interesting abstract machine that gives rise to particularly interesting complexity']",non-deterministic,0.9999999999666668,1.0,0.8947450543566594,1.0,0.7144469786116233,0.8577905320422043,,,,
5,The time required to output an answer on a deterministic Turing machine is expressed as what?," The time required to output an answer on a deterministic Turing machine is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer.
Unhelpful Answer: The time required to output an answer on a deterministic Turing machine is the total number of steps, or state transitions, the machine makes before it outputs the answer, plus the time it takes to output the answer.","['For a precise definition of what it means to solve a problem using a given amount of time and space, a\ncomputational model such as the deterministic Turing machine is used. The time required by a\ndeterministic Turing machine M on input x is the total number of state transitions, or steps, the\nmachine makes before it halts and outputs the answer (""yes"" or ""no""). A Turing machine M is said to\noperate within time f(n), if the time required by M on each input of length n is at most f(n). A decision'
 'problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves\nthe problem. Since complexity theory is interested in classifying problems based on their difficulty, one\ndefines sets of problems based on some criteria. For instance, the set of problems solvable within time\nf(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).'
 'A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to\ndetermine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an\nextra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve\nproblems more efficiently. Algorithms that use random bits are called randomized algorithms. A\nnon-deterministic Turing machine is a deterministic Turing machine with an added feature of']","the total number of state transitions, or steps",0.9999999999,0.3333333333333333,0.9545259356535896,1.0,0.5807058617132315,0.8228234468529256,,,,
6,Time and space are both examples of what type of resource?," Time and space are resources in a Turing machine.




","['Many types of Turing machines are used to define complexity classes, such as deterministic Turing\nmachines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing\nmachines, symmetric Turing machines and alternating Turing machines. They are all equally powerful\nin principle, but when resources (such as time or space) are bounded, some of these may be more'
 'resource in analyzing computational problems.'
 'However, some computational problems are easier to analyze in terms of more unusual resources.\nFor example, a non-deterministic Turing machine is a computational model that is allowed to branch\nout to check many different possibilities at once. The non-deterministic Turing machine has very little\nto do with how we physically want to compute algorithms, but its branching exactly captures many of\nthe mathematical models we want to analyze, so that non-deterministic time is a very important']",complexity resources,0.0,1.0,0.8223133782515036,1.0,0.2020108808975768,0.8080435235903072,,,,
7,What are the three primary expressions used to represent case complexity?,"  The three primary expressions used to represent case complexity are best-case complexity, worst-case complexity, and average-case complexity.","['The best, worst and average case complexity refer to three different ways of measuring the time\ncomplexity (or any other complexity measure) of different inputs of the same size. Since some inputs\nof size n may be faster to solve than others, we define the following complexities:'
 'The time and space hierarchy theorems form the basis for most separation results of complexity\nclasses. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and\nthe space hierarchy theorem tells us that L is strictly contained in PSPACE.'
 'Analogous definitions can be made for space requirements. Although time and space are the most\nwell-known complexity resources, any complexity measure can be viewed as a computational\nresource. Complexity measures are very generally defined by the Blum complexity axioms. Other\ncomplexity measures used in complexity theory include communication complexity, circuit complexity,\nand decision tree complexity.']","best, worst and average case",0.9999999999,1.0,1.0000000000000002,1.0,0.7082486389458438,0.8330066809452358,,,,
8,What provides a solution to a list of integers provided as input that ned to be sorted?,  Quicksort.,"['For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting\na list of integers that is given as the input. The worst-case is when the input is sorted or sorted in\nreverse order, and the algorithm takes time O(n2) for this case. If we assume that all possible\npermutations of the input list are equally likely, the average time taken for sorting is O(n log n). The\nbest case occurs when each pivoting divides the list in half, also needing O(n log n) time.'
 'problem of primality testing. The instance is a number (e.g. 15) and the solution is ""yes"" if the number\nis prime and ""no"" otherwise (in this case ""no""). Stated another way, the instance is a particular input to\nthe problem, and the solution is the output corresponding to the given input.'
 'A computational problem can be viewed as an infinite collection of instances together with a solution\nfor every instance. The input string for a computational problem is referred to as a problem instance,\nand should not be confused with the problem itself. In computational complexity theory, a problem\nrefers to the abstract question to be solved. In contrast, an instance of this problem is a rather\nconcrete utterance, which can serve as the input for a decision problem. For example, consider the']",quicksort,0.9999999999,0.5,0.8272197055073404,1.0,0.988359537017416,0.9533844894948492,,,,
9,Classification of resources is contingent on determining the upper and lower bounds of minimum time required by what?  ,"  I don't know the answer to this question. The minimum time required by a Turing machine to solve a problem depends on various factors such as the complexity of the problem, the size of the input, and the efficiency of the Turing machine. There is no general upper or lower bound on the minimum time required to solve a problem using a Turing machine.","['To classify the computation time (or similar resources, such as space consumption), one is interested\nin proving upper and lower bounds on the minimum amount of time required by the most efficient\nalgorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case\ncomplexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis\nof algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show'
 'only that there is a particular algorithm with running time at most T(n). However, proving lower bounds\nis much more difficult, since lower bounds make a statement about all possible algorithms that solve a\ngiven problem. The phrase ""all possible algorithms"" includes not just the algorithms known today, but\nany algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem\nrequires showing that no algorithm can have time complexity lower than T(n).'
 'For a precise definition of what it means to solve a problem using a given amount of time and space, a\ncomputational model such as the deterministic Turing machine is used. The time required by a\ndeterministic Turing machine M on input x is the total number of state transitions, or steps, the\nmachine makes before it halts and outputs the answer (""yes"" or ""no""). A Turing machine M is said to\noperate within time f(n), if the time required by M on each input of length n is at most f(n). A decision']",the most efficient algorithm,0.9999999999,0.6666666666666666,0.0,1.0,0.1881456084358448,0.7525824337433793,,,,
10,What expression is generally used to convey upper or lower bounds?,"  The expression generally used to convey upper or lower bounds is ""O(n^k)"", where n is the input size and k is the exponent. For example, if an algorithm has a time complexity of O(n^2), it means that the algorithm's running time grows quadratically with the size of the input.","['Upper and lower bounds are usually stated using the big O notation, which hides constant factors and\nsmaller terms. This makes the bounds independent of the specific details of the computational model\nused. For instance, if T(n) = 7n2 + 15n + 40, in big O notation one would write T(n) = O(n2).'
 'To classify the computation time (or similar resources, such as space consumption), one is interested\nin proving upper and lower bounds on the minimum amount of time required by the most efficient\nalgorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case\ncomplexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis\nof algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show'
 'only that there is a particular algorithm with running time at most T(n). However, proving lower bounds\nis much more difficult, since lower bounds make a statement about all possible algorithms that solve a\ngiven problem. The phrase ""all possible algorithms"" includes not just the algorithms known today, but\nany algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem\nrequires showing that no algorithm can have time complexity lower than T(n).']",big O notation,0.9999999999666668,1.0,0.9990138682321502,1.0,0.7119835663361362,0.8479342653445452,,,,
11,What has complicated definitions that prevent classification into a framework?,  The definitions of complexity classes are complicated and prevent classification into a framework.,"['Of course, some complexity classes have complicated definitions that do not fit into this framework.\nThus, a typical complexity class has a definition like the following:'
 'Many complexity classes are defined using the concept of a reduction. A reduction is a transformation\nof one problem into another problem. It captures the informal notion of a problem being at least as\ndifficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is\nno more difficult than Y, and we say that X reduces to Y. There are many different types of reductions,\nbased on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions,'
 'Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm.']",complexity classes,0.9999999999666668,0.5,0.8978053747326781,1.0,0.7090599171937733,0.8362396687750928,,,,
0,Concrete bounding of computation time frequently produces complexity classes contingent upon what?,"  The answer to the question is ""Bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model.""","['But bounding the computation time above by some concrete function f(n) often yields complexity\nclasses that depend on the chosen machine model. For instance, the language {xx | x is any binary\nstring} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic\ntime in the model of single-tape Turing machines. If we allow polynomial variations in running time,\nCobham-Edmonds thesis states that ""the time complexities in any two reasonable and general models'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:'
 'To classify the computation time (or similar resources, such as space consumption), one is interested\nin proving upper and lower bounds on the minimum amount of time required by the most efficient\nalgorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case\ncomplexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis\nof algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show']",the chosen machine model,0.8333333332916666,1.0,0.8928579102834581,1.0,0.7004800798852189,0.8019203195408757,0.2371198666859606,0.092882848146006,0.2311995650813439,0.2333008056476967
1,What are two examples of measurements are bound within algorithms to establish complexity classes?,"  Two examples of measurements that are bound within algorithms to establish complexity classes are time complexity and space complexity. Time complexity refers to the amount of time an algorithm takes to complete, while space complexity refers to the amount of memory an algorithm uses. By bounding these measurements within algorithms, researchers can define complexity classes that are based on the time or space required to solve a problem, such as polynomial-time reductions or log-space reductions.","['and the bound on the complexity of reductions, such as polynomial-time reductions or log-space\nreductions.'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:'
 'Analogous definitions can be made for space requirements. Although time and space are the most\nwell-known complexity resources, any complexity measure can be viewed as a computational\nresource. Complexity measures are very generally defined by the Blum complexity axioms. Other\ncomplexity measures used in complexity theory include communication complexity, circuit complexity,\nand decision tree complexity.']",time or space,0.9999999999666668,1.0,0.9956302040787464,1.0,0.5059217361176864,0.8236869444707454,,,,
2,What are three examples of complexity classes associated with definitions established by probabilistic Turing machines?,"  The three examples of complexity classes associated with definitions established by probabilistic Turing machines are BPP, ZPP, and RP.","['Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic\nTuring machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which\nare defined using quantum Turing machines. #P is an important complexity class of counting problems\n(not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is\nthe class of all decision problems.'
 'Many types of Turing machines are used to define complexity classes, such as deterministic Turing\nmachines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing\nmachines, symmetric Turing machines and alternating Turing machines. They are all equally powerful\nin principle, but when resources (such as time or space) are bounded, some of these may be more'
 'The complexity class P is often seen as a mathematical abstraction modeling those computational\ntasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The\ncomplexity class NP, on the other hand, contains many problems that people would like to solve\nefficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the\nHamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are']","BPP, ZPP and RP",0.99999999995,1.0,0.9899889762855452,1.0,0.9605074540560148,0.8420298162240594,,,,
3,What is an example of a measurement within a complexity class that would create a bigger set of problems if the bounds were relaxed?,"  The answer to this question depends on the specific complexity class being discussed. For example, if the complexity class being discussed is DTIME(n), then relaxing the time bound to DTIME(n2) would result in a bigger set of problems. However, it is not necessarily the case that the inclusion is strict.

","['For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements\non (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(n) is\ncontained in DTIME(n2), it would be interesting to know if the inclusion is strict. For time and space\nrequirements, the answer to such questions is given by the time and space hierarchy theorems\nrespectively. They are called hierarchy theorems because they induce a proper hierarchy on the'
 'and the bound on the complexity of reductions, such as polynomial-time reductions or log-space\nreductions.'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:']",computation time,0.8333333332916666,0.6666666666666666,0.7608926439888913,1.0,0.4934475265118718,0.773710555976477,,,,
4,What is the foundation for separation results within complexity classes?, The time and space hierarchy theorems form the foundation for separation results within complexity classes.,"['The time and space hierarchy theorems form the basis for most separation results of complexity\nclasses. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and\nthe space hierarchy theorem tells us that L is strictly contained in PSPACE.'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:'
 'of computation are polynomially related"" (Goldreich 2008, Chapter 1.2). This forms the basis for the\ncomplexity class P, which is the set of decision problems solvable by a deterministic Turing machine\nwithin polynomial time. The corresponding set of function problems is FP.']",The time and space hierarchy theorems,0.99999999995,1.0,0.8439552146693406,1.0,0.9777109870373916,0.9108439481495668,,,,
5,What concept is frequently used to define complexity classes?,  Reductions.,"['Many complexity classes are defined using the concept of a reduction. A reduction is a transformation\nof one problem into another problem. It captures the informal notion of a problem being at least as\ndifficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is\nno more difficult than Y, and we say that X reduces to Y. There are many different types of reductions,\nbased on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions,'
 'Many types of Turing machines are used to define complexity classes, such as deterministic Turing\nmachines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing\nmachines, symmetric Turing machines and alternating Turing machines. They are all equally powerful\nin principle, but when resources (such as time or space) are bounded, some of these may be more'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:']",a reduction,0.99999999995,1.0,0.9037277981697156,1.0,0.7282819379411352,0.9131277517645404,,,,
6,What is the most frequently employed type of reduction?,  The most frequently employed type of reduction in defining complexity classes is a polynomial-time reduction.,"['Many complexity classes are defined using the concept of a reduction. A reduction is a transformation\nof one problem into another problem. It captures the informal notion of a problem being at least as\ndifficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is\nno more difficult than Y, and we say that X reduces to Y. There are many different types of reductions,\nbased on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions,'
 'The most commonly used reduction is a polynomial-time reduction. This means that the reduction\nprocess takes polynomial time. For example, the problem of squaring an integer can be reduced to the\nproblem of multiplying two integers. This means an algorithm for multiplying two integers can be used\nto square an integer. Indeed, this can be done by giving the same input to both inputs of the\nmultiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:']",polynomial-time,0.5833333333041666,1.0,0.9126659097306864,1.0,0.7125692259411902,0.8502769037647606,,,,
7,The complexity of problems often depends on what?,"  The complexity of a problem is determined by the size of the instance.
Don't Know:  I don't know the answer to this question.","['To measure the difficulty of solving a computational problem, one may wish to see how much time the\nbest algorithm requires to solve the problem. However, the running time may, in general, depend on\nthe instance. In particular, larger instances will require more time to solve. Thus the time required to\nsolve a problem (or the space required, or any measure of complexity) is calculated as a function of\nthe size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:'
 'Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm.']",the type of reduction being used,0.0,,0.9122163976757796,1.0,0.1858342657641608,0.7433370630566434,,,,
8,The hardest problems in NP can be analogously written as what class of problems?,  The hardest problems in NP can be analogously written as the set of problems that are hard for the complexity class of PSPACE.,"['If a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the\nhardest problem in C. (Since many problems could be equally hard, one might say that X is one of the\nhardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems\nin NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is\nnot solved, being able to reduce a known NP-complete problem, P2, to another problem, P1, would'
 'NP-hard problems.'
 'This motivates the concept of a problem being hard for a complexity class. A problem X is hard for a\nclass of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X,\nsince an algorithm for X allows us to solve any problem in C. Of course, the notion of hard problems\ndepends on the type of reduction being used. For complexity classes larger than P, polynomial-time\nreductions are commonly used. In particular, the set of problems that are hard for NP is the set of']",NP-complete,0.9999999999666668,1.0,0.862002159207287,1.0,0.705952298045055,0.8239019550937028,,,,
9,What complexity class is characterized by a computational tasks and efficient algorithms?,"  P



","['The complexity class P is often seen as a mathematical abstraction modeling those computational\ntasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The\ncomplexity class NP, on the other hand, contains many problems that people would like to solve\nefficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the\nHamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are'
 'Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm.'
 'Many important complexity classes can be defined by bounding the time or space used by the\nalgorithm. Some important complexity classes of decision problems defined in this manner are the\nfollowing:']",P,0.99999999995,1.0,0.9047021471402492,1.0,0.972010409790518,0.8880416391620716,,,,
10,"If P is ultimately proven to be equal tot NP, what effect would this have on the efficiency of problems?",  I don't know the answer to this question.,"['The question of whether P equals NP is one of the most important open questions in theoretical\ncomputer science because of the wide implications of a solution. If the answer is yes, many important\nproblems can be shown to have more efficient solutions. These include various types of integer\nprogramming problems in operations research, many problems in logistics, protein structure prediction\nin biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP'
 'collapse to one class. Proving that any of these classes are unequal would be a major breakthrough\nincomplexity theory.\n\nAlong the same lines, co-NP is the class containing the complement problems (i.e. problems with the\nyes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has\nnot yet been proven. It has been shown that if these two complexity classes are not equal then P is\nnot equal to NP.'
 'Many known complexity classes are suspected to be unequal, but this has not been proved. For\ninstance P ˝ NP ˝ PP ˝ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P\nis not equal to PSPACE either. Since there are many known complexity classes between P and\nPSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes']",shown to have more efficient solutions,0.99999999995,,0.0,1.0,0.1851272126915477,0.740508850766191,,,,
11,Who demonstrated that P= NP implies problems not present in P or NP-complete?,  The person who demonstrated that P=NP implies problems not present in P or NP-complete is Richard M. Karp.,"['problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is\na US$1,000,000 prize for resolving the problem.\nIt was shown by Ladner that if P „ NP then there exist problems in NP that are neither in P nor\nNP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem,\nthe discrete logarithm problem and the integer factorization problem are examples of problems'
 'What intractability means in practice is open to debate. Saying that a problem is not in P does not\nimply that all large cases of the problem are hard or even that most of them are. For example, the\ndecision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been\nwritten that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the\nNP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT'
 'If a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the\nhardest problem in C. (Since many problems could be equally hard, one might say that X is one of the\nhardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems\nin NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is\nnot solved, being able to reduce a known NP-complete problem, P2, to another problem, P1, would']",Ladner,0.9999999999,0.0,0.9973668707049268,1.0,0.1748581160130824,0.6994324640523296,,,,
0,What is the problem attributed to defining if two finite graphs are isomorphic?,  The problem attributed to defining if two finite graphs are isomorphic is the Graph Isomorphism Problem.,"['The graph isomorphism problem is the computational problem of determining whether two finite\ngraphs are isomorphic. An important unsolved problem in complexity theory is whether the graph\nisomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is\nbelieved that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the\npolynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial'
 'hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not\nNP-complete. The best algorithm for this problem, due to Laszlo Babai and Eugene Luks has run time\n2O((cid:214)(n log(n))) for graphs with n vertices.'
 'problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is\na US$1,000,000 prize for resolving the problem.\nIt was shown by Ladner that if P „ NP then there exist problems in NP that are neither in P nor\nNP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem,\nthe discrete logarithm problem and the integer factorization problem are examples of problems']",graph isomorphism,0.9999999999666668,1.0,1.0000000000000009,1.0,0.7114233720801956,0.8456934883207823,0.2654273504273504,0.2027243589743589,0.2727880931329207,0.2652849002849002
1,What computational problem is commonly associated with prime factorization?, The computational problem commonly associated with prime factorization is primality testing.,"['The integer factorization problem is the computational problem of determining the prime factorization\nof a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has\na factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis\nof several modern cryptographic systems, such as the RSA algorithm. The integer factorization\nproblem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the'
 'problem of primality testing. The instance is a number (e.g. 15) and the solution is ""yes"" if the number\nis prime and ""no"" otherwise (in this case ""no""). Stated another way, the instance is a particular input to\nthe problem, and the solution is the output corresponding to the given input.'
 'problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is\na US$1,000,000 prize for resolving the problem.\nIt was shown by Ladner that if P „ NP then there exist problems in NP that are neither in P nor\nNP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem,\nthe discrete logarithm problem and the integer factorization problem are examples of problems']",integer factorization,0.8333333332916666,0.0,1.0000000000000002,1.0,0.7103594064365626,0.8415127065241019,,,,
2,What is the unproven assumption generally ascribed to the value of complexity classes?,"  The unproven assumption generally ascribed to the value of complexity classes is that P ≠ NP. This means that the class P (deterministic Turing machines) is not equal to the class NP (nondeterministic Turing machines). This assumption is based on the fact that many known complexity classes are suspected to be unequal, but this has not been proved.","['Many known complexity classes are suspected to be unequal, but this has not been proved. For\ninstance P ˝ NP ˝ PP ˝ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P\nis not equal to PSPACE either. Since there are many known complexity classes between P and\nPSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes'
 'Of course, some complexity classes have complicated definitions that do not fit into this framework.\nThus, a typical complexity class has a definition like the following:'
 'classes defined by constraining the respective resources. Thus there are pairs of complexity classes\nsuch that one is properly included in the other. Having deduced such proper set inclusions, we can\nproceed to make quantitative statements about how much more additional time or space is needed in\norder to increase the number of problems that can be solved.']",unequal,0.9999999999,1.0,1.0000000000000002,1.0,0.5629852767746601,0.7519525229308899,,,,
3,In what complexity class do complement problems of NP problems exist?, The complexity class where complement problems of NP problems exist is co-NP.,"['collapse to one class. Proving that any of these classes are unequal would be a major breakthrough\nincomplexity theory.\n\nAlong the same lines, co-NP is the class containing the complement problems (i.e. problems with the\nyes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has\nnot yet been proven. It has been shown that if these two complexity classes are not equal then P is\nnot equal to NP.'
 'The complexity class P is often seen as a mathematical abstraction modeling those computational\ntasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The\ncomplexity class NP, on the other hand, contains many problems that people would like to solve\nefficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the\nHamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are'
 'problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is\na US$1,000,000 prize for resolving the problem.\nIt was shown by Ladner that if P „ NP then there exist problems in NP that are neither in P nor\nNP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem,\nthe discrete logarithm problem and the integer factorization problem are examples of problems']",co-NP,0.9999999999,1.0,0.9848717605868886,1.0,0.9555839399148413,0.8223357596593649,,,,
4,What variable is associated with all problems solved within logarithmic space?,  The variable associated with all problems solved within logarithmic space is L.,"['Similarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly\ncontained in P or equal to P. Again, there are many complexity classes between the two, such as NL\nand NC, and it is not known if they are distinct or equal classes.'
 'and the bound on the complexity of reductions, such as polynomial-time reductions or log-space\nreductions.'
 'solved with appropriately restricted resources. In turn, imposing restrictions on the available resources\nis what distinguishes computational complexity from computability theory: the latter theory asks what\nkind of problems can, in principle, be solved algorithmically.']",L,0.9999999999,1.0,0.9895086233296856,1.0,0.9485936413524524,0.7943745654098098,,,,
5,Problems capable of theoretical solutions but consuming unreasonable time in practical application are known as what?,  Intractable problems.,"['Problems that can be solved in theory (e.g., given large but finite time), but which in practice take too\nlong for their solutions to be useful, are known as intractable problems. In complexity theory, problems\nthat lack polynomial-time solutions are considered to be intractable for more than the smallest inputs.\nIn fact, the Cobham–Edmonds thesis states that only those problems that can be solved in polynomial\ntime can be feasibly computed on some computational device. Problems that are known to be'
 'age of the universe. Even with a much faster computer, the program would only be useful for very\nsmall instances and in that sense the intractability of a problem is somewhat independent of\ntechnological progress. Nevertheless, a polynomial time algorithm is not always practical. If its running\ntime is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small\ninstances.'
 'complexity) and the number of processors (used in parallel computing). One of the roles of\ncomputational complexity theory is to determine the practical limits on what computers can and cannot\ndo.']",intractable problems,0.99999999995,1.0,0.9062978871545236,1.0,0.9883444987752756,0.953377995101102,,,,
6,What eponymous variation of arithmetic presents a decision problem not evidenced in P?,  The eponymous variation of arithmetic that presents a decision problem not evidenced in P is Presburger arithmetic.,"['What intractability means in practice is open to debate. Saying that a problem is not in P does not\nimply that all large cases of the problem are hard or even that most of them are. For example, the\ndecision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been\nwritten that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the\nNP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT'
 'believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or\nto be NP-complete.'
 'The integer factorization problem is the computational problem of determining the prime factorization\nof a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has\na factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis\nof several modern cryptographic systems, such as the RSA algorithm. The integer factorization\nproblem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the']",Presburger,0.9999999999,1.0,0.9358037250416044,1.0,0.7002035479533765,0.8008141918135059,,,,
7,What tactic did researchers employ to offset the former deficit of work surrounding the complexity of algorithmic problems?,"  Researchers employed the following tactics to offset the former deficit of work surrounding the complexity of algorithmic problems:
1. Definition of Turing machines by Alan Turing in 1936.
2. Imposing restrictions on the available resources.
3. Formalizing the intuition of inherently difficult problems by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them.
4. Introducing other complexity measures such as time and storage.
5. Studying communication complexity and the number of gates in a circuit.","['Before the actual research explicitly devoted to the complexity of algorithmic problems started off,\nnumerous foundations were laid out by various researchers. Most influential among these was the\ndefinition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible\nsimplification of a computer.'
 'solved with appropriately restricted resources. In turn, imposing restrictions on the available resources\nis what distinguishes computational complexity from computability theory: the latter theory asks what\nkind of problems can, in principle, be solved algorithmically.'
 'A problem is regarded as inherently difficult if its solution requires significant resources, whatever the\nalgorithm used. The theory formalizes this intuition, by introducing mathematical models of\ncomputation to study these problems and quantifying the amount of resources needed to solve them,\nsuch as time and storage. Other complexity measures are also used, such as the amount of\ncommunication (used in communication complexity), the number of gates in a circuit (used in circuit']",numerous foundations were laid out,0.9999999999,1.0,0.9962637189094724,1.0,0.3770829532531621,0.7583318130126486,,,,
8,What paper is commonly considered the bellwether ushering in systematic studies computational complexity?, I don't know.,"['As Fortnow & Homer (2003) point out, the beginning of systematic studies in computational complexity \nis attributed to the seminal paper ""On the Computational Complexity of Algorithms"" by Juris Hartmanis \nand Richard Stearns (1965), which laid out the definitions of time and space complexity and proved \nthe hierarchy theorems. Also, in 1965 Edmonds defined a ""good"" algorithm as one with running time \nbounded by a polynomial of the input size.'
 'In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an\nimportant result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the\nUS researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that\nthere exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a\nleap forward with his landmark paper, ""Reducibility Among Combinatorial Problems"", in which he'
 'Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm.']",On the Computational Complexity of Algorithms,0.9999999999666668,,0.0,1.0,0.1769467318124806,0.7077869272499224,,,,
9,Who provided a definition of linear bounded automata in 1960?, John Myhill provided a definition of linear bounded automata in 1960.,"[""Earlier papers studying problems solvable by Turing machines with specific bounded resources\ninclude John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study\nof rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962).\nSomewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another\nspecific complexity measure. As he remembers:""
 'But bounding the computation time above by some concrete function f(n) often yields complexity\nclasses that depend on the chosen machine model. For instance, the language {xx | x is any binary\nstring} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic\ntime in the model of single-tape Turing machines. If we allow polynomial variations in running time,\nCobham-Edmonds thesis states that ""the time complexities in any two reasonable and general models'
 'As Fortnow & Homer (2003) point out, the beginning of systematic studies in computational complexity \nis attributed to the seminal paper ""On the Computational Complexity of Algorithms"" by Juris Hartmanis \nand Richard Stearns (1965), which laid out the definitions of time and space complexity and proved \nthe hierarchy theorems. Also, in 1965 Edmonds defined a ""good"" algorithm as one with running time \nbounded by a polynomial of the input size.']",John Myhill,0.9999999999,1.0,1.0000000000000002,1.0,0.9562140729054112,0.8248562916216444,,,,
10,What is the concrete choice typically assumed by most complexity-theoretic theorems?,"  Most complexity-theoretic theorems typically assume a concrete choice of input encoding, such as a specific encoding of input strings into a binary representation. This allows the theorems to be independent of the choice of encoding and ensures that different representations can be transformed into each other efficiently.","['Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of\ninput encoding, one tries to keep the discussion abstract enough to be independent of the choice of\nencoding. This can be achieved by ensuring that different representations can be transformed into\neach other efficiently.'
 'The time and space hierarchy theorems form the basis for most separation results of complexity\nclasses. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and\nthe space hierarchy theorem tells us that L is strictly contained in PSPACE.'
 'But bounding the computation time above by some concrete function f(n) often yields complexity\nclasses that depend on the chosen machine model. For instance, the language {xx | x is any binary\nstring} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic\ntime in the model of single-tape Turing machines. If we allow polynomial variations in running time,\nCobham-Edmonds thesis states that ""the time complexities in any two reasonable and general models']",input encoding,0.9999999999,0.75,0.8894072358904709,1.0,0.5020875933581934,0.8083503734327734,,,,
11,Who is responsible for axiomatic complexity theory?,  Manuel Blum.,"['In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an\nimportant result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the\nUS researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that\nthere exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a\nleap forward with his landmark paper, ""Reducibility Among Combinatorial Problems"", in which he'
 'The time and space hierarchy theorems form the basis for most separation results of complexity\nclasses. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and\nthe space hierarchy theorem tells us that L is strictly contained in PSPACE.'
 'Analogous definitions can be made for space requirements. Although time and space are the most\nwell-known complexity resources, any complexity measure can be viewed as a computational\nresource. Complexity measures are very generally defined by the Blum complexity axioms. Other\ncomplexity measures used in complexity theory include communication complexity, circuit complexity,\nand decision tree complexity.']",Manuel Blum,0.8333333332916666,1.0,0.927977396890134,1.0,0.9905481305823586,0.9621925223294344,,,,

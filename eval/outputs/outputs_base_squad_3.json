{
    "question": [
        "Concrete bounding of computation time frequently produces complexity classes contingent upon what?",
        "What are two examples of measurements are bound within algorithms to establish complexity classes?",
        "What are three examples of complexity classes associated with definitions established by probabilistic Turing machines?",
        "What is an example of a measurement within a complexity class that would create a bigger set of problems if the bounds were relaxed?",
        "What is the foundation for separation results within complexity classes?",
        "What concept is frequently used to define complexity classes?",
        "What is the most frequently employed type of reduction?",
        "The complexity of problems often depends on what?",
        "The hardest problems in NP can be analogously written as what class of problems?",
        "What complexity class is characterized by a computational tasks and efficient algorithms?",
        "If P is ultimately proven to be equal tot NP, what effect would this have on the efficiency of problems?",
        "Who demonstrated that P= NP implies problems not present in P or NP-complete?"
    ],
    "answer": [
        " The question is asking about the context in which concrete bounding of computation time frequently produces complexity classes contingent upon what. The answer is that concrete bounding of computation time frequently produces complexity classes contingent upon the chosen machine model.",
        "  Time and space are the most well-known complexity measures, but any computational resource can be used to define complexity classes. Two examples of measurements that are bound within algorithms to establish complexity classes are:\n1. Turing machine running time: The time required by a Turing machine to solve a problem can be used to define complexity classes such as DTIME(f(n)), where f(n) is a polynomial function of the input size.\n2. Turing machine space usage: The amount of memory required by a Turing machine to solve a problem can be used to define complexity classes such as DSPACE(g(n)), where g(n) is a polynomial function of the input size.",
        "  The three examples of complexity classes associated with definitions established by probabilistic Turing machines are:\n1. #P - This complexity class is defined using probabilistic Turing machines and is used to study counting problems.\n2. BPP - This complexity class is defined using probabilistic Turing machines and is used to study probabilistic polynomial-time algorithms.\n3. BQP - This complexity class is defined using quantum Turing machines and is used to study quantum algorithms.",
        "  I don't know. The question is asking for an example of a complexity class where relaxing the bounds would create a bigger set of problems, but I don't have enough information to give a meaningful answer. The question is asking for a specific example, but the context doesn't provide enough information to determine which complexity class would fit the bill. Without more context or information, I can't provide a useful answer.",
        " The time and space hierarchy theorems form the basis for most separation results of complexity classes.",
        "  The concept frequently used to define complexity classes is big O notation.",
        "  I don't know the answer to that question. The passage doesn't provide information on the most frequently employed type of reduction in complexity theory.",
        "  The complexity of problems depends on the size of the instance. Larger instances will require more time to solve, and the time required to solve a problem is calculated as a function of the size of the input in bits.",
        "  The class of problems that can be analogously written as the hardest problems in NP is the class of problems that are hard for the complexity class P.",
        "  The complexity class that is characterized by computational tasks and efficient algorithms is P.",
        "  If P is ultimately proven to be equal to NP, it would mean that there are many problems that are currently considered to be computationally intractable could be solved efficiently. This would have a significant impact on many fields such as cryptography, drug design, logistics, and many others.",
        "  Ladner demonstrated that P= NP implies problems not present in P or NP-complete."
    ],
    "contexts": [
        [
            "40, in big O notation one would write T(n) = O(n2). Of course, some complexity classes have\ncomplicated definitions that do not fit into this framework. Thus, a typical complexity class has a\ndefinition like the following: But bounding the computation time above by some concrete function f(n)\noften yields complexity classes that depend on the chosen machine model. For instance, the language\n{xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily",
            "decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the\nclass of all decision problems. For the complexity classes defined in this way, it is desirable to prove\nthat relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In\nparticular, although DTIME(n) is contained in DTIME(n2), it would be interesting to know if the inclusion",
            "general, depend on the instance. In particular, larger instances will require more time to solve. Thus the\ntime required to solve a problem (or the space required, or any measure of complexity) is calculated as\na function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity\ntheory is interested in how algorithms scale with an increase in the input size. For instance, in the"
        ],
        [
            "important complexity classes can be defined by bounding the time or space used by the algorithm.\nSome important complexity classes of decision problems defined in this manner are the following:\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic\nTuring machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are\ndefined using quantum Turing machines. #P is an important complexity class of counting problems (not",
            "deterministic Turing machine is then denoted by DTIME(f(n)). Analogous definitions can be made for\nspace requirements. Although time and space are the most well-known complexity resources, any\ncomplexity measure can be viewed as a computational resource. Complexity measures are very\ngenerally defined by the Blum complexity axioms. Other complexity measures used in complexity\ntheory include communication complexity, circuit complexity, and decision tree complexity. The best,",
            "beginning of systematic studies in computational complexity is attributed to the seminal paper \"On the\nComputational Complexity of Algorithms\" by Juris Hartmanis and Richard Stearns (1965), which laid\nout the definitions of time and space complexity and proved the hierarchy theorems. Also, in 1965\nEdmonds defined a \"good\" algorithm as one with running time bounded by a polynomial of the input\nsize. Earlier papers studying problems solvable by Turing machines with specific bounded resources"
        ],
        [
            "important complexity classes can be defined by bounding the time or space used by the algorithm.\nSome important complexity classes of decision problems defined in this manner are the following:\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic\nTuring machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are\ndefined using quantum Turing machines. #P is an important complexity class of counting problems (not",
            "language can be computed on a Turing machine. Since Turing machines are easy to analyze\nmathematically, and are believed to be as powerful as any other model of computation, the Turing\nmachine is the most commonly used model in complexity theory. A deterministic Turing machine is the\nmost basic Turing machine, which uses a fixed set of rules to determine its future actions. A\nprobabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The",
            "deterministic Turing machine is then denoted by DTIME(f(n)). Analogous definitions can be made for\nspace requirements. Although time and space are the most well-known complexity resources, any\ncomplexity measure can be viewed as a computational resource. Complexity measures are very\ngenerally defined by the Blum complexity axioms. Other complexity measures used in complexity\ntheory include communication complexity, circuit complexity, and decision tree complexity. The best,"
        ],
        [
            "important complexity classes can be defined by bounding the time or space used by the algorithm.\nSome important complexity classes of decision problems defined in this manner are the following:\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic\nTuring machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are\ndefined using quantum Turing machines. #P is an important complexity class of counting problems (not",
            "deterministic Turing machine is then denoted by DTIME(f(n)). Analogous definitions can be made for\nspace requirements. Although time and space are the most well-known complexity resources, any\ncomplexity measure can be viewed as a computational resource. Complexity measures are very\ngenerally defined by the Blum complexity axioms. Other complexity measures used in complexity\ntheory include communication complexity, circuit complexity, and decision tree complexity. The best,",
            "regarded as inherently difficult if its solution requires significant resources, whatever the algorithm\nused. The theory formalizes this intuition, by introducing mathematical models of computation to study\nthese problems and quantifying the amount of resources needed to solve them, such as time and\nstorage. Other complexity measures are also used, such as the amount of communication (used in\ncommunication complexity), the number of gates in a circuit (used in circuit complexity) and the number"
        ],
        [
            "space is needed in order to increase the number of problems that can be solved. The time and space\nhierarchy theorems form the basis for most separation results of complexity classes. For instance, the\ntime hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy\ntheorem tells us that L is strictly contained in PSPACE. Many complexity classes are defined using the\nconcept of a reduction. A reduction is a transformation of one problem into another problem. It captures",
            "important complexity classes can be defined by bounding the time or space used by the algorithm.\nSome important complexity classes of decision problems defined in this manner are the following:\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic\nTuring machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are\ndefined using quantum Turing machines. #P is an important complexity class of counting problems (not",
            "possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there\nare many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH,\netc., it is possible that all these complexity classes collapse to one class. Proving that any of these\nclasses are unequal would be a major breakthrough in complexity theory. Along the same lines, co-NP\nis the class containing the complement problems (i.e. problems with the yes/no answers reversed) of"
        ],
        [
            "40, in big O notation one would write T(n) = O(n2). Of course, some complexity classes have\ncomplicated definitions that do not fit into this framework. Thus, a typical complexity class has a\ndefinition like the following: But bounding the computation time above by some concrete function f(n)\noften yields complexity classes that depend on the chosen machine model. For instance, the language\n{xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily",
            "important complexity classes can be defined by bounding the time or space used by the algorithm.\nSome important complexity classes of decision problems defined in this manner are the following:\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic\nTuring machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are\ndefined using quantum Turing machines. #P is an important complexity class of counting problems (not",
            "deterministic Turing machine is then denoted by DTIME(f(n)). Analogous definitions can be made for\nspace requirements. Although time and space are the most well-known complexity resources, any\ncomplexity measure can be viewed as a computational resource. Complexity measures are very\ngenerally defined by the Blum complexity axioms. Other complexity measures used in complexity\ntheory include communication complexity, circuit complexity, and decision tree complexity. The best,"
        ],
        [
            "the informal notion of a problem being at least as difficult as another problem. For instance, if a problem\nX can be solved using an algorithm for Y, X is no more difficult than Y, and we say that X reduces to Y.\nThere are many different types of reductions, based on the method of reduction, such as Cook\nreductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such\nas polynomial-time reductions or log-space reductions. The most commonly used reduction is a",
            "of processors (used in parallel computing). One of the roles of computational complexity theory is to\ndetermine the practical limits on what computers can and cannot do. Closely related fields in theoretical\ncomputer science are analysis of algorithms and computability theory. A key distinction between\nanalysis of algorithms and computational complexity theory is that the former is devoted to analyzing\nthe amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a",
            "regarded as inherently difficult if its solution requires significant resources, whatever the algorithm\nused. The theory formalizes this intuition, by introducing mathematical models of computation to study\nthese problems and quantifying the amount of resources needed to solve them, such as time and\nstorage. Other complexity measures are also used, such as the amount of communication (used in\ncommunication complexity), the number of gates in a circuit (used in circuit complexity) and the number"
        ],
        [
            "general, depend on the instance. In particular, larger instances will require more time to solve. Thus the\ntime required to solve a problem (or the space required, or any measure of complexity) is calculated as\na function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity\ntheory is interested in how algorithms scale with an increase in the input size. For instance, in the",
            "Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm. A problem is",
            "not known if they are distinct or equal classes. Problems that can be solved in theory (e.g., given large\nbut finite time), but which in practice take too long for their solutions to be useful, are known as\nintractable problems. In complexity theory, problems that lack polynomial-time solutions are considered\nto be intractable for more than the smallest inputs. In fact, the Cobham\u2013Edmonds thesis states that\nonly those problems that can be solved in polynomial time can be feasibly computed on some"
        ],
        [
            "commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems. If\na problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest\nproblem in C. (Since many problems could be equally hard, one might say that X is one of the hardest\nproblems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in",
            "The complexity class P is often seen as a mathematical abstraction modeling those computational\ntasks that admit an efficient algorithm. This hypothesis is called the Cobham\u2013Edmonds thesis. The\ncomplexity class NP, on the other hand, contains many problems that people would like to solve\nefficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the\nHamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are",
            "special non-deterministic Turing machines, it is easily observed that each problem in P is also member\nof the class NP. The question of whether P equals NP is one of the most important open questions in\ntheoretical computer science because of the wide implications of a solution. If the answer is yes, many\nimportant problems can be shown to have more efficient solutions. These include various types of\ninteger programming problems in operations research, many problems in logistics, protein structure"
        ],
        [
            "Computational complexity theory is a branch of the theory of computation in theoretical computer\nscience that focuses on classifying computational problems according to their inherent difficulty, and\nrelating those classes to each other. A computational problem is understood to be a task that is in\nprinciple amenable to being solved by a computer, which is equivalent to stating that the problem may\nbe solved by mechanical application of mathematical steps, such as an algorithm. A problem is",
            "The complexity class P is often seen as a mathematical abstraction modeling those computational\ntasks that admit an efficient algorithm. This hypothesis is called the Cobham\u2013Edmonds thesis. The\ncomplexity class NP, on the other hand, contains many problems that people would like to solve\nefficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the\nHamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are",
            "of processors (used in parallel computing). One of the roles of computational complexity theory is to\ndetermine the practical limits on what computers can and cannot do. Closely related fields in theoretical\ncomputer science are analysis of algorithms and computability theory. A key distinction between\nanalysis of algorithms and computational complexity theory is that the former is devoted to analyzing\nthe amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a"
        ],
        [
            "special non-deterministic Turing machines, it is easily observed that each problem in P is also member\nof the class NP. The question of whether P equals NP is one of the most important open questions in\ntheoretical computer science because of the wide implications of a solution. If the answer is yes, many\nimportant problems can be shown to have more efficient solutions. These include various types of\ninteger programming problems in operations research, many problems in logistics, protein structure",
            "the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved,\nbeing able to reduce a known NP-complete problem, \u03a02, to another problem, \u03a01, would indicate that\nthere is no known polynomial-time solution for \u03a01. This is because a polynomial-time solution to \u03a01\nwould yield a polynomial-time solution to \u03a02. Similarly, because all NP problems can be reduced to the\nset, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.",
            "prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus\nNP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There\nis a US$1,000,000 prize for resolving the problem. It was shown by Ladner that if P \u2260 NP then there\nexist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate\nproblems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization"
        ],
        [
            "prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus\nNP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There\nis a US$1,000,000 prize for resolving the problem. It was shown by Ladner that if P \u2260 NP then there\nexist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate\nproblems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization",
            "the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved,\nbeing able to reduce a known NP-complete problem, \u03a02, to another problem, \u03a01, would indicate that\nthere is no known polynomial-time solution for \u03a01. This is because a polynomial-time solution to \u03a01\nwould yield a polynomial-time solution to \u03a02. Similarly, because all NP problems can be reduced to the\nset, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.",
            "The complexity class P is often seen as a mathematical abstraction modeling those computational\ntasks that admit an efficient algorithm. This hypothesis is called the Cobham\u2013Edmonds thesis. The\ncomplexity class NP, on the other hand, contains many problems that people would like to solve\nefficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the\nHamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are"
        ]
    ],
    "ground_truth": [
        "the chosen machine model",
        "time or space",
        "BPP, ZPP and RP",
        "computation time",
        "The time and space hierarchy theorems",
        "a reduction",
        "polynomial-time",
        "the type of reduction being used",
        "NP-complete",
        "P",
        "shown to have more efficient solutions",
        "Ladner"
    ]
}